{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_HW10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeverton/aml-hw10/blob/master/AML_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vUTgGUi4JEft"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 10 - CIFAR10 Image Classification with PyTorch"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T5Kx8gJBJEW_"
      },
      "cell_type": "markdown",
      "source": [
        "## About"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aRAalfd1GITX"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of the homework is to train a convolutional neural network on the standard CIFAR10 image classfication dataset.\n",
        "\n",
        "When solving machine learning tasks using neural networks, one typically starts with a simple network architecture and then improves the network by adding new layers, retraining, adjusting parameters, retraining, etc.  We attempt to illustrate this process below with several architecture improvements.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OCl5gakxJes-"
      },
      "cell_type": "markdown",
      "source": [
        "## Dev Environment\n",
        "### Working on Google Colab\n",
        "You may choose to work locally or on Google Colaboratory. You have access to free compute through this service.   Colab is recommended since it will be setup correctly and will have access to GPU resources.\n",
        "1. Visit https://colab.research.google.com/drive \n",
        "2. Navigate to the **`Upload`** tab, and upload your `HW10.ipynb`\n",
        "3. Now on the top right corner, under the `Comment` and `Share` options, you should see a `Connect` option. Once you are connected, you will have access to a VM with 12GB RAM, 50 GB disk space and a single GPU. The dropdown menu will allow you to connect to a local runtime as well.\n",
        "\n",
        "**Notes:** \n",
        "* **If you do not have a working setup for Python 3, this is your best bet. It will also save you from heavy installations like `tensorflow` if you don't want to deal with those.**\n",
        "* ***There is a downside*. You can only use this instance for a single 12-hour stretch, after which your data will be deleted, and you would have redownload all your datasets, any libraries not already on the VM, and regenerate your logs**.\n",
        "\n",
        "\n",
        "### Installing PyTorch and Dependencies\n",
        "\n",
        "The instructions for installing and setting up PyTorch can be found at https://pytorch.org/get-started/locally/. Make sure you follow the instructions for your machine. For any of the remaining libraries used in this assignment:\n",
        "* We have provided a `hw8_requirements.txt` file on the homework web page. \n",
        "* Download this file, and in the same directory you can run `pip3 install -r hw8_requirements.txt`\n",
        "â€‹\n",
        "Check that PyTorch installed correctly by running the following:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IaJ6BLrAJeCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0230b66e-2e76-42bc-ae3b-65fc21e14106"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.rand(5, 3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5386, 0.7195, 0.6149],\n",
              "        [0.3394, 0.2004, 0.5501],\n",
              "        [0.7276, 0.2069, 0.0275],\n",
              "        [0.0437, 0.7067, 0.8479],\n",
              "        [0.1983, 0.4461, 0.1958]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nEmmCk1IJiGn"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 0 Imports and Basic Setup  (5 Points)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y3mTddKuXPBU"
      },
      "cell_type": "markdown",
      "source": [
        "First, import the required libraries as follows. The libraries we will use will be the same as those in HW8. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UwtDsq3VbrNY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QWKp_UGlWTyR"
      },
      "cell_type": "markdown",
      "source": [
        "**GPU Support**\n",
        "\n",
        "Training of large network can take a long time. PyTorch supports GPU with just a small amount of effort.\n",
        "\n",
        "When creating our networks, we will call \n",
        "`net.to(device)` to tell the network to train on the GPU, if one is available.  Note, if the network utilizes the GPU, it is important that any tensors we use with it (such as the data) also reside on the CPU.  Thus, a call like `images = images.to(device)` is necessary with any data we want to use with the GPU.\n",
        "\n",
        "Note: If you can't get access to a GPU, don't worry to much.  Since we use very small networks, the difference between CPU and GPU isn't large and in some cases GPU will actually be slower."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gbGGmnIXYca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7d7a498-bdbc-4fc4-a1ee-7ee3c63f7e9e"
      },
      "cell_type": "code",
      "source": [
        "import torch.cuda as cuda\n",
        "\n",
        "# Use a GPU, i.e. cuda:0 device if it available.\n",
        "device = torch.device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_lzWqTRlo0X0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Code"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_DZm8ammPCbL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "  \"\"\"NN Module that flattens the incoming tensor.\"\"\"\n",
        "  def forward(self, input):\n",
        "    return input.view(input.size(0), -1)\n",
        "  \n",
        "def train(model, train_loader, test_loader, loss_func, opt, num_epochs=10):\n",
        "  all_training_loss = np.zeros((0,2))\n",
        "  all_training_acc = np.zeros((0,2))\n",
        "  all_test_loss = np.zeros((0,2))\n",
        "  all_test_acc = np.zeros((0,2))\n",
        "  \n",
        "  training_step = 0\n",
        "  training_loss, training_acc = 2.0, 0.0\n",
        "  print_every = 1000\n",
        "  \n",
        "  start = time.clock()\n",
        "  \n",
        "  for i in range(num_epochs):\n",
        "    epoch_start = time.clock() \n",
        "   \n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      opt.zero_grad()\n",
        "\n",
        "      preds = model(images)\n",
        "      loss = loss_func(preds, labels)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      \n",
        "      training_loss += loss.item()\n",
        "      training_acc += (torch.argmax(preds, dim=1)==labels).float().mean()\n",
        "      \n",
        "      if training_step % print_every == 0:\n",
        "        training_loss /= print_every\n",
        "        training_acc /= print_every\n",
        "        \n",
        "        all_training_loss = np.concatenate((all_training_loss, [[training_step, training_loss]]))\n",
        "        all_training_acc = np.concatenate((all_training_acc, [[training_step, training_acc]]))\n",
        "        \n",
        "        print('  Epoch %d @ step %d: Train Loss: %3f, Train Accuracy: %3f' % (\n",
        "            i, training_step, training_loss, training_acc))\n",
        "        training_loss, training_acc = 0.0, 0.0\n",
        "        \n",
        "      training_step+=1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      validation_loss, validation_acc = 0.0, 0.0\n",
        "      count = 0\n",
        "      for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model(images)\n",
        "        validation_loss+=loss_func(output,labels)\n",
        "        validation_acc+=(torch.argmax(output, dim=1) == labels).float().mean()\n",
        "        count += 1\n",
        "      validation_loss/=count\n",
        "      validation_acc/=count\n",
        "      \n",
        "      all_test_loss = np.concatenate((all_test_loss, [[training_step, validation_loss]]))\n",
        "      all_test_acc = np.concatenate((all_test_acc, [[training_step, validation_acc]]))\n",
        "      \n",
        "      epoch_time = time.clock() - epoch_start\n",
        "      \n",
        "      print('Epoch %d Test Loss: %3f, Test Accuracy: %3f, time: %.1fs' % (\n",
        "          i, validation_loss, validation_acc, epoch_time))\n",
        "      \n",
        "  total_time = time.clock() - start\n",
        "  print('Final Test Loss: %3f, Test Accuracy: %3f, Total time: %.1fs' % (\n",
        "      validation_loss, validation_acc, total_time))\n",
        "\n",
        "  return {'loss': { 'train': all_training_loss, 'test': all_test_loss },\n",
        "          'accuracy': { 'train': all_training_acc, 'test': all_test_acc }}\n",
        "\n",
        "def plot_graphs(model_name, metrics):\n",
        "  for metric, values in metrics.items():\n",
        "    for name, v in values.items():\n",
        "      plt.plot(v[:,0], v[:,1], label=name)\n",
        "    plt.title(f'{metric} for {model_name}')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1RupXQbCaXb3"
      },
      "cell_type": "markdown",
      "source": [
        "Load the** CIFA-10** dataset and define the transformations. You may also want to print its structure, size, as well as sample a few images to get a sense of how to design the network. "
      ]
    },
    {
      "metadata": {
        "id": "mcWyqhGIU7NR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b7a0c9f6-abdf-4c90-f6ad-bd5f059c962b"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/AML/HW10/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mIJmLIgaZ_d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58cc9ba3-f102-4387-b8eb-78a7497a9ab0"
      },
      "cell_type": "code",
      "source": [
        "!mkdir hw10_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜hw10_dataâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aDBbPmPPaQuG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the data.\n",
        "from torchvision import datasets, transforms as T\n",
        "\n",
        "# transformations = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#train_set = datasets.CIFAR10(root='hw10_data/', download=True, transform=transformations)\n",
        "transform_augment = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(32, padding=4)])\n",
        "transform_normalize = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(root='hw10_data/', download=False, transform=T.Compose([transform_augment, transform_normalize]))\n",
        "test_set = datasets.CIFAR10(root='hw10_data', download=False, train=False, transform=transform_normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dfomsGyJiKz9"
      },
      "cell_type": "markdown",
      "source": [
        "Use `DataLoader` to create a loader for the training set and a loader for the testing set. You can use a `batch_size` of 8 to start, and change it if you wish."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ioe04mbSiQiV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#batch_size was 16 for most trials.\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "input_shape = np.array(train_set[0][0]).shape\n",
        "input_dim = input_shape[1]*input_shape[2]*input_shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fGoPdmuUOOiE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3T2-qkh8frqF"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 CIFAR10 with Fully Connected Neural Netowrk (25 Points)\n",
        "\n",
        "As a warm-up, let's begin by training a two-layer fully connected neural network model on ** CIFAR-10** dataset. You may go back to check HW8 for some basics.\n",
        "\n",
        "We will give you this code to use as a baseline to compare against your CNN models."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ck1CGpRycRFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8bf3cdc1-c852-477d-ec1e-e3375bd0031a"
      },
      "cell_type": "code",
      "source": [
        "class TwoLayerModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TwoLayerModel, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      Flatten(), \n",
        "      nn.Linear(input_dim, 64), \n",
        "      nn.ReLU(), \n",
        "      nn.Linear(64, 10))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "model = TwoLayerModel().to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# Training epoch should be about 15-20 sec each on GPU.\n",
        "metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Epoch 0 @ step 0: Train Loss: 0.004327, Train Accuracy: 0.000125\n",
            "  Epoch 0 @ step 1000: Train Loss: 1.997207, Train Accuracy: 0.276500\n",
            "  Epoch 0 @ step 2000: Train Loss: 1.907873, Train Accuracy: 0.301438\n",
            "  Epoch 0 @ step 3000: Train Loss: 1.881575, Train Accuracy: 0.314938\n",
            "Epoch 0 Test Loss: 1.780074, Test Accuracy: 0.368700, time: 9.3s\n",
            "  Epoch 1 @ step 4000: Train Loss: 1.864963, Train Accuracy: 0.321063\n",
            "  Epoch 1 @ step 5000: Train Loss: 1.882433, Train Accuracy: 0.315500\n",
            "  Epoch 1 @ step 6000: Train Loss: 1.861357, Train Accuracy: 0.319313\n",
            "Epoch 1 Test Loss: 1.758743, Test Accuracy: 0.357700, time: 9.1s\n",
            "  Epoch 2 @ step 7000: Train Loss: 1.861368, Train Accuracy: 0.328125\n",
            "  Epoch 2 @ step 8000: Train Loss: 1.859402, Train Accuracy: 0.326375\n",
            "  Epoch 2 @ step 9000: Train Loss: 1.870205, Train Accuracy: 0.316188\n",
            "Epoch 2 Test Loss: 1.791330, Test Accuracy: 0.341000, time: 9.1s\n",
            "  Epoch 3 @ step 10000: Train Loss: 1.851057, Train Accuracy: 0.322438\n",
            "  Epoch 3 @ step 11000: Train Loss: 1.854810, Train Accuracy: 0.325375\n",
            "  Epoch 3 @ step 12000: Train Loss: 1.858980, Train Accuracy: 0.329313\n",
            "Epoch 3 Test Loss: 1.767901, Test Accuracy: 0.366300, time: 9.0s\n",
            "  Epoch 4 @ step 13000: Train Loss: 1.864761, Train Accuracy: 0.321063\n",
            "  Epoch 4 @ step 14000: Train Loss: 1.848743, Train Accuracy: 0.326438\n",
            "  Epoch 4 @ step 15000: Train Loss: 1.859561, Train Accuracy: 0.323063\n",
            "Epoch 4 Test Loss: 1.782988, Test Accuracy: 0.346000, time: 10.2s\n",
            "Final Test Loss: 1.782988, Test Accuracy: 0.346000, Total time: 46.7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qjbncuMnKpf7"
      },
      "cell_type": "markdown",
      "source": [
        "**Plot the model results**\n",
        "\n",
        "Normally we would want to use Tensorboard for looking at metrics.  However, if colab reset while we are working, we might lose our logs and therefore our metrics.  Let's just plot some graphs that will survive across colab instances."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CuVL9MJ_D92h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "10f160b7-4b3f-4a60-e936-5f886ddb07c3"
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"TwoLayerModel\", metrics)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/f+oNcHzYntrwwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkkCJQUREEigxiIhIAiUGERFJ8P8B8TcHW/NHOXAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4_xi8bkkfsM7"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 Convolutional Neural Network (CNN) (35 Points)\n",
        "\n",
        "Now, let's design a convolution neural netwrok!\n",
        "\n",
        "Build a simple CNN model, inserting 2 CNN layers in from of our 2 layer fully connect model from above:\n",
        "\n",
        "1. A convolution with\t3x3 filter, 16 output channels, stride = 1, padding=1\n",
        "2. A ReLU activation\n",
        "2. A Max-Pooling layer with 2x2 window\n",
        "3. A convolution,\t3x3 filter, 16 output channels, stride = 1, padding=1\n",
        "4. A ReLU activation\n",
        "4. Flatten layer\n",
        "5. Fully connected linear layer with output size 64\n",
        "6. ReLU\n",
        "7. Fully connected linear layer, with output size 10\n",
        "\n",
        "You will have to figure out the input sizes of the first fully connnected layer based on the previous layer sizes. Note that you also need to fill those in the report section (see report section in the notebook for details) "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tLXjjywngO7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94163
        },
        "outputId": "0750cc6e-4f9d-4f69-c8e3-301c8d071668"
      },
      "cell_type": "code",
      "source": [
        "class ConvModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvModel, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      # 3 x 32 x 32\n",
        "      nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), \n",
        "      nn.ReLU(),\n",
        "      # 16 x 32 x 32\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "      # 16 x 16 x 16\n",
        "      nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      # 16 x 16 x 16\n",
        "      Flatten(),\n",
        "      # 65536\n",
        "      nn.Linear(16 * 16 * 16, 64), \n",
        "      nn.ReLU(), \n",
        "      nn.Linear(64, 10))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "  \n",
        "\n",
        "model = ConvModel().to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "#metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)\n",
        "metrics = train(model, train_loader, test_loader, loss, optimizer, 2160)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Epoch 0 @ step 0: Train Loss: 0.004320, Train Accuracy: 0.000156\n",
            "  Epoch 0 @ step 1000: Train Loss: 1.666413, Train Accuracy: 0.394344\n",
            "Epoch 0 Test Loss: 1.429174, Test Accuracy: 0.480731, time: 7.3s\n",
            "  Epoch 1 @ step 2000: Train Loss: 1.429471, Train Accuracy: 0.485906\n",
            "  Epoch 1 @ step 3000: Train Loss: 1.363743, Train Accuracy: 0.507969\n",
            "Epoch 1 Test Loss: 1.349894, Test Accuracy: 0.507388, time: 7.1s\n",
            "  Epoch 2 @ step 4000: Train Loss: 1.315544, Train Accuracy: 0.524312\n",
            "Epoch 2 Test Loss: 1.310443, Test Accuracy: 0.525958, time: 7.2s\n",
            "  Epoch 3 @ step 5000: Train Loss: 1.288123, Train Accuracy: 0.535594\n",
            "  Epoch 3 @ step 6000: Train Loss: 1.269364, Train Accuracy: 0.544906\n",
            "Epoch 3 Test Loss: 1.334845, Test Accuracy: 0.518371, time: 7.2s\n",
            "  Epoch 4 @ step 7000: Train Loss: 1.232066, Train Accuracy: 0.558906\n",
            "Epoch 4 Test Loss: 1.290115, Test Accuracy: 0.530551, time: 7.2s\n",
            "  Epoch 5 @ step 8000: Train Loss: 1.217211, Train Accuracy: 0.563406\n",
            "  Epoch 5 @ step 9000: Train Loss: 1.201212, Train Accuracy: 0.571313\n",
            "Epoch 5 Test Loss: 1.198660, Test Accuracy: 0.562800, time: 7.9s\n",
            "  Epoch 6 @ step 10000: Train Loss: 1.183919, Train Accuracy: 0.579469\n",
            "Epoch 6 Test Loss: 1.178393, Test Accuracy: 0.580671, time: 7.5s\n",
            "  Epoch 7 @ step 11000: Train Loss: 1.173989, Train Accuracy: 0.579344\n",
            "  Epoch 7 @ step 12000: Train Loss: 1.164531, Train Accuracy: 0.586844\n",
            "Epoch 7 Test Loss: 1.241890, Test Accuracy: 0.546825, time: 7.2s\n",
            "  Epoch 8 @ step 13000: Train Loss: 1.156709, Train Accuracy: 0.587156\n",
            "  Epoch 8 @ step 14000: Train Loss: 1.150238, Train Accuracy: 0.590406\n",
            "Epoch 8 Test Loss: 1.216557, Test Accuracy: 0.563299, time: 7.2s\n",
            "  Epoch 9 @ step 15000: Train Loss: 1.148360, Train Accuracy: 0.591594\n",
            "Epoch 9 Test Loss: 1.180496, Test Accuracy: 0.565096, time: 7.8s\n",
            "  Epoch 10 @ step 16000: Train Loss: 1.128491, Train Accuracy: 0.602594\n",
            "  Epoch 10 @ step 17000: Train Loss: 1.135148, Train Accuracy: 0.596625\n",
            "Epoch 10 Test Loss: 1.155487, Test Accuracy: 0.587360, time: 7.2s\n",
            "  Epoch 11 @ step 18000: Train Loss: 1.121483, Train Accuracy: 0.597844\n",
            "Epoch 11 Test Loss: 1.230042, Test Accuracy: 0.561502, time: 8.0s\n",
            "  Epoch 12 @ step 19000: Train Loss: 1.129016, Train Accuracy: 0.596500\n",
            "  Epoch 12 @ step 20000: Train Loss: 1.111951, Train Accuracy: 0.604594\n",
            "Epoch 12 Test Loss: 1.138187, Test Accuracy: 0.598542, time: 7.5s\n",
            "  Epoch 13 @ step 21000: Train Loss: 1.104329, Train Accuracy: 0.610563\n",
            "Epoch 13 Test Loss: 1.109141, Test Accuracy: 0.601737, time: 7.2s\n",
            "  Epoch 14 @ step 22000: Train Loss: 1.114579, Train Accuracy: 0.607250\n",
            "  Epoch 14 @ step 23000: Train Loss: 1.099109, Train Accuracy: 0.609469\n",
            "Epoch 14 Test Loss: 1.146912, Test Accuracy: 0.593151, time: 7.2s\n",
            "  Epoch 15 @ step 24000: Train Loss: 1.105818, Train Accuracy: 0.606594\n",
            "  Epoch 15 @ step 25000: Train Loss: 1.090638, Train Accuracy: 0.611406\n",
            "Epoch 15 Test Loss: 1.170304, Test Accuracy: 0.578175, time: 7.3s\n",
            "  Epoch 16 @ step 26000: Train Loss: 1.079753, Train Accuracy: 0.617250\n",
            "Epoch 16 Test Loss: 1.100829, Test Accuracy: 0.611821, time: 7.3s\n",
            "  Epoch 17 @ step 27000: Train Loss: 1.092157, Train Accuracy: 0.615281\n",
            "  Epoch 17 @ step 28000: Train Loss: 1.081010, Train Accuracy: 0.618469\n",
            "Epoch 17 Test Loss: 1.162889, Test Accuracy: 0.586661, time: 8.0s\n",
            "  Epoch 18 @ step 29000: Train Loss: 1.068052, Train Accuracy: 0.623000\n",
            "Epoch 18 Test Loss: 1.151987, Test Accuracy: 0.587760, time: 7.6s\n",
            "  Epoch 19 @ step 30000: Train Loss: 1.072620, Train Accuracy: 0.623031\n",
            "  Epoch 19 @ step 31000: Train Loss: 1.067748, Train Accuracy: 0.621875\n",
            "Epoch 19 Test Loss: 1.161055, Test Accuracy: 0.577975, time: 7.3s\n",
            "  Epoch 20 @ step 32000: Train Loss: 1.065286, Train Accuracy: 0.626125\n",
            "Epoch 20 Test Loss: 1.169603, Test Accuracy: 0.584764, time: 7.3s\n",
            "  Epoch 21 @ step 33000: Train Loss: 1.058084, Train Accuracy: 0.624656\n",
            "  Epoch 21 @ step 34000: Train Loss: 1.053742, Train Accuracy: 0.631094\n",
            "Epoch 21 Test Loss: 1.076221, Test Accuracy: 0.619609, time: 7.6s\n",
            "  Epoch 22 @ step 35000: Train Loss: 1.052781, Train Accuracy: 0.629156\n",
            "Epoch 22 Test Loss: 1.091821, Test Accuracy: 0.611022, time: 8.1s\n",
            "  Epoch 23 @ step 36000: Train Loss: 1.053780, Train Accuracy: 0.630313\n",
            "  Epoch 23 @ step 37000: Train Loss: 1.034699, Train Accuracy: 0.634906\n",
            "Epoch 23 Test Loss: 1.076044, Test Accuracy: 0.620108, time: 8.3s\n",
            "  Epoch 24 @ step 38000: Train Loss: 1.046798, Train Accuracy: 0.630531\n",
            "  Epoch 24 @ step 39000: Train Loss: 1.045313, Train Accuracy: 0.630500\n",
            "Epoch 24 Test Loss: 1.231992, Test Accuracy: 0.555411, time: 7.3s\n",
            "  Epoch 25 @ step 40000: Train Loss: 1.023808, Train Accuracy: 0.638000\n",
            "Epoch 25 Test Loss: 1.033723, Test Accuracy: 0.640076, time: 7.3s\n",
            "  Epoch 26 @ step 41000: Train Loss: 1.046602, Train Accuracy: 0.635781\n",
            "  Epoch 26 @ step 42000: Train Loss: 1.032056, Train Accuracy: 0.637281\n",
            "Epoch 26 Test Loss: 1.103847, Test Accuracy: 0.604133, time: 7.3s\n",
            "  Epoch 27 @ step 43000: Train Loss: 1.033263, Train Accuracy: 0.637906\n",
            "Epoch 27 Test Loss: 1.246734, Test Accuracy: 0.563199, time: 7.2s\n",
            "  Epoch 28 @ step 44000: Train Loss: 1.020284, Train Accuracy: 0.642438\n",
            "  Epoch 28 @ step 45000: Train Loss: 1.036098, Train Accuracy: 0.637531\n",
            "Epoch 28 Test Loss: 1.130775, Test Accuracy: 0.594649, time: 7.3s\n",
            "  Epoch 29 @ step 46000: Train Loss: 1.014018, Train Accuracy: 0.644469\n",
            "Epoch 29 Test Loss: 1.034288, Test Accuracy: 0.638279, time: 8.4s\n",
            "  Epoch 30 @ step 47000: Train Loss: 1.033787, Train Accuracy: 0.637094\n",
            "  Epoch 30 @ step 48000: Train Loss: 1.021811, Train Accuracy: 0.643031\n",
            "Epoch 30 Test Loss: 1.082304, Test Accuracy: 0.620707, time: 7.3s\n",
            "  Epoch 31 @ step 49000: Train Loss: 1.023602, Train Accuracy: 0.639906\n",
            "  Epoch 31 @ step 50000: Train Loss: 1.021600, Train Accuracy: 0.640969\n",
            "Epoch 31 Test Loss: 1.070167, Test Accuracy: 0.624201, time: 7.3s\n",
            "  Epoch 32 @ step 51000: Train Loss: 1.018528, Train Accuracy: 0.642531\n",
            "Epoch 32 Test Loss: 1.103247, Test Accuracy: 0.613219, time: 8.0s\n",
            "  Epoch 33 @ step 52000: Train Loss: 1.015872, Train Accuracy: 0.642969\n",
            "  Epoch 33 @ step 53000: Train Loss: 1.019942, Train Accuracy: 0.639844\n",
            "Epoch 33 Test Loss: 1.028710, Test Accuracy: 0.637480, time: 7.3s\n",
            "  Epoch 34 @ step 54000: Train Loss: 1.011415, Train Accuracy: 0.645688\n",
            "Epoch 34 Test Loss: 1.013272, Test Accuracy: 0.645567, time: 7.3s\n",
            "  Epoch 35 @ step 55000: Train Loss: 1.018957, Train Accuracy: 0.640094\n",
            "  Epoch 35 @ step 56000: Train Loss: 1.016123, Train Accuracy: 0.644156\n",
            "Epoch 35 Test Loss: 1.047624, Test Accuracy: 0.632089, time: 8.5s\n",
            "  Epoch 36 @ step 57000: Train Loss: 1.010433, Train Accuracy: 0.646188\n",
            "Epoch 36 Test Loss: 1.133507, Test Accuracy: 0.601138, time: 7.3s\n",
            "  Epoch 37 @ step 58000: Train Loss: 1.017039, Train Accuracy: 0.640781\n",
            "  Epoch 37 @ step 59000: Train Loss: 1.007408, Train Accuracy: 0.645031\n",
            "Epoch 37 Test Loss: 1.032026, Test Accuracy: 0.644569, time: 7.3s\n",
            "  Epoch 38 @ step 60000: Train Loss: 1.006548, Train Accuracy: 0.646000\n",
            "Epoch 38 Test Loss: 1.016730, Test Accuracy: 0.643470, time: 7.3s\n",
            "  Epoch 39 @ step 61000: Train Loss: 1.017062, Train Accuracy: 0.643281\n",
            "  Epoch 39 @ step 62000: Train Loss: 1.004049, Train Accuracy: 0.646125\n",
            "Epoch 39 Test Loss: 1.031251, Test Accuracy: 0.640575, time: 7.3s\n",
            "  Epoch 40 @ step 63000: Train Loss: 1.010816, Train Accuracy: 0.644906\n",
            "  Epoch 40 @ step 64000: Train Loss: 1.009626, Train Accuracy: 0.645031\n",
            "Epoch 40 Test Loss: 1.169724, Test Accuracy: 0.598942, time: 7.3s\n",
            "  Epoch 41 @ step 65000: Train Loss: 1.000912, Train Accuracy: 0.650813\n",
            "Epoch 41 Test Loss: 1.030890, Test Accuracy: 0.644569, time: 8.4s\n",
            "  Epoch 42 @ step 66000: Train Loss: 1.007052, Train Accuracy: 0.645625\n",
            "  Epoch 42 @ step 67000: Train Loss: 1.012300, Train Accuracy: 0.643531\n",
            "Epoch 42 Test Loss: 1.080839, Test Accuracy: 0.611921, time: 7.3s\n",
            "  Epoch 43 @ step 68000: Train Loss: 0.997112, Train Accuracy: 0.648188\n",
            "Epoch 43 Test Loss: 1.032775, Test Accuracy: 0.634485, time: 7.3s\n",
            "  Epoch 44 @ step 69000: Train Loss: 1.012504, Train Accuracy: 0.646750\n",
            "  Epoch 44 @ step 70000: Train Loss: 1.001837, Train Accuracy: 0.647688\n",
            "Epoch 44 Test Loss: 1.064272, Test Accuracy: 0.619409, time: 7.9s\n",
            "  Epoch 45 @ step 71000: Train Loss: 0.998396, Train Accuracy: 0.649594\n",
            "Epoch 45 Test Loss: 1.102479, Test Accuracy: 0.601637, time: 8.1s\n",
            "  Epoch 46 @ step 72000: Train Loss: 1.008797, Train Accuracy: 0.643906\n",
            "  Epoch 46 @ step 73000: Train Loss: 1.004235, Train Accuracy: 0.648469\n",
            "Epoch 46 Test Loss: 1.041399, Test Accuracy: 0.631090, time: 7.4s\n",
            "  Epoch 47 @ step 74000: Train Loss: 0.997656, Train Accuracy: 0.650813\n",
            "  Epoch 47 @ step 75000: Train Loss: 1.014674, Train Accuracy: 0.645063\n",
            "Epoch 47 Test Loss: 1.048246, Test Accuracy: 0.628095, time: 8.6s\n",
            "  Epoch 48 @ step 76000: Train Loss: 0.996792, Train Accuracy: 0.650719\n",
            "Epoch 48 Test Loss: 1.096335, Test Accuracy: 0.614417, time: 7.3s\n",
            "  Epoch 49 @ step 77000: Train Loss: 1.001835, Train Accuracy: 0.648781\n",
            "  Epoch 49 @ step 78000: Train Loss: 1.008329, Train Accuracy: 0.645844\n",
            "Epoch 49 Test Loss: 1.026348, Test Accuracy: 0.634984, time: 7.3s\n",
            "  Epoch 50 @ step 79000: Train Loss: 0.992240, Train Accuracy: 0.656250\n",
            "Epoch 50 Test Loss: 1.143010, Test Accuracy: 0.602336, time: 7.3s\n",
            "  Epoch 51 @ step 80000: Train Loss: 1.013727, Train Accuracy: 0.642500\n",
            "  Epoch 51 @ step 81000: Train Loss: 1.001793, Train Accuracy: 0.648875\n",
            "Epoch 51 Test Loss: 0.969686, Test Accuracy: 0.659744, time: 7.3s\n",
            "  Epoch 52 @ step 82000: Train Loss: 0.993545, Train Accuracy: 0.650094\n",
            "Epoch 52 Test Loss: 1.055511, Test Accuracy: 0.624401, time: 7.3s\n",
            "  Epoch 53 @ step 83000: Train Loss: 1.004252, Train Accuracy: 0.645438\n",
            "  Epoch 53 @ step 84000: Train Loss: 0.998109, Train Accuracy: 0.647594\n",
            "Epoch 53 Test Loss: 1.057681, Test Accuracy: 0.627296, time: 8.6s\n",
            "  Epoch 54 @ step 85000: Train Loss: 0.998860, Train Accuracy: 0.650688\n",
            "Epoch 54 Test Loss: 1.003151, Test Accuracy: 0.641374, time: 7.9s\n",
            "  Epoch 55 @ step 86000: Train Loss: 1.004742, Train Accuracy: 0.650125\n",
            "  Epoch 55 @ step 87000: Train Loss: 0.997333, Train Accuracy: 0.649781\n",
            "Epoch 55 Test Loss: 1.070399, Test Accuracy: 0.622604, time: 7.3s\n",
            "  Epoch 56 @ step 88000: Train Loss: 1.000810, Train Accuracy: 0.649938\n",
            "  Epoch 56 @ step 89000: Train Loss: 0.995235, Train Accuracy: 0.651406\n",
            "Epoch 56 Test Loss: 0.985836, Test Accuracy: 0.659345, time: 7.3s\n",
            "  Epoch 57 @ step 90000: Train Loss: 1.001320, Train Accuracy: 0.649063\n",
            "Epoch 57 Test Loss: 0.980833, Test Accuracy: 0.653255, time: 7.3s\n",
            "  Epoch 58 @ step 91000: Train Loss: 0.994834, Train Accuracy: 0.649594\n",
            "  Epoch 58 @ step 92000: Train Loss: 1.001193, Train Accuracy: 0.648156\n",
            "Epoch 58 Test Loss: 1.064973, Test Accuracy: 0.616214, time: 7.3s\n",
            "  Epoch 59 @ step 93000: Train Loss: 0.991894, Train Accuracy: 0.656375\n",
            "Epoch 59 Test Loss: 1.053930, Test Accuracy: 0.631789, time: 8.5s\n",
            "  Epoch 60 @ step 94000: Train Loss: 0.995724, Train Accuracy: 0.649969\n",
            "  Epoch 60 @ step 95000: Train Loss: 0.999640, Train Accuracy: 0.647656\n",
            "Epoch 60 Test Loss: 1.089413, Test Accuracy: 0.612420, time: 7.3s\n",
            "  Epoch 61 @ step 96000: Train Loss: 0.992979, Train Accuracy: 0.652313\n",
            "Epoch 61 Test Loss: 1.089812, Test Accuracy: 0.618610, time: 7.3s\n",
            "  Epoch 62 @ step 97000: Train Loss: 0.999171, Train Accuracy: 0.649063\n",
            "  Epoch 62 @ step 98000: Train Loss: 0.994459, Train Accuracy: 0.651094\n",
            "Epoch 62 Test Loss: 1.096318, Test Accuracy: 0.601737, time: 7.4s\n",
            "  Epoch 63 @ step 99000: Train Loss: 1.000552, Train Accuracy: 0.647719\n",
            "  Epoch 63 @ step 100000: Train Loss: 0.998220, Train Accuracy: 0.652844\n",
            "Epoch 63 Test Loss: 1.159773, Test Accuracy: 0.581669, time: 7.3s\n",
            "  Epoch 64 @ step 101000: Train Loss: 0.992795, Train Accuracy: 0.653063\n",
            "Epoch 64 Test Loss: 1.093279, Test Accuracy: 0.610623, time: 7.4s\n",
            "  Epoch 65 @ step 102000: Train Loss: 0.998317, Train Accuracy: 0.650000\n",
            "  Epoch 65 @ step 103000: Train Loss: 1.001900, Train Accuracy: 0.647688\n",
            "Epoch 65 Test Loss: 1.023653, Test Accuracy: 0.642272, time: 8.6s\n",
            "  Epoch 66 @ step 104000: Train Loss: 0.992774, Train Accuracy: 0.652313\n",
            "Epoch 66 Test Loss: 1.065023, Test Accuracy: 0.630092, time: 7.3s\n",
            "  Epoch 67 @ step 105000: Train Loss: 0.993837, Train Accuracy: 0.649844\n",
            "  Epoch 67 @ step 106000: Train Loss: 1.003864, Train Accuracy: 0.646406\n",
            "Epoch 67 Test Loss: 1.059855, Test Accuracy: 0.628395, time: 8.0s\n",
            "  Epoch 68 @ step 107000: Train Loss: 0.984463, Train Accuracy: 0.655219\n",
            "Epoch 68 Test Loss: 1.033005, Test Accuracy: 0.623303, time: 7.9s\n",
            "  Epoch 69 @ step 108000: Train Loss: 0.998850, Train Accuracy: 0.647719\n",
            "  Epoch 69 @ step 109000: Train Loss: 0.989412, Train Accuracy: 0.652563\n",
            "Epoch 69 Test Loss: 1.062526, Test Accuracy: 0.630391, time: 7.3s\n",
            "  Epoch 70 @ step 110000: Train Loss: 0.992744, Train Accuracy: 0.651375\n",
            "Epoch 70 Test Loss: 1.101700, Test Accuracy: 0.608227, time: 7.4s\n",
            "  Epoch 71 @ step 111000: Train Loss: 1.005489, Train Accuracy: 0.648625\n",
            "  Epoch 71 @ step 112000: Train Loss: 0.988000, Train Accuracy: 0.655656\n",
            "Epoch 71 Test Loss: 1.062214, Test Accuracy: 0.627796, time: 8.5s\n",
            "  Epoch 72 @ step 113000: Train Loss: 0.998681, Train Accuracy: 0.648531\n",
            "  Epoch 72 @ step 114000: Train Loss: 1.000466, Train Accuracy: 0.648375\n",
            "Epoch 72 Test Loss: 1.030450, Test Accuracy: 0.637580, time: 7.3s\n",
            "  Epoch 73 @ step 115000: Train Loss: 0.993636, Train Accuracy: 0.653000\n",
            "Epoch 73 Test Loss: 1.046113, Test Accuracy: 0.629992, time: 7.3s\n",
            "  Epoch 74 @ step 116000: Train Loss: 0.996814, Train Accuracy: 0.649906\n",
            "  Epoch 74 @ step 117000: Train Loss: 0.998108, Train Accuracy: 0.649750\n",
            "Epoch 74 Test Loss: 1.034384, Test Accuracy: 0.635284, time: 7.3s\n",
            "  Epoch 75 @ step 118000: Train Loss: 0.990083, Train Accuracy: 0.650906\n",
            "Epoch 75 Test Loss: 1.034316, Test Accuracy: 0.627796, time: 7.3s\n",
            "  Epoch 76 @ step 119000: Train Loss: 0.999198, Train Accuracy: 0.648156\n",
            "  Epoch 76 @ step 120000: Train Loss: 0.990660, Train Accuracy: 0.651094\n",
            "Epoch 76 Test Loss: 1.050402, Test Accuracy: 0.628295, time: 7.3s\n",
            "  Epoch 77 @ step 121000: Train Loss: 0.994974, Train Accuracy: 0.653375\n",
            "Epoch 77 Test Loss: 1.019479, Test Accuracy: 0.645268, time: 8.6s\n",
            "  Epoch 78 @ step 122000: Train Loss: 0.998766, Train Accuracy: 0.648594\n",
            "  Epoch 78 @ step 123000: Train Loss: 0.989700, Train Accuracy: 0.652219\n",
            "Epoch 78 Test Loss: 1.098718, Test Accuracy: 0.607827, time: 7.3s\n",
            "  Epoch 79 @ step 124000: Train Loss: 1.000052, Train Accuracy: 0.649438\n",
            "  Epoch 79 @ step 125000: Train Loss: 1.000484, Train Accuracy: 0.647844\n",
            "Epoch 79 Test Loss: 1.060515, Test Accuracy: 0.629593, time: 7.3s\n",
            "  Epoch 80 @ step 126000: Train Loss: 0.993036, Train Accuracy: 0.653375\n",
            "Epoch 80 Test Loss: 1.032894, Test Accuracy: 0.640775, time: 7.3s\n",
            "  Epoch 81 @ step 127000: Train Loss: 0.988739, Train Accuracy: 0.650313\n",
            "  Epoch 81 @ step 128000: Train Loss: 0.998756, Train Accuracy: 0.648344\n",
            "Epoch 81 Test Loss: 1.015006, Test Accuracy: 0.639077, time: 7.4s\n",
            "  Epoch 82 @ step 129000: Train Loss: 0.994445, Train Accuracy: 0.651000\n",
            "Epoch 82 Test Loss: 1.013848, Test Accuracy: 0.637780, time: 7.3s\n",
            "  Epoch 83 @ step 130000: Train Loss: 0.994018, Train Accuracy: 0.651406\n",
            "  Epoch 83 @ step 131000: Train Loss: 0.993595, Train Accuracy: 0.652563\n",
            "Epoch 83 Test Loss: 1.084746, Test Accuracy: 0.611522, time: 8.6s\n",
            "  Epoch 84 @ step 132000: Train Loss: 0.990265, Train Accuracy: 0.652156\n",
            "Epoch 84 Test Loss: 1.081135, Test Accuracy: 0.624401, time: 7.4s\n",
            "  Epoch 85 @ step 133000: Train Loss: 0.999194, Train Accuracy: 0.647625\n",
            "  Epoch 85 @ step 134000: Train Loss: 0.992988, Train Accuracy: 0.649281\n",
            "Epoch 85 Test Loss: 0.991334, Test Accuracy: 0.655551, time: 7.3s\n",
            "  Epoch 86 @ step 135000: Train Loss: 0.988975, Train Accuracy: 0.653063\n",
            "Epoch 86 Test Loss: 1.298860, Test Accuracy: 0.544129, time: 7.4s\n",
            "  Epoch 87 @ step 136000: Train Loss: 0.999148, Train Accuracy: 0.650906\n",
            "  Epoch 87 @ step 137000: Train Loss: 0.992780, Train Accuracy: 0.652688\n",
            "Epoch 87 Test Loss: 1.032435, Test Accuracy: 0.632688, time: 7.4s\n",
            "  Epoch 88 @ step 138000: Train Loss: 0.989083, Train Accuracy: 0.652906\n",
            "  Epoch 88 @ step 139000: Train Loss: 1.001464, Train Accuracy: 0.647250\n",
            "Epoch 88 Test Loss: 1.010542, Test Accuracy: 0.647963, time: 7.3s\n",
            "  Epoch 89 @ step 140000: Train Loss: 0.995835, Train Accuracy: 0.650000\n",
            "Epoch 89 Test Loss: 1.014400, Test Accuracy: 0.640375, time: 8.5s\n",
            "  Epoch 90 @ step 141000: Train Loss: 0.986449, Train Accuracy: 0.652031\n",
            "  Epoch 90 @ step 142000: Train Loss: 1.000562, Train Accuracy: 0.648313\n",
            "Epoch 90 Test Loss: 1.171992, Test Accuracy: 0.581270, time: 8.0s\n",
            "  Epoch 91 @ step 143000: Train Loss: 0.984743, Train Accuracy: 0.655344\n",
            "Epoch 91 Test Loss: 1.008618, Test Accuracy: 0.646865, time: 7.8s\n",
            "  Epoch 92 @ step 144000: Train Loss: 1.004309, Train Accuracy: 0.649281\n",
            "  Epoch 92 @ step 145000: Train Loss: 0.988019, Train Accuracy: 0.650406\n",
            "Epoch 92 Test Loss: 1.091993, Test Accuracy: 0.609026, time: 7.3s\n",
            "  Epoch 93 @ step 146000: Train Loss: 0.992254, Train Accuracy: 0.652094\n",
            "Epoch 93 Test Loss: 1.065619, Test Accuracy: 0.625699, time: 7.3s\n",
            "  Epoch 94 @ step 147000: Train Loss: 1.003316, Train Accuracy: 0.646719\n",
            "  Epoch 94 @ step 148000: Train Loss: 0.987015, Train Accuracy: 0.655031\n",
            "Epoch 94 Test Loss: 1.027435, Test Accuracy: 0.641374, time: 7.6s\n",
            "  Epoch 95 @ step 149000: Train Loss: 0.998469, Train Accuracy: 0.649281\n",
            "  Epoch 95 @ step 150000: Train Loss: 0.995954, Train Accuracy: 0.649750\n",
            "Epoch 95 Test Loss: 1.032841, Test Accuracy: 0.629593, time: 8.1s\n",
            "  Epoch 96 @ step 151000: Train Loss: 0.988089, Train Accuracy: 0.655219\n",
            "Epoch 96 Test Loss: 1.024485, Test Accuracy: 0.634884, time: 7.4s\n",
            "  Epoch 97 @ step 152000: Train Loss: 0.999438, Train Accuracy: 0.649031\n",
            "  Epoch 97 @ step 153000: Train Loss: 0.993938, Train Accuracy: 0.652094\n",
            "Epoch 97 Test Loss: 1.055538, Test Accuracy: 0.608127, time: 7.4s\n",
            "  Epoch 98 @ step 154000: Train Loss: 0.980187, Train Accuracy: 0.654031\n",
            "Epoch 98 Test Loss: 1.010358, Test Accuracy: 0.640375, time: 7.3s\n",
            "  Epoch 99 @ step 155000: Train Loss: 1.000924, Train Accuracy: 0.648000\n",
            "  Epoch 99 @ step 156000: Train Loss: 0.995297, Train Accuracy: 0.649031\n",
            "Epoch 99 Test Loss: 1.060006, Test Accuracy: 0.632488, time: 7.6s\n",
            "  Epoch 100 @ step 157000: Train Loss: 0.990533, Train Accuracy: 0.651594\n",
            "Epoch 100 Test Loss: 1.120476, Test Accuracy: 0.595847, time: 8.1s\n",
            "  Epoch 101 @ step 158000: Train Loss: 0.995754, Train Accuracy: 0.649938\n",
            "  Epoch 101 @ step 159000: Train Loss: 0.992370, Train Accuracy: 0.653750\n",
            "Epoch 101 Test Loss: 1.018484, Test Accuracy: 0.645467, time: 8.0s\n",
            "  Epoch 102 @ step 160000: Train Loss: 0.983202, Train Accuracy: 0.654188\n",
            "Epoch 102 Test Loss: 1.085150, Test Accuracy: 0.612121, time: 7.3s\n",
            "  Epoch 103 @ step 161000: Train Loss: 1.000446, Train Accuracy: 0.649406\n",
            "  Epoch 103 @ step 162000: Train Loss: 0.987172, Train Accuracy: 0.655094\n",
            "Epoch 103 Test Loss: 1.089904, Test Accuracy: 0.606829, time: 7.4s\n",
            "  Epoch 104 @ step 163000: Train Loss: 0.993686, Train Accuracy: 0.648844\n",
            "  Epoch 104 @ step 164000: Train Loss: 0.995227, Train Accuracy: 0.652125\n",
            "Epoch 104 Test Loss: 0.986692, Test Accuracy: 0.658347, time: 7.4s\n",
            "  Epoch 105 @ step 165000: Train Loss: 0.979683, Train Accuracy: 0.653813\n",
            "Epoch 105 Test Loss: 1.022415, Test Accuracy: 0.644169, time: 7.3s\n",
            "  Epoch 106 @ step 166000: Train Loss: 0.996896, Train Accuracy: 0.651344\n",
            "  Epoch 106 @ step 167000: Train Loss: 0.997472, Train Accuracy: 0.648313\n",
            "Epoch 106 Test Loss: 1.045997, Test Accuracy: 0.629892, time: 7.8s\n",
            "  Epoch 107 @ step 168000: Train Loss: 0.986268, Train Accuracy: 0.656000\n",
            "Epoch 107 Test Loss: 1.042364, Test Accuracy: 0.631090, time: 8.0s\n",
            "  Epoch 108 @ step 169000: Train Loss: 0.995886, Train Accuracy: 0.653344\n",
            "  Epoch 108 @ step 170000: Train Loss: 0.993373, Train Accuracy: 0.652500\n",
            "Epoch 108 Test Loss: 1.106530, Test Accuracy: 0.604433, time: 7.4s\n",
            "  Epoch 109 @ step 171000: Train Loss: 0.990833, Train Accuracy: 0.652031\n",
            "Epoch 109 Test Loss: 1.007327, Test Accuracy: 0.648163, time: 7.4s\n",
            "  Epoch 110 @ step 172000: Train Loss: 0.990069, Train Accuracy: 0.652813\n",
            "  Epoch 110 @ step 173000: Train Loss: 0.984657, Train Accuracy: 0.656531\n",
            "Epoch 110 Test Loss: 1.067357, Test Accuracy: 0.616014, time: 7.5s\n",
            "  Epoch 111 @ step 174000: Train Loss: 0.989743, Train Accuracy: 0.654250\n",
            "  Epoch 111 @ step 175000: Train Loss: 0.994110, Train Accuracy: 0.649438\n",
            "Epoch 111 Test Loss: 1.037300, Test Accuracy: 0.630092, time: 7.5s\n",
            "  Epoch 112 @ step 176000: Train Loss: 0.986721, Train Accuracy: 0.655688\n",
            "Epoch 112 Test Loss: 1.009827, Test Accuracy: 0.641673, time: 8.0s\n",
            "  Epoch 113 @ step 177000: Train Loss: 0.988392, Train Accuracy: 0.652313\n",
            "  Epoch 113 @ step 178000: Train Loss: 0.996619, Train Accuracy: 0.652344\n",
            "Epoch 113 Test Loss: 1.028587, Test Accuracy: 0.637979, time: 9.1s\n",
            "  Epoch 114 @ step 179000: Train Loss: 0.986544, Train Accuracy: 0.654063\n",
            "Epoch 114 Test Loss: 1.005828, Test Accuracy: 0.647764, time: 7.6s\n",
            "  Epoch 115 @ step 180000: Train Loss: 0.988841, Train Accuracy: 0.652500\n",
            "  Epoch 115 @ step 181000: Train Loss: 0.987901, Train Accuracy: 0.655719\n",
            "Epoch 115 Test Loss: 0.984926, Test Accuracy: 0.654852, time: 7.3s\n",
            "  Epoch 116 @ step 182000: Train Loss: 0.992169, Train Accuracy: 0.651000\n",
            "Epoch 116 Test Loss: 1.033568, Test Accuracy: 0.635783, time: 7.3s\n",
            "  Epoch 117 @ step 183000: Train Loss: 0.995385, Train Accuracy: 0.651656\n",
            "  Epoch 117 @ step 184000: Train Loss: 0.978828, Train Accuracy: 0.655125\n",
            "Epoch 117 Test Loss: 1.092279, Test Accuracy: 0.613019, time: 7.4s\n",
            "  Epoch 118 @ step 185000: Train Loss: 0.991129, Train Accuracy: 0.654656\n",
            "Epoch 118 Test Loss: 1.107676, Test Accuracy: 0.607129, time: 8.3s\n",
            "  Epoch 119 @ step 186000: Train Loss: 0.999428, Train Accuracy: 0.646563\n",
            "  Epoch 119 @ step 187000: Train Loss: 0.987620, Train Accuracy: 0.654406\n",
            "Epoch 119 Test Loss: 1.037194, Test Accuracy: 0.635982, time: 7.5s\n",
            "  Epoch 120 @ step 188000: Train Loss: 0.986900, Train Accuracy: 0.652625\n",
            "  Epoch 120 @ step 189000: Train Loss: 0.987096, Train Accuracy: 0.652688\n",
            "Epoch 120 Test Loss: 1.035655, Test Accuracy: 0.639177, time: 7.3s\n",
            "  Epoch 121 @ step 190000: Train Loss: 0.988715, Train Accuracy: 0.651875\n",
            "Epoch 121 Test Loss: 1.004056, Test Accuracy: 0.648263, time: 7.3s\n",
            "  Epoch 122 @ step 191000: Train Loss: 0.986723, Train Accuracy: 0.653406\n",
            "  Epoch 122 @ step 192000: Train Loss: 0.993374, Train Accuracy: 0.651313\n",
            "Epoch 122 Test Loss: 1.021445, Test Accuracy: 0.637280, time: 8.0s\n",
            "  Epoch 123 @ step 193000: Train Loss: 0.987907, Train Accuracy: 0.654281\n",
            "Epoch 123 Test Loss: 1.048742, Test Accuracy: 0.626697, time: 7.3s\n",
            "  Epoch 124 @ step 194000: Train Loss: 0.990660, Train Accuracy: 0.653125\n",
            "  Epoch 124 @ step 195000: Train Loss: 0.983151, Train Accuracy: 0.656156\n",
            "Epoch 124 Test Loss: 1.073435, Test Accuracy: 0.628894, time: 8.5s\n",
            "  Epoch 125 @ step 196000: Train Loss: 0.995446, Train Accuracy: 0.650750\n",
            "Epoch 125 Test Loss: 0.993950, Test Accuracy: 0.655551, time: 7.3s\n",
            "  Epoch 126 @ step 197000: Train Loss: 0.988147, Train Accuracy: 0.652906\n",
            "  Epoch 126 @ step 198000: Train Loss: 0.984420, Train Accuracy: 0.654719\n",
            "Epoch 126 Test Loss: 1.143850, Test Accuracy: 0.609824, time: 7.3s\n",
            "  Epoch 127 @ step 199000: Train Loss: 0.989244, Train Accuracy: 0.653500\n",
            "  Epoch 127 @ step 200000: Train Loss: 0.994046, Train Accuracy: 0.649156\n",
            "Epoch 127 Test Loss: 1.054741, Test Accuracy: 0.616314, time: 7.3s\n",
            "  Epoch 128 @ step 201000: Train Loss: 0.972687, Train Accuracy: 0.657813\n",
            "Epoch 128 Test Loss: 0.985754, Test Accuracy: 0.661741, time: 7.4s\n",
            "  Epoch 129 @ step 202000: Train Loss: 0.988731, Train Accuracy: 0.654625\n",
            "  Epoch 129 @ step 203000: Train Loss: 0.993735, Train Accuracy: 0.652594\n",
            "Epoch 129 Test Loss: 1.008676, Test Accuracy: 0.644569, time: 7.4s\n",
            "  Epoch 130 @ step 204000: Train Loss: 0.989801, Train Accuracy: 0.654125\n",
            "Epoch 130 Test Loss: 1.136834, Test Accuracy: 0.605831, time: 8.5s\n",
            "  Epoch 131 @ step 205000: Train Loss: 0.986494, Train Accuracy: 0.652313\n",
            "  Epoch 131 @ step 206000: Train Loss: 0.982176, Train Accuracy: 0.655406\n",
            "Epoch 131 Test Loss: 1.031454, Test Accuracy: 0.641174, time: 7.4s\n",
            "  Epoch 132 @ step 207000: Train Loss: 0.986870, Train Accuracy: 0.656281\n",
            "Epoch 132 Test Loss: 1.046705, Test Accuracy: 0.629593, time: 7.4s\n",
            "  Epoch 133 @ step 208000: Train Loss: 0.985925, Train Accuracy: 0.652063\n",
            "  Epoch 133 @ step 209000: Train Loss: 0.984050, Train Accuracy: 0.655313\n",
            "Epoch 133 Test Loss: 1.113015, Test Accuracy: 0.607228, time: 7.4s\n",
            "  Epoch 134 @ step 210000: Train Loss: 0.996698, Train Accuracy: 0.649250\n",
            "  Epoch 134 @ step 211000: Train Loss: 0.987831, Train Accuracy: 0.655250\n",
            "Epoch 134 Test Loss: 1.100587, Test Accuracy: 0.605531, time: 7.3s\n",
            "  Epoch 135 @ step 212000: Train Loss: 0.989960, Train Accuracy: 0.655875\n",
            "Epoch 135 Test Loss: 1.011719, Test Accuracy: 0.645068, time: 7.4s\n",
            "  Epoch 136 @ step 213000: Train Loss: 0.975774, Train Accuracy: 0.659063\n",
            "  Epoch 136 @ step 214000: Train Loss: 0.995908, Train Accuracy: 0.651375\n",
            "Epoch 136 Test Loss: 1.003854, Test Accuracy: 0.656050, time: 8.7s\n",
            "  Epoch 137 @ step 215000: Train Loss: 0.988029, Train Accuracy: 0.653375\n",
            "Epoch 137 Test Loss: 1.022503, Test Accuracy: 0.640375, time: 7.4s\n",
            "  Epoch 138 @ step 216000: Train Loss: 0.976204, Train Accuracy: 0.656438\n",
            "  Epoch 138 @ step 217000: Train Loss: 0.993467, Train Accuracy: 0.650031\n",
            "Epoch 138 Test Loss: 1.167337, Test Accuracy: 0.595547, time: 7.4s\n",
            "  Epoch 139 @ step 218000: Train Loss: 0.978772, Train Accuracy: 0.657281\n",
            "Epoch 139 Test Loss: 1.064650, Test Accuracy: 0.630192, time: 7.4s\n",
            "  Epoch 140 @ step 219000: Train Loss: 0.989896, Train Accuracy: 0.652344\n",
            "  Epoch 140 @ step 220000: Train Loss: 0.985769, Train Accuracy: 0.654625\n",
            "Epoch 140 Test Loss: 0.990350, Test Accuracy: 0.658746, time: 7.4s\n",
            "  Epoch 141 @ step 221000: Train Loss: 0.985551, Train Accuracy: 0.655438\n",
            "Epoch 141 Test Loss: 1.147036, Test Accuracy: 0.601937, time: 7.3s\n",
            "  Epoch 142 @ step 222000: Train Loss: 0.996101, Train Accuracy: 0.649656\n",
            "  Epoch 142 @ step 223000: Train Loss: 0.988554, Train Accuracy: 0.653688\n",
            "Epoch 142 Test Loss: 0.985785, Test Accuracy: 0.657548, time: 8.6s\n",
            "  Epoch 143 @ step 224000: Train Loss: 0.979837, Train Accuracy: 0.656094\n",
            "  Epoch 143 @ step 225000: Train Loss: 0.986782, Train Accuracy: 0.656438\n",
            "Epoch 143 Test Loss: 1.051471, Test Accuracy: 0.628794, time: 7.3s\n",
            "  Epoch 144 @ step 226000: Train Loss: 0.978156, Train Accuracy: 0.658281\n",
            "Epoch 144 Test Loss: 1.040890, Test Accuracy: 0.631390, time: 7.7s\n",
            "  Epoch 145 @ step 227000: Train Loss: 0.985325, Train Accuracy: 0.654250\n",
            "  Epoch 145 @ step 228000: Train Loss: 0.983855, Train Accuracy: 0.653344\n",
            "Epoch 145 Test Loss: 1.037823, Test Accuracy: 0.627895, time: 7.5s\n",
            "  Epoch 146 @ step 229000: Train Loss: 0.984079, Train Accuracy: 0.653813\n",
            "Epoch 146 Test Loss: 1.021635, Test Accuracy: 0.638379, time: 7.3s\n",
            "  Epoch 147 @ step 230000: Train Loss: 0.992100, Train Accuracy: 0.653125\n",
            "  Epoch 147 @ step 231000: Train Loss: 0.981005, Train Accuracy: 0.653344\n",
            "Epoch 147 Test Loss: 1.038736, Test Accuracy: 0.632388, time: 7.4s\n",
            "  Epoch 148 @ step 232000: Train Loss: 0.985271, Train Accuracy: 0.653719\n",
            "Epoch 148 Test Loss: 1.003582, Test Accuracy: 0.646466, time: 8.6s\n",
            "  Epoch 149 @ step 233000: Train Loss: 0.988915, Train Accuracy: 0.652719\n",
            "  Epoch 149 @ step 234000: Train Loss: 0.985694, Train Accuracy: 0.653125\n",
            "Epoch 149 Test Loss: 1.155321, Test Accuracy: 0.596546, time: 7.4s\n",
            "  Epoch 150 @ step 235000: Train Loss: 0.985766, Train Accuracy: 0.656094\n",
            "  Epoch 150 @ step 236000: Train Loss: 0.986414, Train Accuracy: 0.654688\n",
            "Epoch 150 Test Loss: 1.087677, Test Accuracy: 0.622204, time: 7.4s\n",
            "  Epoch 151 @ step 237000: Train Loss: 0.980212, Train Accuracy: 0.655031\n",
            "Epoch 151 Test Loss: 1.002155, Test Accuracy: 0.648163, time: 7.4s\n",
            "  Epoch 152 @ step 238000: Train Loss: 0.984232, Train Accuracy: 0.657156\n",
            "  Epoch 152 @ step 239000: Train Loss: 0.989399, Train Accuracy: 0.654563\n",
            "Epoch 152 Test Loss: 1.069402, Test Accuracy: 0.626198, time: 7.3s\n",
            "  Epoch 153 @ step 240000: Train Loss: 0.979587, Train Accuracy: 0.656313\n",
            "Epoch 153 Test Loss: 1.110825, Test Accuracy: 0.611122, time: 7.4s\n",
            "  Epoch 154 @ step 241000: Train Loss: 0.992065, Train Accuracy: 0.652813\n",
            "  Epoch 154 @ step 242000: Train Loss: 0.987879, Train Accuracy: 0.653031\n",
            "Epoch 154 Test Loss: 1.129824, Test Accuracy: 0.593051, time: 8.6s\n",
            "  Epoch 155 @ step 243000: Train Loss: 0.982000, Train Accuracy: 0.656938\n",
            "Epoch 155 Test Loss: 1.110014, Test Accuracy: 0.615715, time: 7.4s\n",
            "  Epoch 156 @ step 244000: Train Loss: 0.987701, Train Accuracy: 0.651594\n",
            "  Epoch 156 @ step 245000: Train Loss: 0.984054, Train Accuracy: 0.653375\n",
            "Epoch 156 Test Loss: 1.083201, Test Accuracy: 0.612819, time: 7.3s\n",
            "  Epoch 157 @ step 246000: Train Loss: 0.981259, Train Accuracy: 0.656406\n",
            "Epoch 157 Test Loss: 1.016347, Test Accuracy: 0.641873, time: 7.3s\n",
            "  Epoch 158 @ step 247000: Train Loss: 0.993782, Train Accuracy: 0.649500\n",
            "  Epoch 158 @ step 248000: Train Loss: 0.984478, Train Accuracy: 0.657938\n",
            "Epoch 158 Test Loss: 1.026863, Test Accuracy: 0.629293, time: 7.4s\n",
            "  Epoch 159 @ step 249000: Train Loss: 0.988917, Train Accuracy: 0.656125\n",
            "  Epoch 159 @ step 250000: Train Loss: 0.986563, Train Accuracy: 0.654281\n",
            "Epoch 159 Test Loss: 1.059120, Test Accuracy: 0.618610, time: 8.6s\n",
            "  Epoch 160 @ step 251000: Train Loss: 0.976627, Train Accuracy: 0.655688\n",
            "Epoch 160 Test Loss: 0.988222, Test Accuracy: 0.653854, time: 8.5s\n",
            "  Epoch 161 @ step 252000: Train Loss: 0.983734, Train Accuracy: 0.656438\n",
            "  Epoch 161 @ step 253000: Train Loss: 0.989660, Train Accuracy: 0.652625\n",
            "Epoch 161 Test Loss: 1.081808, Test Accuracy: 0.616813, time: 7.4s\n",
            "  Epoch 162 @ step 254000: Train Loss: 0.984786, Train Accuracy: 0.656969\n",
            "Epoch 162 Test Loss: 0.983993, Test Accuracy: 0.659645, time: 7.4s\n",
            "  Epoch 163 @ step 255000: Train Loss: 0.984187, Train Accuracy: 0.656469\n",
            "  Epoch 163 @ step 256000: Train Loss: 0.987131, Train Accuracy: 0.652531\n",
            "Epoch 163 Test Loss: 1.010414, Test Accuracy: 0.645367, time: 7.4s\n",
            "  Epoch 164 @ step 257000: Train Loss: 0.986224, Train Accuracy: 0.654750\n",
            "Epoch 164 Test Loss: 1.026615, Test Accuracy: 0.638878, time: 7.4s\n",
            "  Epoch 165 @ step 258000: Train Loss: 0.987525, Train Accuracy: 0.653656\n",
            "  Epoch 165 @ step 259000: Train Loss: 0.981969, Train Accuracy: 0.655469\n",
            "Epoch 165 Test Loss: 1.026170, Test Accuracy: 0.637480, time: 7.4s\n",
            "  Epoch 166 @ step 260000: Train Loss: 0.986106, Train Accuracy: 0.653625\n",
            "  Epoch 166 @ step 261000: Train Loss: 0.990986, Train Accuracy: 0.652875\n",
            "Epoch 166 Test Loss: 1.138704, Test Accuracy: 0.601138, time: 8.5s\n",
            "  Epoch 167 @ step 262000: Train Loss: 0.985856, Train Accuracy: 0.656125\n",
            "Epoch 167 Test Loss: 1.056476, Test Accuracy: 0.632688, time: 8.0s\n",
            "  Epoch 168 @ step 263000: Train Loss: 0.981706, Train Accuracy: 0.661219\n",
            "  Epoch 168 @ step 264000: Train Loss: 0.987367, Train Accuracy: 0.652781\n",
            "Epoch 168 Test Loss: 1.036404, Test Accuracy: 0.636981, time: 7.3s\n",
            "  Epoch 169 @ step 265000: Train Loss: 0.979241, Train Accuracy: 0.657750\n",
            "Epoch 169 Test Loss: 0.985515, Test Accuracy: 0.659345, time: 7.3s\n",
            "  Epoch 170 @ step 266000: Train Loss: 0.985951, Train Accuracy: 0.654250\n",
            "  Epoch 170 @ step 267000: Train Loss: 0.987066, Train Accuracy: 0.656156\n",
            "Epoch 170 Test Loss: 1.026914, Test Accuracy: 0.641973, time: 7.3s\n",
            "  Epoch 171 @ step 268000: Train Loss: 0.982438, Train Accuracy: 0.657125\n",
            "Epoch 171 Test Loss: 1.011962, Test Accuracy: 0.649860, time: 7.5s\n",
            "  Epoch 172 @ step 269000: Train Loss: 0.987849, Train Accuracy: 0.650000\n",
            "  Epoch 172 @ step 270000: Train Loss: 0.987263, Train Accuracy: 0.654406\n",
            "Epoch 172 Test Loss: 0.989035, Test Accuracy: 0.653754, time: 8.2s\n",
            "  Epoch 173 @ step 271000: Train Loss: 0.977785, Train Accuracy: 0.658281\n",
            "Epoch 173 Test Loss: 1.033922, Test Accuracy: 0.628195, time: 7.3s\n",
            "  Epoch 174 @ step 272000: Train Loss: 0.990646, Train Accuracy: 0.651656\n",
            "  Epoch 174 @ step 273000: Train Loss: 0.988961, Train Accuracy: 0.654906\n",
            "Epoch 174 Test Loss: 1.036050, Test Accuracy: 0.638279, time: 7.3s\n",
            "  Epoch 175 @ step 274000: Train Loss: 0.974305, Train Accuracy: 0.658125\n",
            "  Epoch 175 @ step 275000: Train Loss: 0.991186, Train Accuracy: 0.652969\n",
            "Epoch 175 Test Loss: 1.183657, Test Accuracy: 0.580771, time: 7.3s\n",
            "  Epoch 176 @ step 276000: Train Loss: 0.981171, Train Accuracy: 0.658406\n",
            "Epoch 176 Test Loss: 1.093281, Test Accuracy: 0.612320, time: 7.3s\n",
            "  Epoch 177 @ step 277000: Train Loss: 0.989761, Train Accuracy: 0.651219\n",
            "  Epoch 177 @ step 278000: Train Loss: 0.986712, Train Accuracy: 0.652688\n",
            "Epoch 177 Test Loss: 1.119842, Test Accuracy: 0.599940, time: 7.6s\n",
            "  Epoch 178 @ step 279000: Train Loss: 0.978763, Train Accuracy: 0.656313\n",
            "Epoch 178 Test Loss: 1.008512, Test Accuracy: 0.653255, time: 8.2s\n",
            "  Epoch 179 @ step 280000: Train Loss: 0.993715, Train Accuracy: 0.651344\n",
            "  Epoch 179 @ step 281000: Train Loss: 0.987887, Train Accuracy: 0.654156\n",
            "Epoch 179 Test Loss: 1.051333, Test Accuracy: 0.624501, time: 7.3s\n",
            "  Epoch 180 @ step 282000: Train Loss: 0.982539, Train Accuracy: 0.656188\n",
            "Epoch 180 Test Loss: 1.070414, Test Accuracy: 0.619409, time: 7.4s\n",
            "  Epoch 181 @ step 283000: Train Loss: 0.985578, Train Accuracy: 0.655750\n",
            "  Epoch 181 @ step 284000: Train Loss: 0.981992, Train Accuracy: 0.656313\n",
            "Epoch 181 Test Loss: 1.193624, Test Accuracy: 0.568990, time: 7.3s\n",
            "  Epoch 182 @ step 285000: Train Loss: 0.983007, Train Accuracy: 0.655781\n",
            "  Epoch 182 @ step 286000: Train Loss: 0.994636, Train Accuracy: 0.650813\n",
            "Epoch 182 Test Loss: 1.150120, Test Accuracy: 0.592452, time: 8.6s\n",
            "  Epoch 183 @ step 287000: Train Loss: 0.984864, Train Accuracy: 0.656094\n",
            "Epoch 183 Test Loss: 0.991282, Test Accuracy: 0.657248, time: 8.0s\n",
            "  Epoch 184 @ step 288000: Train Loss: 0.984584, Train Accuracy: 0.653594\n",
            "  Epoch 184 @ step 289000: Train Loss: 0.984113, Train Accuracy: 0.654625\n",
            "Epoch 184 Test Loss: 1.008019, Test Accuracy: 0.643371, time: 7.7s\n",
            "  Epoch 185 @ step 290000: Train Loss: 0.982154, Train Accuracy: 0.656406\n",
            "Epoch 185 Test Loss: 1.005043, Test Accuracy: 0.647065, time: 7.3s\n",
            "  Epoch 186 @ step 291000: Train Loss: 0.982367, Train Accuracy: 0.655156\n",
            "  Epoch 186 @ step 292000: Train Loss: 0.989755, Train Accuracy: 0.652781\n",
            "Epoch 186 Test Loss: 0.986695, Test Accuracy: 0.658247, time: 7.3s\n",
            "  Epoch 187 @ step 293000: Train Loss: 0.978806, Train Accuracy: 0.656688\n",
            "Epoch 187 Test Loss: 1.101697, Test Accuracy: 0.608127, time: 7.3s\n",
            "  Epoch 188 @ step 294000: Train Loss: 0.991125, Train Accuracy: 0.654781\n",
            "  Epoch 188 @ step 295000: Train Loss: 0.984973, Train Accuracy: 0.656469\n",
            "Epoch 188 Test Loss: 1.025773, Test Accuracy: 0.641773, time: 7.4s\n",
            "  Epoch 189 @ step 296000: Train Loss: 0.975899, Train Accuracy: 0.659313\n",
            "Epoch 189 Test Loss: 0.993420, Test Accuracy: 0.657149, time: 8.1s\n",
            "  Epoch 190 @ step 297000: Train Loss: 0.990335, Train Accuracy: 0.652594\n",
            "  Epoch 190 @ step 298000: Train Loss: 0.982033, Train Accuracy: 0.653750\n",
            "Epoch 190 Test Loss: 1.023474, Test Accuracy: 0.645268, time: 7.6s\n",
            "  Epoch 191 @ step 299000: Train Loss: 0.982231, Train Accuracy: 0.658094\n",
            "  Epoch 191 @ step 300000: Train Loss: 0.988989, Train Accuracy: 0.653938\n",
            "Epoch 191 Test Loss: 1.027262, Test Accuracy: 0.643970, time: 7.3s\n",
            "  Epoch 192 @ step 301000: Train Loss: 0.984915, Train Accuracy: 0.654719\n",
            "Epoch 192 Test Loss: 1.069049, Test Accuracy: 0.623003, time: 7.3s\n",
            "  Epoch 193 @ step 302000: Train Loss: 0.981216, Train Accuracy: 0.656656\n",
            "  Epoch 193 @ step 303000: Train Loss: 0.987962, Train Accuracy: 0.654625\n",
            "Epoch 193 Test Loss: 1.090858, Test Accuracy: 0.611621, time: 7.3s\n",
            "  Epoch 194 @ step 304000: Train Loss: 0.981478, Train Accuracy: 0.658250\n",
            "Epoch 194 Test Loss: 1.032705, Test Accuracy: 0.638279, time: 7.3s\n",
            "  Epoch 195 @ step 305000: Train Loss: 0.985127, Train Accuracy: 0.655906\n",
            "  Epoch 195 @ step 306000: Train Loss: 0.976495, Train Accuracy: 0.656938\n",
            "Epoch 195 Test Loss: 1.070723, Test Accuracy: 0.612420, time: 8.2s\n",
            "  Epoch 196 @ step 307000: Train Loss: 0.984946, Train Accuracy: 0.658281\n",
            "Epoch 196 Test Loss: 1.035983, Test Accuracy: 0.634085, time: 7.6s\n",
            "  Epoch 197 @ step 308000: Train Loss: 0.990627, Train Accuracy: 0.652781\n",
            "  Epoch 197 @ step 309000: Train Loss: 0.984025, Train Accuracy: 0.657250\n",
            "Epoch 197 Test Loss: 1.032674, Test Accuracy: 0.636082, time: 7.4s\n",
            "  Epoch 198 @ step 310000: Train Loss: 0.983361, Train Accuracy: 0.655281\n",
            "  Epoch 198 @ step 311000: Train Loss: 0.989712, Train Accuracy: 0.655719\n",
            "Epoch 198 Test Loss: 1.007225, Test Accuracy: 0.645966, time: 7.4s\n",
            "  Epoch 199 @ step 312000: Train Loss: 0.984091, Train Accuracy: 0.654313\n",
            "Epoch 199 Test Loss: 1.007589, Test Accuracy: 0.650459, time: 7.3s\n",
            "  Epoch 200 @ step 313000: Train Loss: 0.982194, Train Accuracy: 0.655969\n",
            "  Epoch 200 @ step 314000: Train Loss: 0.984999, Train Accuracy: 0.654344\n",
            "Epoch 200 Test Loss: 0.997994, Test Accuracy: 0.649461, time: 7.3s\n",
            "  Epoch 201 @ step 315000: Train Loss: 0.972270, Train Accuracy: 0.658750\n",
            "Epoch 201 Test Loss: 1.080566, Test Accuracy: 0.615016, time: 8.2s\n",
            "  Epoch 202 @ step 316000: Train Loss: 0.979096, Train Accuracy: 0.656344\n",
            "  Epoch 202 @ step 317000: Train Loss: 0.988699, Train Accuracy: 0.653406\n",
            "Epoch 202 Test Loss: 1.188013, Test Accuracy: 0.564996, time: 7.5s\n",
            "  Epoch 203 @ step 318000: Train Loss: 0.983941, Train Accuracy: 0.656875\n",
            "Epoch 203 Test Loss: 1.052516, Test Accuracy: 0.629493, time: 7.3s\n",
            "  Epoch 204 @ step 319000: Train Loss: 0.988649, Train Accuracy: 0.653563\n",
            "  Epoch 204 @ step 320000: Train Loss: 0.978716, Train Accuracy: 0.654875\n",
            "Epoch 204 Test Loss: 1.019506, Test Accuracy: 0.641773, time: 7.3s\n",
            "  Epoch 205 @ step 321000: Train Loss: 0.993854, Train Accuracy: 0.651469\n",
            "Epoch 205 Test Loss: 0.994776, Test Accuracy: 0.651058, time: 8.5s\n",
            "  Epoch 206 @ step 322000: Train Loss: 0.982076, Train Accuracy: 0.655781\n",
            "  Epoch 206 @ step 323000: Train Loss: 0.986655, Train Accuracy: 0.652781\n",
            "Epoch 206 Test Loss: 1.093782, Test Accuracy: 0.607528, time: 7.3s\n",
            "  Epoch 207 @ step 324000: Train Loss: 0.984055, Train Accuracy: 0.656719\n",
            "  Epoch 207 @ step 325000: Train Loss: 0.985481, Train Accuracy: 0.656031\n",
            "Epoch 207 Test Loss: 1.098986, Test Accuracy: 0.606030, time: 8.5s\n",
            "  Epoch 208 @ step 326000: Train Loss: 0.978665, Train Accuracy: 0.656281\n",
            "Epoch 208 Test Loss: 1.039046, Test Accuracy: 0.636581, time: 7.3s\n",
            "  Epoch 209 @ step 327000: Train Loss: 0.980233, Train Accuracy: 0.655781\n",
            "  Epoch 209 @ step 328000: Train Loss: 0.987472, Train Accuracy: 0.652063\n",
            "Epoch 209 Test Loss: 1.019318, Test Accuracy: 0.645667, time: 7.3s\n",
            "  Epoch 210 @ step 329000: Train Loss: 0.977497, Train Accuracy: 0.657031\n",
            "Epoch 210 Test Loss: 1.007818, Test Accuracy: 0.639277, time: 7.3s\n",
            "  Epoch 211 @ step 330000: Train Loss: 0.986423, Train Accuracy: 0.654969\n",
            "  Epoch 211 @ step 331000: Train Loss: 0.987911, Train Accuracy: 0.653438\n",
            "Epoch 211 Test Loss: 1.066842, Test Accuracy: 0.621605, time: 7.4s\n",
            "  Epoch 212 @ step 332000: Train Loss: 0.978392, Train Accuracy: 0.656813\n",
            "Epoch 212 Test Loss: 1.016056, Test Accuracy: 0.644269, time: 8.0s\n",
            "  Epoch 213 @ step 333000: Train Loss: 0.992132, Train Accuracy: 0.654938\n",
            "  Epoch 213 @ step 334000: Train Loss: 0.984214, Train Accuracy: 0.654781\n",
            "Epoch 213 Test Loss: 1.130036, Test Accuracy: 0.603534, time: 8.6s\n",
            "  Epoch 214 @ step 335000: Train Loss: 0.983881, Train Accuracy: 0.656125\n",
            "  Epoch 214 @ step 336000: Train Loss: 0.987551, Train Accuracy: 0.653719\n",
            "Epoch 214 Test Loss: 0.986381, Test Accuracy: 0.650260, time: 7.4s\n",
            "  Epoch 215 @ step 337000: Train Loss: 0.980389, Train Accuracy: 0.656563\n",
            "Epoch 215 Test Loss: 1.022248, Test Accuracy: 0.644269, time: 7.4s\n",
            "  Epoch 216 @ step 338000: Train Loss: 0.978876, Train Accuracy: 0.659344\n",
            "  Epoch 216 @ step 339000: Train Loss: 0.992335, Train Accuracy: 0.653125\n",
            "Epoch 216 Test Loss: 1.035951, Test Accuracy: 0.639876, time: 7.4s\n",
            "  Epoch 217 @ step 340000: Train Loss: 0.983625, Train Accuracy: 0.656563\n",
            "Epoch 217 Test Loss: 0.994641, Test Accuracy: 0.647863, time: 7.4s\n",
            "  Epoch 218 @ step 341000: Train Loss: 0.981443, Train Accuracy: 0.655063\n",
            "  Epoch 218 @ step 342000: Train Loss: 0.982361, Train Accuracy: 0.655000\n",
            "Epoch 218 Test Loss: 1.049399, Test Accuracy: 0.622304, time: 7.4s\n",
            "  Epoch 219 @ step 343000: Train Loss: 0.973029, Train Accuracy: 0.656844\n",
            "Epoch 219 Test Loss: 1.007854, Test Accuracy: 0.646765, time: 8.6s\n",
            "  Epoch 220 @ step 344000: Train Loss: 0.993366, Train Accuracy: 0.652750\n",
            "  Epoch 220 @ step 345000: Train Loss: 0.981108, Train Accuracy: 0.653125\n",
            "Epoch 220 Test Loss: 1.024600, Test Accuracy: 0.632388, time: 7.4s\n",
            "  Epoch 221 @ step 346000: Train Loss: 0.980626, Train Accuracy: 0.655594\n",
            "Epoch 221 Test Loss: 1.085429, Test Accuracy: 0.613918, time: 7.4s\n",
            "  Epoch 222 @ step 347000: Train Loss: 0.987292, Train Accuracy: 0.654906\n",
            "  Epoch 222 @ step 348000: Train Loss: 0.978433, Train Accuracy: 0.654750\n",
            "Epoch 222 Test Loss: 1.002295, Test Accuracy: 0.649461, time: 7.3s\n",
            "  Epoch 223 @ step 349000: Train Loss: 0.977188, Train Accuracy: 0.658063\n",
            "  Epoch 223 @ step 350000: Train Loss: 0.989581, Train Accuracy: 0.654906\n",
            "Epoch 223 Test Loss: 0.989600, Test Accuracy: 0.648962, time: 7.3s\n",
            "  Epoch 224 @ step 351000: Train Loss: 0.979604, Train Accuracy: 0.658281\n",
            "Epoch 224 Test Loss: 1.089725, Test Accuracy: 0.616114, time: 7.3s\n",
            "  Epoch 225 @ step 352000: Train Loss: 0.983570, Train Accuracy: 0.653594\n",
            "  Epoch 225 @ step 353000: Train Loss: 0.984614, Train Accuracy: 0.654500\n",
            "Epoch 225 Test Loss: 1.094126, Test Accuracy: 0.605731, time: 8.5s\n",
            "  Epoch 226 @ step 354000: Train Loss: 0.983661, Train Accuracy: 0.652625\n",
            "Epoch 226 Test Loss: 0.985788, Test Accuracy: 0.655551, time: 7.4s\n",
            "  Epoch 227 @ step 355000: Train Loss: 0.983681, Train Accuracy: 0.654750\n",
            "  Epoch 227 @ step 356000: Train Loss: 0.979381, Train Accuracy: 0.656500\n",
            "Epoch 227 Test Loss: 1.134025, Test Accuracy: 0.599740, time: 7.7s\n",
            "  Epoch 228 @ step 357000: Train Loss: 0.990263, Train Accuracy: 0.653469\n",
            "Epoch 228 Test Loss: 1.151417, Test Accuracy: 0.592552, time: 8.1s\n",
            "  Epoch 229 @ step 358000: Train Loss: 0.979975, Train Accuracy: 0.656031\n",
            "  Epoch 229 @ step 359000: Train Loss: 0.984186, Train Accuracy: 0.652281\n",
            "Epoch 229 Test Loss: 1.021823, Test Accuracy: 0.640575, time: 7.3s\n",
            "  Epoch 230 @ step 360000: Train Loss: 0.984532, Train Accuracy: 0.658906\n",
            "  Epoch 230 @ step 361000: Train Loss: 0.984785, Train Accuracy: 0.654219\n",
            "Epoch 230 Test Loss: 1.021351, Test Accuracy: 0.641074, time: 7.4s\n",
            "  Epoch 231 @ step 362000: Train Loss: 0.980580, Train Accuracy: 0.657906\n",
            "Epoch 231 Test Loss: 1.108764, Test Accuracy: 0.609525, time: 8.4s\n",
            "  Epoch 232 @ step 363000: Train Loss: 0.980777, Train Accuracy: 0.657156\n",
            "  Epoch 232 @ step 364000: Train Loss: 0.984600, Train Accuracy: 0.655125\n",
            "Epoch 232 Test Loss: 1.070042, Test Accuracy: 0.623403, time: 7.4s\n",
            "  Epoch 233 @ step 365000: Train Loss: 0.974822, Train Accuracy: 0.659531\n",
            "Epoch 233 Test Loss: 1.054269, Test Accuracy: 0.625000, time: 7.3s\n",
            "  Epoch 234 @ step 366000: Train Loss: 0.987435, Train Accuracy: 0.654469\n",
            "  Epoch 234 @ step 367000: Train Loss: 0.985755, Train Accuracy: 0.654188\n",
            "Epoch 234 Test Loss: 1.045788, Test Accuracy: 0.623103, time: 8.0s\n",
            "  Epoch 235 @ step 368000: Train Loss: 0.983466, Train Accuracy: 0.653500\n",
            "Epoch 235 Test Loss: 1.174630, Test Accuracy: 0.586362, time: 7.3s\n",
            "  Epoch 236 @ step 369000: Train Loss: 0.980545, Train Accuracy: 0.658438\n",
            "  Epoch 236 @ step 370000: Train Loss: 0.982279, Train Accuracy: 0.653688\n",
            "Epoch 236 Test Loss: 1.141272, Test Accuracy: 0.597943, time: 7.5s\n",
            "  Epoch 237 @ step 371000: Train Loss: 0.984208, Train Accuracy: 0.653969\n",
            "Epoch 237 Test Loss: 1.003224, Test Accuracy: 0.648962, time: 8.2s\n",
            "  Epoch 238 @ step 372000: Train Loss: 0.988497, Train Accuracy: 0.655313\n",
            "  Epoch 238 @ step 373000: Train Loss: 0.976557, Train Accuracy: 0.656031\n",
            "Epoch 238 Test Loss: 1.028708, Test Accuracy: 0.641673, time: 7.3s\n",
            "  Epoch 239 @ step 374000: Train Loss: 0.983565, Train Accuracy: 0.654906\n",
            "  Epoch 239 @ step 375000: Train Loss: 0.986933, Train Accuracy: 0.655375\n",
            "Epoch 239 Test Loss: 1.000404, Test Accuracy: 0.650759, time: 7.3s\n",
            "  Epoch 240 @ step 376000: Train Loss: 0.975623, Train Accuracy: 0.655344\n",
            "Epoch 240 Test Loss: 1.091427, Test Accuracy: 0.614916, time: 7.3s\n",
            "  Epoch 241 @ step 377000: Train Loss: 0.985757, Train Accuracy: 0.652563\n",
            "  Epoch 241 @ step 378000: Train Loss: 0.986528, Train Accuracy: 0.652875\n",
            "Epoch 241 Test Loss: 1.250849, Test Accuracy: 0.579972, time: 7.3s\n",
            "  Epoch 242 @ step 379000: Train Loss: 0.979706, Train Accuracy: 0.658531\n",
            "Epoch 242 Test Loss: 1.304552, Test Accuracy: 0.550619, time: 7.6s\n",
            "  Epoch 243 @ step 380000: Train Loss: 0.986688, Train Accuracy: 0.653969\n",
            "  Epoch 243 @ step 381000: Train Loss: 0.983578, Train Accuracy: 0.655719\n",
            "Epoch 243 Test Loss: 1.138696, Test Accuracy: 0.601538, time: 8.1s\n",
            "  Epoch 244 @ step 382000: Train Loss: 0.976611, Train Accuracy: 0.661531\n",
            "Epoch 244 Test Loss: 1.013916, Test Accuracy: 0.636082, time: 7.3s\n",
            "  Epoch 245 @ step 383000: Train Loss: 0.992065, Train Accuracy: 0.653813\n",
            "  Epoch 245 @ step 384000: Train Loss: 0.986577, Train Accuracy: 0.651969\n",
            "Epoch 245 Test Loss: 1.043392, Test Accuracy: 0.637580, time: 7.3s\n",
            "  Epoch 246 @ step 385000: Train Loss: 0.981684, Train Accuracy: 0.656563\n",
            "  Epoch 246 @ step 386000: Train Loss: 0.983270, Train Accuracy: 0.653094\n",
            "Epoch 246 Test Loss: 1.104076, Test Accuracy: 0.599241, time: 7.2s\n",
            "  Epoch 247 @ step 387000: Train Loss: 0.975254, Train Accuracy: 0.659156\n",
            "Epoch 247 Test Loss: 1.028581, Test Accuracy: 0.639177, time: 7.3s\n",
            "  Epoch 248 @ step 388000: Train Loss: 0.987071, Train Accuracy: 0.654750\n",
            "  Epoch 248 @ step 389000: Train Loss: 0.989968, Train Accuracy: 0.655156\n",
            "Epoch 248 Test Loss: 0.997936, Test Accuracy: 0.655551, time: 7.5s\n",
            "  Epoch 249 @ step 390000: Train Loss: 0.964035, Train Accuracy: 0.662906\n",
            "Epoch 249 Test Loss: 1.073066, Test Accuracy: 0.624101, time: 8.0s\n",
            "  Epoch 250 @ step 391000: Train Loss: 0.990545, Train Accuracy: 0.649781\n",
            "  Epoch 250 @ step 392000: Train Loss: 0.980039, Train Accuracy: 0.656250\n",
            "Epoch 250 Test Loss: 0.994891, Test Accuracy: 0.653954, time: 8.1s\n",
            "  Epoch 251 @ step 393000: Train Loss: 0.984491, Train Accuracy: 0.656031\n",
            "Epoch 251 Test Loss: 1.024758, Test Accuracy: 0.638279, time: 7.6s\n",
            "  Epoch 252 @ step 394000: Train Loss: 0.982724, Train Accuracy: 0.656219\n",
            "  Epoch 252 @ step 395000: Train Loss: 0.979450, Train Accuracy: 0.654469\n",
            "Epoch 252 Test Loss: 1.008186, Test Accuracy: 0.644469, time: 7.2s\n",
            "  Epoch 253 @ step 396000: Train Loss: 0.986894, Train Accuracy: 0.655281\n",
            "  Epoch 253 @ step 397000: Train Loss: 0.980657, Train Accuracy: 0.659156\n",
            "Epoch 253 Test Loss: 1.040862, Test Accuracy: 0.630891, time: 7.2s\n",
            "  Epoch 254 @ step 398000: Train Loss: 0.968444, Train Accuracy: 0.661469\n",
            "Epoch 254 Test Loss: 1.008869, Test Accuracy: 0.642971, time: 7.9s\n",
            "  Epoch 255 @ step 399000: Train Loss: 0.992504, Train Accuracy: 0.651531\n",
            "  Epoch 255 @ step 400000: Train Loss: 0.983627, Train Accuracy: 0.656063\n",
            "Epoch 255 Test Loss: 1.067411, Test Accuracy: 0.630691, time: 7.7s\n",
            "  Epoch 256 @ step 401000: Train Loss: 0.980976, Train Accuracy: 0.653656\n",
            "Epoch 256 Test Loss: 0.998494, Test Accuracy: 0.653255, time: 7.2s\n",
            "  Epoch 257 @ step 402000: Train Loss: 0.980081, Train Accuracy: 0.658094\n",
            "  Epoch 257 @ step 403000: Train Loss: 0.985400, Train Accuracy: 0.656500\n",
            "Epoch 257 Test Loss: 1.047445, Test Accuracy: 0.629692, time: 7.9s\n",
            "  Epoch 258 @ step 404000: Train Loss: 0.982281, Train Accuracy: 0.657125\n",
            "Epoch 258 Test Loss: 0.978199, Test Accuracy: 0.654054, time: 7.3s\n",
            "  Epoch 259 @ step 405000: Train Loss: 0.982873, Train Accuracy: 0.657469\n",
            "  Epoch 259 @ step 406000: Train Loss: 0.974292, Train Accuracy: 0.656938\n",
            "Epoch 259 Test Loss: 1.026504, Test Accuracy: 0.645268, time: 7.3s\n",
            "  Epoch 260 @ step 407000: Train Loss: 0.978914, Train Accuracy: 0.657563\n",
            "Epoch 260 Test Loss: 1.008487, Test Accuracy: 0.638778, time: 8.0s\n",
            "  Epoch 261 @ step 408000: Train Loss: 0.986245, Train Accuracy: 0.657063\n",
            "  Epoch 261 @ step 409000: Train Loss: 0.981811, Train Accuracy: 0.657031\n",
            "Epoch 261 Test Loss: 1.052858, Test Accuracy: 0.630491, time: 7.6s\n",
            "  Epoch 262 @ step 410000: Train Loss: 0.979961, Train Accuracy: 0.656719\n",
            "  Epoch 262 @ step 411000: Train Loss: 0.990061, Train Accuracy: 0.651188\n",
            "Epoch 262 Test Loss: 0.999469, Test Accuracy: 0.650260, time: 7.3s\n",
            "  Epoch 263 @ step 412000: Train Loss: 0.978236, Train Accuracy: 0.657906\n",
            "Epoch 263 Test Loss: 1.079639, Test Accuracy: 0.611222, time: 7.3s\n",
            "  Epoch 264 @ step 413000: Train Loss: 0.981992, Train Accuracy: 0.656375\n",
            "  Epoch 264 @ step 414000: Train Loss: 0.984094, Train Accuracy: 0.655969\n",
            "Epoch 264 Test Loss: 0.981606, Test Accuracy: 0.657748, time: 7.3s\n",
            "  Epoch 265 @ step 415000: Train Loss: 0.976695, Train Accuracy: 0.655844\n",
            "Epoch 265 Test Loss: 1.050034, Test Accuracy: 0.629393, time: 7.3s\n",
            "  Epoch 266 @ step 416000: Train Loss: 0.986137, Train Accuracy: 0.655156\n",
            "  Epoch 266 @ step 417000: Train Loss: 0.978139, Train Accuracy: 0.658656\n",
            "Epoch 266 Test Loss: 1.083182, Test Accuracy: 0.615415, time: 8.0s\n",
            "  Epoch 267 @ step 418000: Train Loss: 0.979189, Train Accuracy: 0.654969\n",
            "Epoch 267 Test Loss: 0.978212, Test Accuracy: 0.655950, time: 7.7s\n",
            "  Epoch 268 @ step 419000: Train Loss: 0.983405, Train Accuracy: 0.654875\n",
            "  Epoch 268 @ step 420000: Train Loss: 0.984433, Train Accuracy: 0.657500\n",
            "Epoch 268 Test Loss: 1.037300, Test Accuracy: 0.636282, time: 7.3s\n",
            "  Epoch 269 @ step 421000: Train Loss: 0.972267, Train Accuracy: 0.660719\n",
            "  Epoch 269 @ step 422000: Train Loss: 0.990495, Train Accuracy: 0.652438\n",
            "Epoch 269 Test Loss: 1.017180, Test Accuracy: 0.641274, time: 7.3s\n",
            "  Epoch 270 @ step 423000: Train Loss: 0.975851, Train Accuracy: 0.656531\n",
            "Epoch 270 Test Loss: 1.025289, Test Accuracy: 0.643071, time: 7.3s\n",
            "  Epoch 271 @ step 424000: Train Loss: 0.979297, Train Accuracy: 0.657281\n",
            "  Epoch 271 @ step 425000: Train Loss: 0.986237, Train Accuracy: 0.653906\n",
            "Epoch 271 Test Loss: 0.997877, Test Accuracy: 0.655651, time: 7.3s\n",
            "  Epoch 272 @ step 426000: Train Loss: 0.972496, Train Accuracy: 0.658438\n",
            "Epoch 272 Test Loss: 0.999586, Test Accuracy: 0.646466, time: 8.0s\n",
            "  Epoch 273 @ step 427000: Train Loss: 0.988881, Train Accuracy: 0.653625\n",
            "  Epoch 273 @ step 428000: Train Loss: 0.977749, Train Accuracy: 0.658500\n",
            "Epoch 273 Test Loss: 1.160223, Test Accuracy: 0.599641, time: 8.7s\n",
            "  Epoch 274 @ step 429000: Train Loss: 0.984391, Train Accuracy: 0.655344\n",
            "Epoch 274 Test Loss: 1.040904, Test Accuracy: 0.640076, time: 7.5s\n",
            "  Epoch 275 @ step 430000: Train Loss: 0.980218, Train Accuracy: 0.657781\n",
            "  Epoch 275 @ step 431000: Train Loss: 0.982169, Train Accuracy: 0.657188\n",
            "Epoch 275 Test Loss: 1.056498, Test Accuracy: 0.612420, time: 7.3s\n",
            "  Epoch 276 @ step 432000: Train Loss: 0.973873, Train Accuracy: 0.660219\n",
            "Epoch 276 Test Loss: 1.013155, Test Accuracy: 0.650260, time: 7.3s\n",
            "  Epoch 277 @ step 433000: Train Loss: 0.989085, Train Accuracy: 0.653500\n",
            "  Epoch 277 @ step 434000: Train Loss: 0.979768, Train Accuracy: 0.656688\n",
            "Epoch 277 Test Loss: 1.028972, Test Accuracy: 0.633486, time: 7.3s\n",
            "  Epoch 278 @ step 435000: Train Loss: 0.977659, Train Accuracy: 0.653656\n",
            "  Epoch 278 @ step 436000: Train Loss: 0.989582, Train Accuracy: 0.656875\n",
            "Epoch 278 Test Loss: 1.058758, Test Accuracy: 0.630891, time: 8.2s\n",
            "  Epoch 279 @ step 437000: Train Loss: 0.970659, Train Accuracy: 0.658750\n",
            "Epoch 279 Test Loss: 1.019469, Test Accuracy: 0.629892, time: 8.0s\n",
            "  Epoch 280 @ step 438000: Train Loss: 0.986048, Train Accuracy: 0.653500\n",
            "  Epoch 280 @ step 439000: Train Loss: 0.985498, Train Accuracy: 0.656219\n",
            "Epoch 280 Test Loss: 1.016314, Test Accuracy: 0.634485, time: 7.3s\n",
            "  Epoch 281 @ step 440000: Train Loss: 0.973798, Train Accuracy: 0.655188\n",
            "Epoch 281 Test Loss: 1.028546, Test Accuracy: 0.627696, time: 7.3s\n",
            "  Epoch 282 @ step 441000: Train Loss: 0.980011, Train Accuracy: 0.659188\n",
            "  Epoch 282 @ step 442000: Train Loss: 0.986763, Train Accuracy: 0.653156\n",
            "Epoch 282 Test Loss: 0.996825, Test Accuracy: 0.650060, time: 7.3s\n",
            "  Epoch 283 @ step 443000: Train Loss: 0.980954, Train Accuracy: 0.656469\n",
            "Epoch 283 Test Loss: 1.077264, Test Accuracy: 0.617312, time: 7.3s\n",
            "  Epoch 284 @ step 444000: Train Loss: 0.974824, Train Accuracy: 0.656344\n",
            "  Epoch 284 @ step 445000: Train Loss: 0.974169, Train Accuracy: 0.659688\n",
            "Epoch 284 Test Loss: 1.057051, Test Accuracy: 0.620507, time: 8.3s\n",
            "  Epoch 285 @ step 446000: Train Loss: 0.986476, Train Accuracy: 0.656031\n",
            "  Epoch 285 @ step 447000: Train Loss: 0.985909, Train Accuracy: 0.655125\n",
            "Epoch 285 Test Loss: 1.063189, Test Accuracy: 0.626298, time: 7.4s\n",
            "  Epoch 286 @ step 448000: Train Loss: 0.977805, Train Accuracy: 0.655375\n",
            "Epoch 286 Test Loss: 1.223844, Test Accuracy: 0.553215, time: 7.3s\n",
            "  Epoch 287 @ step 449000: Train Loss: 0.982085, Train Accuracy: 0.655531\n",
            "  Epoch 287 @ step 450000: Train Loss: 0.986587, Train Accuracy: 0.653000\n",
            "Epoch 287 Test Loss: 1.089775, Test Accuracy: 0.608127, time: 7.3s\n",
            "  Epoch 288 @ step 451000: Train Loss: 0.976470, Train Accuracy: 0.658719\n",
            "Epoch 288 Test Loss: 1.032707, Test Accuracy: 0.630990, time: 7.3s\n",
            "  Epoch 289 @ step 452000: Train Loss: 0.975672, Train Accuracy: 0.657688\n",
            "  Epoch 289 @ step 453000: Train Loss: 0.992395, Train Accuracy: 0.652250\n",
            "Epoch 289 Test Loss: 1.031425, Test Accuracy: 0.642073, time: 7.3s\n",
            "  Epoch 290 @ step 454000: Train Loss: 0.971652, Train Accuracy: 0.657000\n",
            "Epoch 290 Test Loss: 1.098925, Test Accuracy: 0.611821, time: 8.3s\n",
            "  Epoch 291 @ step 455000: Train Loss: 0.980273, Train Accuracy: 0.658813\n",
            "  Epoch 291 @ step 456000: Train Loss: 0.977695, Train Accuracy: 0.658719\n",
            "Epoch 291 Test Loss: 1.031257, Test Accuracy: 0.635084, time: 7.5s\n",
            "  Epoch 292 @ step 457000: Train Loss: 0.983207, Train Accuracy: 0.656500\n",
            "Epoch 292 Test Loss: 0.984555, Test Accuracy: 0.660443, time: 7.3s\n",
            "  Epoch 293 @ step 458000: Train Loss: 0.982738, Train Accuracy: 0.656000\n",
            "  Epoch 293 @ step 459000: Train Loss: 0.979142, Train Accuracy: 0.655844\n",
            "Epoch 293 Test Loss: 1.025399, Test Accuracy: 0.637979, time: 7.3s\n",
            "  Epoch 294 @ step 460000: Train Loss: 0.978278, Train Accuracy: 0.656313\n",
            "  Epoch 294 @ step 461000: Train Loss: 0.984134, Train Accuracy: 0.655719\n",
            "Epoch 294 Test Loss: 0.996730, Test Accuracy: 0.649361, time: 7.3s\n",
            "  Epoch 295 @ step 462000: Train Loss: 0.979788, Train Accuracy: 0.656125\n",
            "Epoch 295 Test Loss: 1.063280, Test Accuracy: 0.627895, time: 7.3s\n",
            "  Epoch 296 @ step 463000: Train Loss: 0.972744, Train Accuracy: 0.657969\n",
            "  Epoch 296 @ step 464000: Train Loss: 0.986934, Train Accuracy: 0.655219\n",
            "Epoch 296 Test Loss: 0.998849, Test Accuracy: 0.650260, time: 8.3s\n",
            "  Epoch 297 @ step 465000: Train Loss: 0.976290, Train Accuracy: 0.659094\n",
            "Epoch 297 Test Loss: 1.090035, Test Accuracy: 0.611222, time: 7.8s\n",
            "  Epoch 298 @ step 466000: Train Loss: 0.986903, Train Accuracy: 0.654313\n",
            "  Epoch 298 @ step 467000: Train Loss: 0.976312, Train Accuracy: 0.658188\n",
            "Epoch 298 Test Loss: 1.006626, Test Accuracy: 0.647065, time: 7.3s\n",
            "  Epoch 299 @ step 468000: Train Loss: 0.975229, Train Accuracy: 0.658656\n",
            "Epoch 299 Test Loss: 1.008165, Test Accuracy: 0.646366, time: 7.3s\n",
            "  Epoch 300 @ step 469000: Train Loss: 0.994424, Train Accuracy: 0.653063\n",
            "  Epoch 300 @ step 470000: Train Loss: 0.981333, Train Accuracy: 0.657375\n",
            "Epoch 300 Test Loss: 0.990483, Test Accuracy: 0.652456, time: 7.3s\n",
            "  Epoch 301 @ step 471000: Train Loss: 0.970358, Train Accuracy: 0.662750\n",
            "  Epoch 301 @ step 472000: Train Loss: 0.987913, Train Accuracy: 0.654719\n",
            "Epoch 301 Test Loss: 1.086698, Test Accuracy: 0.615615, time: 7.3s\n",
            "  Epoch 302 @ step 473000: Train Loss: 0.974154, Train Accuracy: 0.659938\n",
            "Epoch 302 Test Loss: 0.989698, Test Accuracy: 0.653055, time: 8.3s\n",
            "  Epoch 303 @ step 474000: Train Loss: 0.975736, Train Accuracy: 0.656188\n",
            "  Epoch 303 @ step 475000: Train Loss: 0.986466, Train Accuracy: 0.654375\n",
            "Epoch 303 Test Loss: 1.054705, Test Accuracy: 0.627496, time: 7.4s\n",
            "  Epoch 304 @ step 476000: Train Loss: 0.974442, Train Accuracy: 0.658531\n",
            "Epoch 304 Test Loss: 1.024252, Test Accuracy: 0.638179, time: 7.3s\n",
            "  Epoch 305 @ step 477000: Train Loss: 0.987200, Train Accuracy: 0.654563\n",
            "  Epoch 305 @ step 478000: Train Loss: 0.985234, Train Accuracy: 0.650469\n",
            "Epoch 305 Test Loss: 1.076869, Test Accuracy: 0.615116, time: 7.4s\n",
            "  Epoch 306 @ step 479000: Train Loss: 0.974597, Train Accuracy: 0.662344\n",
            "Epoch 306 Test Loss: 1.015525, Test Accuracy: 0.644269, time: 7.3s\n",
            "  Epoch 307 @ step 480000: Train Loss: 0.988889, Train Accuracy: 0.653625\n",
            "  Epoch 307 @ step 481000: Train Loss: 0.976576, Train Accuracy: 0.657875\n",
            "Epoch 307 Test Loss: 1.029249, Test Accuracy: 0.633986, time: 7.3s\n",
            "  Epoch 308 @ step 482000: Train Loss: 0.973530, Train Accuracy: 0.659813\n",
            "Epoch 308 Test Loss: 1.081429, Test Accuracy: 0.624601, time: 8.3s\n",
            "  Epoch 309 @ step 483000: Train Loss: 0.988159, Train Accuracy: 0.654125\n",
            "  Epoch 309 @ step 484000: Train Loss: 0.980600, Train Accuracy: 0.655094\n",
            "Epoch 309 Test Loss: 1.069146, Test Accuracy: 0.627995, time: 7.5s\n",
            "  Epoch 310 @ step 485000: Train Loss: 0.977510, Train Accuracy: 0.656906\n",
            "  Epoch 310 @ step 486000: Train Loss: 0.982615, Train Accuracy: 0.657563\n",
            "Epoch 310 Test Loss: 0.981599, Test Accuracy: 0.650759, time: 7.3s\n",
            "  Epoch 311 @ step 487000: Train Loss: 0.961771, Train Accuracy: 0.664688\n",
            "Epoch 311 Test Loss: 0.986159, Test Accuracy: 0.655950, time: 7.4s\n",
            "  Epoch 312 @ step 488000: Train Loss: 0.990349, Train Accuracy: 0.654594\n",
            "  Epoch 312 @ step 489000: Train Loss: 0.981677, Train Accuracy: 0.656344\n",
            "Epoch 312 Test Loss: 1.258458, Test Accuracy: 0.570387, time: 7.4s\n",
            "  Epoch 313 @ step 490000: Train Loss: 0.979747, Train Accuracy: 0.659438\n",
            "Epoch 313 Test Loss: 1.109285, Test Accuracy: 0.611721, time: 7.3s\n",
            "  Epoch 314 @ step 491000: Train Loss: 0.980443, Train Accuracy: 0.655063\n",
            "  Epoch 314 @ step 492000: Train Loss: 0.983081, Train Accuracy: 0.652563\n",
            "Epoch 314 Test Loss: 1.080353, Test Accuracy: 0.618211, time: 8.3s\n",
            "  Epoch 315 @ step 493000: Train Loss: 0.972113, Train Accuracy: 0.658156\n",
            "Epoch 315 Test Loss: 1.069491, Test Accuracy: 0.617812, time: 7.5s\n",
            "  Epoch 316 @ step 494000: Train Loss: 0.990597, Train Accuracy: 0.652719\n",
            "  Epoch 316 @ step 495000: Train Loss: 0.979756, Train Accuracy: 0.657688\n",
            "Epoch 316 Test Loss: 1.045414, Test Accuracy: 0.631190, time: 7.4s\n",
            "  Epoch 317 @ step 496000: Train Loss: 0.975094, Train Accuracy: 0.659250\n",
            "  Epoch 317 @ step 497000: Train Loss: 0.986082, Train Accuracy: 0.654438\n",
            "Epoch 317 Test Loss: 1.031962, Test Accuracy: 0.642073, time: 7.4s\n",
            "  Epoch 318 @ step 498000: Train Loss: 0.976616, Train Accuracy: 0.655875\n",
            "Epoch 318 Test Loss: 1.039493, Test Accuracy: 0.638778, time: 7.4s\n",
            "  Epoch 319 @ step 499000: Train Loss: 0.983049, Train Accuracy: 0.655531\n",
            "  Epoch 319 @ step 500000: Train Loss: 0.981932, Train Accuracy: 0.656406\n",
            "Epoch 319 Test Loss: 1.133457, Test Accuracy: 0.596446, time: 7.9s\n",
            "  Epoch 320 @ step 501000: Train Loss: 0.977750, Train Accuracy: 0.655750\n",
            "Epoch 320 Test Loss: 1.067605, Test Accuracy: 0.619509, time: 9.1s\n",
            "  Epoch 321 @ step 502000: Train Loss: 0.976187, Train Accuracy: 0.657656\n",
            "  Epoch 321 @ step 503000: Train Loss: 0.982955, Train Accuracy: 0.655281\n",
            "Epoch 321 Test Loss: 1.036501, Test Accuracy: 0.631689, time: 7.4s\n",
            "  Epoch 322 @ step 504000: Train Loss: 0.973646, Train Accuracy: 0.659656\n",
            "Epoch 322 Test Loss: 1.082649, Test Accuracy: 0.611422, time: 7.4s\n",
            "  Epoch 323 @ step 505000: Train Loss: 0.982154, Train Accuracy: 0.652906\n",
            "  Epoch 323 @ step 506000: Train Loss: 0.979509, Train Accuracy: 0.660094\n",
            "Epoch 323 Test Loss: 1.057145, Test Accuracy: 0.621406, time: 7.4s\n",
            "  Epoch 324 @ step 507000: Train Loss: 0.979874, Train Accuracy: 0.657906\n",
            "Epoch 324 Test Loss: 1.060797, Test Accuracy: 0.635184, time: 7.3s\n",
            "  Epoch 325 @ step 508000: Train Loss: 0.982511, Train Accuracy: 0.654344\n",
            "  Epoch 325 @ step 509000: Train Loss: 0.974168, Train Accuracy: 0.658219\n",
            "Epoch 325 Test Loss: 1.031085, Test Accuracy: 0.637280, time: 8.0s\n",
            "  Epoch 326 @ step 510000: Train Loss: 0.982986, Train Accuracy: 0.654750\n",
            "  Epoch 326 @ step 511000: Train Loss: 0.981335, Train Accuracy: 0.653469\n",
            "Epoch 326 Test Loss: 1.005283, Test Accuracy: 0.645068, time: 8.6s\n",
            "  Epoch 327 @ step 512000: Train Loss: 0.974175, Train Accuracy: 0.658906\n",
            "Epoch 327 Test Loss: 0.980502, Test Accuracy: 0.652556, time: 7.4s\n",
            "  Epoch 328 @ step 513000: Train Loss: 0.986194, Train Accuracy: 0.654656\n",
            "  Epoch 328 @ step 514000: Train Loss: 0.979563, Train Accuracy: 0.657281\n",
            "Epoch 328 Test Loss: 1.052960, Test Accuracy: 0.629892, time: 7.4s\n",
            "  Epoch 329 @ step 515000: Train Loss: 0.981278, Train Accuracy: 0.653375\n",
            "Epoch 329 Test Loss: 1.091925, Test Accuracy: 0.613019, time: 7.5s\n",
            "  Epoch 330 @ step 516000: Train Loss: 0.980439, Train Accuracy: 0.656375\n",
            "  Epoch 330 @ step 517000: Train Loss: 0.978717, Train Accuracy: 0.658219\n",
            "Epoch 330 Test Loss: 1.074600, Test Accuracy: 0.615415, time: 7.4s\n",
            "  Epoch 331 @ step 518000: Train Loss: 0.975979, Train Accuracy: 0.659406\n",
            "Epoch 331 Test Loss: 1.017716, Test Accuracy: 0.640575, time: 7.4s\n",
            "  Epoch 332 @ step 519000: Train Loss: 0.986273, Train Accuracy: 0.652875\n",
            "  Epoch 332 @ step 520000: Train Loss: 0.976382, Train Accuracy: 0.657625\n",
            "Epoch 332 Test Loss: 1.047053, Test Accuracy: 0.623303, time: 8.6s\n",
            "  Epoch 333 @ step 521000: Train Loss: 0.975527, Train Accuracy: 0.659313\n",
            "  Epoch 333 @ step 522000: Train Loss: 0.985098, Train Accuracy: 0.654844\n",
            "Epoch 333 Test Loss: 1.048548, Test Accuracy: 0.624501, time: 7.4s\n",
            "  Epoch 334 @ step 523000: Train Loss: 0.979038, Train Accuracy: 0.656906\n",
            "Epoch 334 Test Loss: 0.994413, Test Accuracy: 0.653654, time: 7.4s\n",
            "  Epoch 335 @ step 524000: Train Loss: 0.981363, Train Accuracy: 0.654719\n",
            "  Epoch 335 @ step 525000: Train Loss: 0.981560, Train Accuracy: 0.655594\n",
            "Epoch 335 Test Loss: 1.151947, Test Accuracy: 0.603734, time: 7.4s\n",
            "  Epoch 336 @ step 526000: Train Loss: 0.974687, Train Accuracy: 0.658656\n",
            "Epoch 336 Test Loss: 1.005711, Test Accuracy: 0.644768, time: 7.5s\n",
            "  Epoch 337 @ step 527000: Train Loss: 0.984277, Train Accuracy: 0.652406\n",
            "  Epoch 337 @ step 528000: Train Loss: 0.980090, Train Accuracy: 0.656938\n",
            "Epoch 337 Test Loss: 1.029513, Test Accuracy: 0.643670, time: 7.4s\n",
            "  Epoch 338 @ step 529000: Train Loss: 0.982069, Train Accuracy: 0.657563\n",
            "Epoch 338 Test Loss: 0.984773, Test Accuracy: 0.652356, time: 8.6s\n",
            "  Epoch 339 @ step 530000: Train Loss: 0.982683, Train Accuracy: 0.654031\n",
            "  Epoch 339 @ step 531000: Train Loss: 0.973305, Train Accuracy: 0.661594\n",
            "Epoch 339 Test Loss: 1.006512, Test Accuracy: 0.637680, time: 7.4s\n",
            "  Epoch 340 @ step 532000: Train Loss: 0.971669, Train Accuracy: 0.658031\n",
            "Epoch 340 Test Loss: 1.031152, Test Accuracy: 0.636482, time: 7.4s\n",
            "  Epoch 341 @ step 533000: Train Loss: 0.996755, Train Accuracy: 0.649969\n",
            "  Epoch 341 @ step 534000: Train Loss: 0.972211, Train Accuracy: 0.660438\n",
            "Epoch 341 Test Loss: 1.015715, Test Accuracy: 0.640974, time: 7.4s\n",
            "  Epoch 342 @ step 535000: Train Loss: 0.974586, Train Accuracy: 0.658906\n",
            "  Epoch 342 @ step 536000: Train Loss: 0.990159, Train Accuracy: 0.654750\n",
            "Epoch 342 Test Loss: 1.001834, Test Accuracy: 0.648063, time: 8.3s\n",
            "  Epoch 343 @ step 537000: Train Loss: 0.977670, Train Accuracy: 0.660688\n",
            "Epoch 343 Test Loss: 1.085710, Test Accuracy: 0.619109, time: 7.7s\n",
            "  Epoch 344 @ step 538000: Train Loss: 0.986550, Train Accuracy: 0.651594\n",
            "  Epoch 344 @ step 539000: Train Loss: 0.982559, Train Accuracy: 0.655688\n",
            "Epoch 344 Test Loss: 1.049024, Test Accuracy: 0.631689, time: 8.3s\n",
            "  Epoch 345 @ step 540000: Train Loss: 0.971405, Train Accuracy: 0.661531\n",
            "Epoch 345 Test Loss: 0.952859, Test Accuracy: 0.667532, time: 7.3s\n",
            "  Epoch 346 @ step 541000: Train Loss: 0.979868, Train Accuracy: 0.657469\n",
            "  Epoch 346 @ step 542000: Train Loss: 0.980513, Train Accuracy: 0.657406\n",
            "Epoch 346 Test Loss: 1.009930, Test Accuracy: 0.648263, time: 7.3s\n",
            "  Epoch 347 @ step 543000: Train Loss: 0.983462, Train Accuracy: 0.654344\n",
            "Epoch 347 Test Loss: 1.074831, Test Accuracy: 0.626997, time: 8.0s\n",
            "  Epoch 348 @ step 544000: Train Loss: 0.978137, Train Accuracy: 0.657469\n",
            "  Epoch 348 @ step 545000: Train Loss: 0.977324, Train Accuracy: 0.661375\n",
            "Epoch 348 Test Loss: 0.993207, Test Accuracy: 0.652556, time: 7.4s\n",
            "  Epoch 349 @ step 546000: Train Loss: 0.964982, Train Accuracy: 0.662063\n",
            "  Epoch 349 @ step 547000: Train Loss: 0.994023, Train Accuracy: 0.651281\n",
            "Epoch 349 Test Loss: 1.030147, Test Accuracy: 0.643870, time: 7.7s\n",
            "  Epoch 350 @ step 548000: Train Loss: 0.974217, Train Accuracy: 0.660094\n",
            "Epoch 350 Test Loss: 1.117461, Test Accuracy: 0.596546, time: 8.1s\n",
            "  Epoch 351 @ step 549000: Train Loss: 0.980380, Train Accuracy: 0.655188\n",
            "  Epoch 351 @ step 550000: Train Loss: 0.986841, Train Accuracy: 0.654875\n",
            "Epoch 351 Test Loss: 1.021365, Test Accuracy: 0.643071, time: 7.3s\n",
            "  Epoch 352 @ step 551000: Train Loss: 0.977406, Train Accuracy: 0.655656\n",
            "Epoch 352 Test Loss: 1.089822, Test Accuracy: 0.608926, time: 7.3s\n",
            "  Epoch 353 @ step 552000: Train Loss: 0.971568, Train Accuracy: 0.659094\n",
            "  Epoch 353 @ step 553000: Train Loss: 0.986256, Train Accuracy: 0.655750\n",
            "Epoch 353 Test Loss: 0.980941, Test Accuracy: 0.659744, time: 7.4s\n",
            "  Epoch 354 @ step 554000: Train Loss: 0.982523, Train Accuracy: 0.657281\n",
            "Epoch 354 Test Loss: 1.117953, Test Accuracy: 0.594649, time: 7.3s\n",
            "  Epoch 355 @ step 555000: Train Loss: 0.977572, Train Accuracy: 0.658438\n",
            "  Epoch 355 @ step 556000: Train Loss: 0.982051, Train Accuracy: 0.658594\n",
            "Epoch 355 Test Loss: 1.043580, Test Accuracy: 0.626398, time: 7.7s\n",
            "  Epoch 356 @ step 557000: Train Loss: 0.980251, Train Accuracy: 0.655750\n",
            "Epoch 356 Test Loss: 1.082111, Test Accuracy: 0.624800, time: 8.1s\n",
            "  Epoch 357 @ step 558000: Train Loss: 0.978308, Train Accuracy: 0.657563\n",
            "  Epoch 357 @ step 559000: Train Loss: 0.980283, Train Accuracy: 0.658844\n",
            "Epoch 357 Test Loss: 1.038951, Test Accuracy: 0.639477, time: 7.5s\n",
            "  Epoch 358 @ step 560000: Train Loss: 0.971705, Train Accuracy: 0.659656\n",
            "  Epoch 358 @ step 561000: Train Loss: 0.983642, Train Accuracy: 0.657563\n",
            "Epoch 358 Test Loss: 0.980123, Test Accuracy: 0.656849, time: 7.5s\n",
            "  Epoch 359 @ step 562000: Train Loss: 0.969909, Train Accuracy: 0.659656\n",
            "Epoch 359 Test Loss: 1.039075, Test Accuracy: 0.631689, time: 7.6s\n",
            "  Epoch 360 @ step 563000: Train Loss: 0.986877, Train Accuracy: 0.655281\n",
            "  Epoch 360 @ step 564000: Train Loss: 0.981863, Train Accuracy: 0.656750\n",
            "Epoch 360 Test Loss: 1.020177, Test Accuracy: 0.638079, time: 7.8s\n",
            "  Epoch 361 @ step 565000: Train Loss: 0.972965, Train Accuracy: 0.660594\n",
            "Epoch 361 Test Loss: 1.038219, Test Accuracy: 0.636581, time: 8.2s\n",
            "  Epoch 362 @ step 566000: Train Loss: 0.986428, Train Accuracy: 0.656375\n",
            "  Epoch 362 @ step 567000: Train Loss: 0.979190, Train Accuracy: 0.657375\n",
            "Epoch 362 Test Loss: 1.071981, Test Accuracy: 0.623702, time: 8.2s\n",
            "  Epoch 363 @ step 568000: Train Loss: 0.971166, Train Accuracy: 0.657000\n",
            "Epoch 363 Test Loss: 1.116661, Test Accuracy: 0.611222, time: 7.9s\n",
            "  Epoch 364 @ step 569000: Train Loss: 0.986014, Train Accuracy: 0.656656\n",
            "  Epoch 364 @ step 570000: Train Loss: 0.980422, Train Accuracy: 0.658875\n",
            "Epoch 364 Test Loss: 1.086210, Test Accuracy: 0.612919, time: 7.5s\n",
            "  Epoch 365 @ step 571000: Train Loss: 0.977388, Train Accuracy: 0.657219\n",
            "  Epoch 365 @ step 572000: Train Loss: 0.983239, Train Accuracy: 0.656719\n",
            "Epoch 365 Test Loss: 1.004706, Test Accuracy: 0.640475, time: 8.7s\n",
            "  Epoch 366 @ step 573000: Train Loss: 0.979087, Train Accuracy: 0.659781\n",
            "Epoch 366 Test Loss: 0.975633, Test Accuracy: 0.661442, time: 7.6s\n",
            "  Epoch 367 @ step 574000: Train Loss: 0.977406, Train Accuracy: 0.656000\n",
            "  Epoch 367 @ step 575000: Train Loss: 0.988106, Train Accuracy: 0.653625\n",
            "Epoch 367 Test Loss: 1.109909, Test Accuracy: 0.603934, time: 8.6s\n",
            "  Epoch 368 @ step 576000: Train Loss: 0.966916, Train Accuracy: 0.662219\n",
            "Epoch 368 Test Loss: 1.000013, Test Accuracy: 0.650958, time: 7.5s\n",
            "  Epoch 369 @ step 577000: Train Loss: 0.982920, Train Accuracy: 0.656969\n",
            "  Epoch 369 @ step 578000: Train Loss: 0.979988, Train Accuracy: 0.658094\n",
            "Epoch 369 Test Loss: 1.014880, Test Accuracy: 0.646266, time: 8.2s\n",
            "  Epoch 370 @ step 579000: Train Loss: 0.979703, Train Accuracy: 0.657250\n",
            "Epoch 370 Test Loss: 1.045271, Test Accuracy: 0.633686, time: 7.6s\n",
            "  Epoch 371 @ step 580000: Train Loss: 0.979592, Train Accuracy: 0.656219\n",
            "  Epoch 371 @ step 581000: Train Loss: 0.972914, Train Accuracy: 0.658563\n",
            "Epoch 371 Test Loss: 1.079756, Test Accuracy: 0.622704, time: 7.6s\n",
            "  Epoch 372 @ step 582000: Train Loss: 0.975406, Train Accuracy: 0.657125\n",
            "Epoch 372 Test Loss: 1.278647, Test Accuracy: 0.548922, time: 7.6s\n",
            "  Epoch 373 @ step 583000: Train Loss: 0.986698, Train Accuracy: 0.653563\n",
            "  Epoch 373 @ step 584000: Train Loss: 0.975226, Train Accuracy: 0.660656\n",
            "Epoch 373 Test Loss: 1.116891, Test Accuracy: 0.609125, time: 8.6s\n",
            "  Epoch 374 @ step 585000: Train Loss: 0.980000, Train Accuracy: 0.655594\n",
            "  Epoch 374 @ step 586000: Train Loss: 0.980042, Train Accuracy: 0.656875\n",
            "Epoch 374 Test Loss: 1.033711, Test Accuracy: 0.631989, time: 7.6s\n",
            "  Epoch 375 @ step 587000: Train Loss: 0.977932, Train Accuracy: 0.654688\n",
            "Epoch 375 Test Loss: 1.063290, Test Accuracy: 0.631490, time: 7.5s\n",
            "  Epoch 376 @ step 588000: Train Loss: 0.975385, Train Accuracy: 0.658844\n",
            "  Epoch 376 @ step 589000: Train Loss: 0.985732, Train Accuracy: 0.656031\n",
            "Epoch 376 Test Loss: 1.005773, Test Accuracy: 0.647364, time: 7.6s\n",
            "  Epoch 377 @ step 590000: Train Loss: 0.972662, Train Accuracy: 0.659281\n",
            "Epoch 377 Test Loss: 1.048688, Test Accuracy: 0.625998, time: 7.5s\n",
            "  Epoch 378 @ step 591000: Train Loss: 0.978672, Train Accuracy: 0.657000\n",
            "  Epoch 378 @ step 592000: Train Loss: 0.986281, Train Accuracy: 0.656000\n",
            "Epoch 378 Test Loss: 1.015191, Test Accuracy: 0.646765, time: 7.9s\n",
            "  Epoch 379 @ step 593000: Train Loss: 0.973830, Train Accuracy: 0.659688\n",
            "Epoch 379 Test Loss: 1.009088, Test Accuracy: 0.646466, time: 8.2s\n",
            "  Epoch 380 @ step 594000: Train Loss: 0.985770, Train Accuracy: 0.654938\n",
            "  Epoch 380 @ step 595000: Train Loss: 0.974906, Train Accuracy: 0.660719\n",
            "Epoch 380 Test Loss: 1.001304, Test Accuracy: 0.651657, time: 7.7s\n",
            "  Epoch 381 @ step 596000: Train Loss: 0.972900, Train Accuracy: 0.660469\n",
            "  Epoch 381 @ step 597000: Train Loss: 0.985543, Train Accuracy: 0.654375\n",
            "Epoch 381 Test Loss: 1.008415, Test Accuracy: 0.639177, time: 7.7s\n",
            "  Epoch 382 @ step 598000: Train Loss: 0.970841, Train Accuracy: 0.658375\n",
            "Epoch 382 Test Loss: 1.056181, Test Accuracy: 0.617911, time: 7.7s\n",
            "  Epoch 383 @ step 599000: Train Loss: 0.979679, Train Accuracy: 0.657469\n",
            "  Epoch 383 @ step 600000: Train Loss: 0.978856, Train Accuracy: 0.661031\n",
            "Epoch 383 Test Loss: 1.044955, Test Accuracy: 0.636282, time: 7.6s\n",
            "  Epoch 384 @ step 601000: Train Loss: 0.985109, Train Accuracy: 0.655594\n",
            "Epoch 384 Test Loss: 1.023794, Test Accuracy: 0.635084, time: 8.4s\n",
            "  Epoch 385 @ step 602000: Train Loss: 0.974633, Train Accuracy: 0.658813\n",
            "  Epoch 385 @ step 603000: Train Loss: 0.982307, Train Accuracy: 0.656000\n",
            "Epoch 385 Test Loss: 1.064295, Test Accuracy: 0.629093, time: 7.8s\n",
            "  Epoch 386 @ step 604000: Train Loss: 0.975296, Train Accuracy: 0.657375\n",
            "Epoch 386 Test Loss: 1.298611, Test Accuracy: 0.562600, time: 7.6s\n",
            "  Epoch 387 @ step 605000: Train Loss: 0.986360, Train Accuracy: 0.654625\n",
            "  Epoch 387 @ step 606000: Train Loss: 0.982555, Train Accuracy: 0.655094\n",
            "Epoch 387 Test Loss: 0.990602, Test Accuracy: 0.651158, time: 8.7s\n",
            "  Epoch 388 @ step 607000: Train Loss: 0.973604, Train Accuracy: 0.659344\n",
            "  Epoch 388 @ step 608000: Train Loss: 0.983488, Train Accuracy: 0.655125\n",
            "Epoch 388 Test Loss: 1.016341, Test Accuracy: 0.644669, time: 7.6s\n",
            "  Epoch 389 @ step 609000: Train Loss: 0.977836, Train Accuracy: 0.658656\n",
            "Epoch 389 Test Loss: 1.043862, Test Accuracy: 0.632188, time: 7.6s\n",
            "  Epoch 390 @ step 610000: Train Loss: 0.970510, Train Accuracy: 0.660219\n",
            "  Epoch 390 @ step 611000: Train Loss: 0.986950, Train Accuracy: 0.652969\n",
            "Epoch 390 Test Loss: 1.066000, Test Accuracy: 0.617512, time: 8.6s\n",
            "  Epoch 391 @ step 612000: Train Loss: 0.985398, Train Accuracy: 0.655594\n",
            "Epoch 391 Test Loss: 1.070249, Test Accuracy: 0.617312, time: 8.1s\n",
            "  Epoch 392 @ step 613000: Train Loss: 0.967999, Train Accuracy: 0.658750\n",
            "  Epoch 392 @ step 614000: Train Loss: 0.980509, Train Accuracy: 0.655625\n",
            "Epoch 392 Test Loss: 0.997712, Test Accuracy: 0.645168, time: 7.6s\n",
            "  Epoch 393 @ step 615000: Train Loss: 0.981988, Train Accuracy: 0.655156\n",
            "Epoch 393 Test Loss: 1.009840, Test Accuracy: 0.644669, time: 7.5s\n",
            "  Epoch 394 @ step 616000: Train Loss: 0.973741, Train Accuracy: 0.658250\n",
            "  Epoch 394 @ step 617000: Train Loss: 0.983241, Train Accuracy: 0.656625\n",
            "Epoch 394 Test Loss: 1.032844, Test Accuracy: 0.647764, time: 7.6s\n",
            "  Epoch 395 @ step 618000: Train Loss: 0.974581, Train Accuracy: 0.661813\n",
            "Epoch 395 Test Loss: 1.049745, Test Accuracy: 0.629892, time: 7.7s\n",
            "  Epoch 396 @ step 619000: Train Loss: 0.987320, Train Accuracy: 0.652125\n",
            "  Epoch 396 @ step 620000: Train Loss: 0.973961, Train Accuracy: 0.657813\n",
            "Epoch 396 Test Loss: 0.966085, Test Accuracy: 0.663438, time: 8.3s\n",
            "  Epoch 397 @ step 621000: Train Loss: 0.984287, Train Accuracy: 0.653625\n",
            "  Epoch 397 @ step 622000: Train Loss: 0.983726, Train Accuracy: 0.654563\n",
            "Epoch 397 Test Loss: 1.076090, Test Accuracy: 0.620507, time: 7.5s\n",
            "  Epoch 398 @ step 623000: Train Loss: 0.974010, Train Accuracy: 0.659375\n",
            "Epoch 398 Test Loss: 1.062681, Test Accuracy: 0.622704, time: 7.6s\n",
            "  Epoch 399 @ step 624000: Train Loss: 0.980793, Train Accuracy: 0.656188\n",
            "  Epoch 399 @ step 625000: Train Loss: 0.981795, Train Accuracy: 0.655844\n",
            "Epoch 399 Test Loss: 0.979325, Test Accuracy: 0.655651, time: 7.6s\n",
            "  Epoch 400 @ step 626000: Train Loss: 0.965751, Train Accuracy: 0.663656\n",
            "Epoch 400 Test Loss: 1.016794, Test Accuracy: 0.642772, time: 7.6s\n",
            "  Epoch 401 @ step 627000: Train Loss: 0.989478, Train Accuracy: 0.653375\n",
            "  Epoch 401 @ step 628000: Train Loss: 0.982579, Train Accuracy: 0.655625\n",
            "Epoch 401 Test Loss: 1.010764, Test Accuracy: 0.645767, time: 8.1s\n",
            "  Epoch 402 @ step 629000: Train Loss: 0.978198, Train Accuracy: 0.660438\n",
            "Epoch 402 Test Loss: 1.168982, Test Accuracy: 0.594948, time: 7.9s\n",
            "  Epoch 403 @ step 630000: Train Loss: 0.979985, Train Accuracy: 0.655656\n",
            "  Epoch 403 @ step 631000: Train Loss: 0.974660, Train Accuracy: 0.658813\n",
            "Epoch 403 Test Loss: 0.969316, Test Accuracy: 0.664736, time: 7.5s\n",
            "  Epoch 404 @ step 632000: Train Loss: 0.980415, Train Accuracy: 0.657906\n",
            "  Epoch 404 @ step 633000: Train Loss: 0.984825, Train Accuracy: 0.652344\n",
            "Epoch 404 Test Loss: 1.013274, Test Accuracy: 0.641573, time: 7.6s\n",
            "  Epoch 405 @ step 634000: Train Loss: 0.976115, Train Accuracy: 0.655719\n",
            "Epoch 405 Test Loss: 0.969743, Test Accuracy: 0.663838, time: 7.6s\n",
            "  Epoch 406 @ step 635000: Train Loss: 0.975474, Train Accuracy: 0.658656\n",
            "  Epoch 406 @ step 636000: Train Loss: 0.983727, Train Accuracy: 0.653813\n",
            "Epoch 406 Test Loss: 1.025830, Test Accuracy: 0.631390, time: 7.5s\n",
            "  Epoch 407 @ step 637000: Train Loss: 0.979268, Train Accuracy: 0.655906\n",
            "Epoch 407 Test Loss: 0.983492, Test Accuracy: 0.659345, time: 8.4s\n",
            "  Epoch 408 @ step 638000: Train Loss: 0.973422, Train Accuracy: 0.656313\n",
            "  Epoch 408 @ step 639000: Train Loss: 0.981185, Train Accuracy: 0.654875\n",
            "Epoch 408 Test Loss: 1.071869, Test Accuracy: 0.615315, time: 7.6s\n",
            "  Epoch 409 @ step 640000: Train Loss: 0.981088, Train Accuracy: 0.657250\n",
            "Epoch 409 Test Loss: 0.993839, Test Accuracy: 0.650459, time: 8.6s\n",
            "  Epoch 410 @ step 641000: Train Loss: 0.971631, Train Accuracy: 0.659844\n",
            "  Epoch 410 @ step 642000: Train Loss: 0.981711, Train Accuracy: 0.655000\n",
            "Epoch 410 Test Loss: 1.075580, Test Accuracy: 0.624800, time: 7.5s\n",
            "  Epoch 411 @ step 643000: Train Loss: 0.974244, Train Accuracy: 0.660250\n",
            "Epoch 411 Test Loss: 1.019963, Test Accuracy: 0.643171, time: 7.5s\n",
            "  Epoch 412 @ step 644000: Train Loss: 0.985254, Train Accuracy: 0.654906\n",
            "  Epoch 412 @ step 645000: Train Loss: 0.975118, Train Accuracy: 0.659125\n",
            "Epoch 412 Test Loss: 1.024689, Test Accuracy: 0.642971, time: 7.5s\n",
            "  Epoch 413 @ step 646000: Train Loss: 0.971923, Train Accuracy: 0.658688\n",
            "  Epoch 413 @ step 647000: Train Loss: 0.986812, Train Accuracy: 0.651594\n",
            "Epoch 413 Test Loss: 1.073839, Test Accuracy: 0.621805, time: 8.7s\n",
            "  Epoch 414 @ step 648000: Train Loss: 0.978732, Train Accuracy: 0.657625\n",
            "Epoch 414 Test Loss: 1.098462, Test Accuracy: 0.612819, time: 7.5s\n",
            "  Epoch 415 @ step 649000: Train Loss: 0.974397, Train Accuracy: 0.659469\n",
            "  Epoch 415 @ step 650000: Train Loss: 0.981820, Train Accuracy: 0.659188\n",
            "Epoch 415 Test Loss: 1.069348, Test Accuracy: 0.623502, time: 7.5s\n",
            "  Epoch 416 @ step 651000: Train Loss: 0.976895, Train Accuracy: 0.658188\n",
            "Epoch 416 Test Loss: 1.078849, Test Accuracy: 0.609325, time: 7.5s\n",
            "  Epoch 417 @ step 652000: Train Loss: 0.982755, Train Accuracy: 0.653688\n",
            "  Epoch 417 @ step 653000: Train Loss: 0.977432, Train Accuracy: 0.659469\n",
            "Epoch 417 Test Loss: 1.090471, Test Accuracy: 0.607228, time: 7.5s\n",
            "  Epoch 418 @ step 654000: Train Loss: 0.975000, Train Accuracy: 0.660781\n",
            "Epoch 418 Test Loss: 1.069875, Test Accuracy: 0.622504, time: 7.5s\n",
            "  Epoch 419 @ step 655000: Train Loss: 0.984831, Train Accuracy: 0.653281\n",
            "  Epoch 419 @ step 656000: Train Loss: 0.972686, Train Accuracy: 0.658406\n",
            "Epoch 419 Test Loss: 1.116809, Test Accuracy: 0.599241, time: 8.4s\n",
            "  Epoch 420 @ step 657000: Train Loss: 0.980038, Train Accuracy: 0.655969\n",
            "  Epoch 420 @ step 658000: Train Loss: 0.981194, Train Accuracy: 0.656563\n",
            "Epoch 420 Test Loss: 1.018407, Test Accuracy: 0.648562, time: 7.5s\n",
            "  Epoch 421 @ step 659000: Train Loss: 0.973535, Train Accuracy: 0.657938\n",
            "Epoch 421 Test Loss: 1.122387, Test Accuracy: 0.599141, time: 7.6s\n",
            "  Epoch 422 @ step 660000: Train Loss: 0.976827, Train Accuracy: 0.659188\n",
            "  Epoch 422 @ step 661000: Train Loss: 0.981538, Train Accuracy: 0.655813\n",
            "Epoch 422 Test Loss: 1.151019, Test Accuracy: 0.592352, time: 7.7s\n",
            "  Epoch 423 @ step 662000: Train Loss: 0.975613, Train Accuracy: 0.655938\n",
            "Epoch 423 Test Loss: 1.026287, Test Accuracy: 0.640076, time: 7.7s\n",
            "  Epoch 424 @ step 663000: Train Loss: 0.975900, Train Accuracy: 0.657469\n",
            "  Epoch 424 @ step 664000: Train Loss: 0.985553, Train Accuracy: 0.657281\n",
            "Epoch 424 Test Loss: 1.062324, Test Accuracy: 0.630391, time: 8.1s\n",
            "  Epoch 425 @ step 665000: Train Loss: 0.977743, Train Accuracy: 0.654375\n",
            "Epoch 425 Test Loss: 0.973756, Test Accuracy: 0.663438, time: 8.2s\n",
            "  Epoch 426 @ step 666000: Train Loss: 0.976935, Train Accuracy: 0.657313\n",
            "  Epoch 426 @ step 667000: Train Loss: 0.983039, Train Accuracy: 0.655406\n",
            "Epoch 426 Test Loss: 1.039524, Test Accuracy: 0.635883, time: 7.6s\n",
            "  Epoch 427 @ step 668000: Train Loss: 0.977115, Train Accuracy: 0.657031\n",
            "Epoch 427 Test Loss: 1.058799, Test Accuracy: 0.613019, time: 7.7s\n",
            "  Epoch 428 @ step 669000: Train Loss: 0.979334, Train Accuracy: 0.656156\n",
            "  Epoch 428 @ step 670000: Train Loss: 0.975709, Train Accuracy: 0.655750\n",
            "Epoch 428 Test Loss: 1.052005, Test Accuracy: 0.625499, time: 7.7s\n",
            "  Epoch 429 @ step 671000: Train Loss: 0.970565, Train Accuracy: 0.661063\n",
            "  Epoch 429 @ step 672000: Train Loss: 0.989390, Train Accuracy: 0.653188\n",
            "Epoch 429 Test Loss: 1.002357, Test Accuracy: 0.650859, time: 7.6s\n",
            "  Epoch 430 @ step 673000: Train Loss: 0.980848, Train Accuracy: 0.658344\n",
            "Epoch 430 Test Loss: 1.064913, Test Accuracy: 0.620208, time: 8.3s\n",
            "  Epoch 431 @ step 674000: Train Loss: 0.974274, Train Accuracy: 0.654781\n",
            "  Epoch 431 @ step 675000: Train Loss: 0.983896, Train Accuracy: 0.658406\n",
            "Epoch 431 Test Loss: 0.999534, Test Accuracy: 0.650359, time: 8.6s\n",
            "  Epoch 432 @ step 676000: Train Loss: 0.977204, Train Accuracy: 0.657844\n",
            "Epoch 432 Test Loss: 1.007635, Test Accuracy: 0.640276, time: 7.8s\n",
            "  Epoch 433 @ step 677000: Train Loss: 0.974991, Train Accuracy: 0.660438\n",
            "  Epoch 433 @ step 678000: Train Loss: 0.986462, Train Accuracy: 0.655406\n",
            "Epoch 433 Test Loss: 1.007750, Test Accuracy: 0.648462, time: 7.7s\n",
            "  Epoch 434 @ step 679000: Train Loss: 0.981051, Train Accuracy: 0.653406\n",
            "Epoch 434 Test Loss: 1.094933, Test Accuracy: 0.614117, time: 7.9s\n",
            "  Epoch 435 @ step 680000: Train Loss: 0.976381, Train Accuracy: 0.659969\n",
            "  Epoch 435 @ step 681000: Train Loss: 0.981139, Train Accuracy: 0.656281\n",
            "Epoch 435 Test Loss: 1.011909, Test Accuracy: 0.641873, time: 8.0s\n",
            "  Epoch 436 @ step 682000: Train Loss: 0.978466, Train Accuracy: 0.659094\n",
            "  Epoch 436 @ step 683000: Train Loss: 0.980190, Train Accuracy: 0.655219\n",
            "Epoch 436 Test Loss: 1.160006, Test Accuracy: 0.582169, time: 8.6s\n",
            "  Epoch 437 @ step 684000: Train Loss: 0.979611, Train Accuracy: 0.657094\n",
            "Epoch 437 Test Loss: 1.044505, Test Accuracy: 0.638978, time: 7.7s\n",
            "  Epoch 438 @ step 685000: Train Loss: 0.975247, Train Accuracy: 0.659406\n",
            "  Epoch 438 @ step 686000: Train Loss: 0.981013, Train Accuracy: 0.656156\n",
            "Epoch 438 Test Loss: 1.001161, Test Accuracy: 0.643770, time: 7.7s\n",
            "  Epoch 439 @ step 687000: Train Loss: 0.968409, Train Accuracy: 0.661563\n",
            "Epoch 439 Test Loss: 1.170758, Test Accuracy: 0.593950, time: 7.7s\n",
            "  Epoch 440 @ step 688000: Train Loss: 0.984213, Train Accuracy: 0.653813\n",
            "  Epoch 440 @ step 689000: Train Loss: 0.978212, Train Accuracy: 0.657469\n",
            "Epoch 440 Test Loss: 1.017152, Test Accuracy: 0.633886, time: 7.7s\n",
            "  Epoch 441 @ step 690000: Train Loss: 0.982574, Train Accuracy: 0.657219\n",
            "Epoch 441 Test Loss: 1.047327, Test Accuracy: 0.627696, time: 8.0s\n",
            "  Epoch 442 @ step 691000: Train Loss: 0.974207, Train Accuracy: 0.657375\n",
            "  Epoch 442 @ step 692000: Train Loss: 0.977188, Train Accuracy: 0.658531\n",
            "Epoch 442 Test Loss: 1.075521, Test Accuracy: 0.611122, time: 8.3s\n",
            "  Epoch 443 @ step 693000: Train Loss: 0.982666, Train Accuracy: 0.654344\n",
            "Epoch 443 Test Loss: 1.020410, Test Accuracy: 0.637181, time: 7.7s\n",
            "  Epoch 444 @ step 694000: Train Loss: 0.981428, Train Accuracy: 0.656500\n",
            "  Epoch 444 @ step 695000: Train Loss: 0.982046, Train Accuracy: 0.657219\n",
            "Epoch 444 Test Loss: 0.965045, Test Accuracy: 0.662540, time: 7.7s\n",
            "  Epoch 445 @ step 696000: Train Loss: 0.969512, Train Accuracy: 0.658438\n",
            "  Epoch 445 @ step 697000: Train Loss: 0.981650, Train Accuracy: 0.656250\n",
            "Epoch 445 Test Loss: 1.056296, Test Accuracy: 0.628095, time: 7.8s\n",
            "  Epoch 446 @ step 698000: Train Loss: 0.969943, Train Accuracy: 0.660781\n",
            "Epoch 446 Test Loss: 0.999556, Test Accuracy: 0.647264, time: 7.7s\n",
            "  Epoch 447 @ step 699000: Train Loss: 0.977463, Train Accuracy: 0.657844\n",
            "  Epoch 447 @ step 700000: Train Loss: 0.983174, Train Accuracy: 0.655438\n",
            "Epoch 447 Test Loss: 0.988711, Test Accuracy: 0.651258, time: 8.4s\n",
            "  Epoch 448 @ step 701000: Train Loss: 0.973528, Train Accuracy: 0.660281\n",
            "Epoch 448 Test Loss: 1.123964, Test Accuracy: 0.590655, time: 8.0s\n",
            "  Epoch 449 @ step 702000: Train Loss: 0.979798, Train Accuracy: 0.656094\n",
            "  Epoch 449 @ step 703000: Train Loss: 0.984865, Train Accuracy: 0.656250\n",
            "Epoch 449 Test Loss: 1.007872, Test Accuracy: 0.654153, time: 7.8s\n",
            "  Epoch 450 @ step 704000: Train Loss: 0.975121, Train Accuracy: 0.658750\n",
            "Epoch 450 Test Loss: 1.041677, Test Accuracy: 0.625100, time: 7.7s\n",
            "  Epoch 451 @ step 705000: Train Loss: 0.980150, Train Accuracy: 0.656156\n",
            "  Epoch 451 @ step 706000: Train Loss: 0.967707, Train Accuracy: 0.660031\n",
            "Epoch 451 Test Loss: 1.039846, Test Accuracy: 0.635184, time: 7.7s\n",
            "  Epoch 452 @ step 707000: Train Loss: 0.985915, Train Accuracy: 0.655594\n",
            "  Epoch 452 @ step 708000: Train Loss: 0.980133, Train Accuracy: 0.656313\n",
            "Epoch 452 Test Loss: 0.984399, Test Accuracy: 0.657149, time: 7.8s\n",
            "  Epoch 453 @ step 709000: Train Loss: 0.976551, Train Accuracy: 0.658313\n",
            "Epoch 453 Test Loss: 1.059153, Test Accuracy: 0.631390, time: 8.9s\n",
            "  Epoch 454 @ step 710000: Train Loss: 0.974560, Train Accuracy: 0.656781\n",
            "  Epoch 454 @ step 711000: Train Loss: 0.989915, Train Accuracy: 0.650781\n",
            "Epoch 454 Test Loss: 1.033586, Test Accuracy: 0.632788, time: 7.9s\n",
            "  Epoch 455 @ step 712000: Train Loss: 0.968736, Train Accuracy: 0.661219\n",
            "Epoch 455 Test Loss: 0.958249, Test Accuracy: 0.663738, time: 7.7s\n",
            "  Epoch 456 @ step 713000: Train Loss: 0.979407, Train Accuracy: 0.657063\n",
            "  Epoch 456 @ step 714000: Train Loss: 0.981451, Train Accuracy: 0.655125\n",
            "Epoch 456 Test Loss: 1.038777, Test Accuracy: 0.631689, time: 8.2s\n",
            "  Epoch 457 @ step 715000: Train Loss: 0.971699, Train Accuracy: 0.659906\n",
            "Epoch 457 Test Loss: 1.043319, Test Accuracy: 0.637780, time: 7.7s\n",
            "  Epoch 458 @ step 716000: Train Loss: 0.987812, Train Accuracy: 0.655375\n",
            "  Epoch 458 @ step 717000: Train Loss: 0.980287, Train Accuracy: 0.655781\n",
            "Epoch 458 Test Loss: 1.214309, Test Accuracy: 0.579073, time: 7.9s\n",
            "  Epoch 459 @ step 718000: Train Loss: 0.977611, Train Accuracy: 0.657844\n",
            "Epoch 459 Test Loss: 1.081232, Test Accuracy: 0.622504, time: 8.4s\n",
            "  Epoch 460 @ step 719000: Train Loss: 0.980219, Train Accuracy: 0.655625\n",
            "  Epoch 460 @ step 720000: Train Loss: 0.976810, Train Accuracy: 0.658000\n",
            "Epoch 460 Test Loss: 0.990059, Test Accuracy: 0.656550, time: 7.7s\n",
            "  Epoch 461 @ step 721000: Train Loss: 0.976368, Train Accuracy: 0.659344\n",
            "  Epoch 461 @ step 722000: Train Loss: 0.986064, Train Accuracy: 0.655406\n",
            "Epoch 461 Test Loss: 0.981719, Test Accuracy: 0.649960, time: 7.8s\n",
            "  Epoch 462 @ step 723000: Train Loss: 0.975265, Train Accuracy: 0.660313\n",
            "Epoch 462 Test Loss: 1.006884, Test Accuracy: 0.646965, time: 7.8s\n",
            "  Epoch 463 @ step 724000: Train Loss: 0.977875, Train Accuracy: 0.661281\n",
            "  Epoch 463 @ step 725000: Train Loss: 0.984610, Train Accuracy: 0.655781\n",
            "Epoch 463 Test Loss: 1.008413, Test Accuracy: 0.647764, time: 7.7s\n",
            "  Epoch 464 @ step 726000: Train Loss: 0.972809, Train Accuracy: 0.659813\n",
            "Epoch 464 Test Loss: 0.995456, Test Accuracy: 0.654054, time: 8.4s\n",
            "  Epoch 465 @ step 727000: Train Loss: 0.983797, Train Accuracy: 0.658156\n",
            "  Epoch 465 @ step 728000: Train Loss: 0.981591, Train Accuracy: 0.652781\n",
            "Epoch 465 Test Loss: 1.086431, Test Accuracy: 0.610323, time: 8.0s\n",
            "  Epoch 466 @ step 729000: Train Loss: 0.973689, Train Accuracy: 0.660906\n",
            "Epoch 466 Test Loss: 0.999864, Test Accuracy: 0.646166, time: 7.7s\n",
            "  Epoch 467 @ step 730000: Train Loss: 0.984120, Train Accuracy: 0.654750\n",
            "  Epoch 467 @ step 731000: Train Loss: 0.974159, Train Accuracy: 0.659438\n",
            "Epoch 467 Test Loss: 1.085337, Test Accuracy: 0.613918, time: 7.7s\n",
            "  Epoch 468 @ step 732000: Train Loss: 0.978898, Train Accuracy: 0.656531\n",
            "  Epoch 468 @ step 733000: Train Loss: 0.984628, Train Accuracy: 0.656031\n",
            "Epoch 468 Test Loss: 1.082480, Test Accuracy: 0.608726, time: 7.7s\n",
            "  Epoch 469 @ step 734000: Train Loss: 0.974545, Train Accuracy: 0.658781\n",
            "Epoch 469 Test Loss: 0.978290, Test Accuracy: 0.654054, time: 7.6s\n",
            "  Epoch 470 @ step 735000: Train Loss: 0.975665, Train Accuracy: 0.659031\n",
            "  Epoch 470 @ step 736000: Train Loss: 0.984688, Train Accuracy: 0.654750\n",
            "Epoch 470 Test Loss: 1.015626, Test Accuracy: 0.641374, time: 8.6s\n",
            "  Epoch 471 @ step 737000: Train Loss: 0.976995, Train Accuracy: 0.657406\n",
            "Epoch 471 Test Loss: 1.025828, Test Accuracy: 0.637380, time: 7.7s\n",
            "  Epoch 472 @ step 738000: Train Loss: 0.980137, Train Accuracy: 0.656438\n",
            "  Epoch 472 @ step 739000: Train Loss: 0.979887, Train Accuracy: 0.657906\n",
            "Epoch 472 Test Loss: 0.978595, Test Accuracy: 0.659046, time: 7.7s\n",
            "  Epoch 473 @ step 740000: Train Loss: 0.979204, Train Accuracy: 0.655750\n",
            "Epoch 473 Test Loss: 1.080847, Test Accuracy: 0.604034, time: 7.7s\n",
            "  Epoch 474 @ step 741000: Train Loss: 0.975491, Train Accuracy: 0.658281\n",
            "  Epoch 474 @ step 742000: Train Loss: 0.978133, Train Accuracy: 0.660656\n",
            "Epoch 474 Test Loss: 1.048734, Test Accuracy: 0.635383, time: 7.7s\n",
            "  Epoch 475 @ step 743000: Train Loss: 0.976882, Train Accuracy: 0.658313\n",
            "Epoch 475 Test Loss: 1.127648, Test Accuracy: 0.592352, time: 8.5s\n",
            "  Epoch 476 @ step 744000: Train Loss: 0.982642, Train Accuracy: 0.655375\n",
            "  Epoch 476 @ step 745000: Train Loss: 0.977051, Train Accuracy: 0.658188\n",
            "Epoch 476 Test Loss: 1.082241, Test Accuracy: 0.609425, time: 8.4s\n",
            "  Epoch 477 @ step 746000: Train Loss: 0.971105, Train Accuracy: 0.658031\n",
            "  Epoch 477 @ step 747000: Train Loss: 0.987242, Train Accuracy: 0.653219\n",
            "Epoch 477 Test Loss: 0.978652, Test Accuracy: 0.652756, time: 7.8s\n",
            "  Epoch 478 @ step 748000: Train Loss: 0.977978, Train Accuracy: 0.657781\n",
            "Epoch 478 Test Loss: 1.001717, Test Accuracy: 0.647065, time: 8.1s\n",
            "  Epoch 479 @ step 749000: Train Loss: 0.973940, Train Accuracy: 0.656125\n",
            "  Epoch 479 @ step 750000: Train Loss: 0.981949, Train Accuracy: 0.660250\n",
            "Epoch 479 Test Loss: 1.144488, Test Accuracy: 0.586362, time: 7.7s\n",
            "  Epoch 480 @ step 751000: Train Loss: 0.971774, Train Accuracy: 0.661281\n",
            "Epoch 480 Test Loss: 0.987957, Test Accuracy: 0.648562, time: 7.6s\n",
            "  Epoch 481 @ step 752000: Train Loss: 0.984598, Train Accuracy: 0.656219\n",
            "  Epoch 481 @ step 753000: Train Loss: 0.974702, Train Accuracy: 0.657000\n",
            "Epoch 481 Test Loss: 0.972598, Test Accuracy: 0.663438, time: 8.2s\n",
            "  Epoch 482 @ step 754000: Train Loss: 0.983342, Train Accuracy: 0.655500\n",
            "Epoch 482 Test Loss: 1.081639, Test Accuracy: 0.616414, time: 8.0s\n",
            "  Epoch 483 @ step 755000: Train Loss: 0.973825, Train Accuracy: 0.659625\n",
            "  Epoch 483 @ step 756000: Train Loss: 0.980442, Train Accuracy: 0.655188\n",
            "Epoch 483 Test Loss: 1.036533, Test Accuracy: 0.638279, time: 7.9s\n",
            "  Epoch 484 @ step 757000: Train Loss: 0.968508, Train Accuracy: 0.661344\n",
            "  Epoch 484 @ step 758000: Train Loss: 0.987471, Train Accuracy: 0.653313\n",
            "Epoch 484 Test Loss: 0.982244, Test Accuracy: 0.653454, time: 8.1s\n",
            "  Epoch 485 @ step 759000: Train Loss: 0.980123, Train Accuracy: 0.655156\n",
            "Epoch 485 Test Loss: 1.040657, Test Accuracy: 0.628195, time: 8.1s\n",
            "  Epoch 486 @ step 760000: Train Loss: 0.975235, Train Accuracy: 0.659531\n",
            "  Epoch 486 @ step 761000: Train Loss: 0.986164, Train Accuracy: 0.652188\n",
            "Epoch 486 Test Loss: 1.040530, Test Accuracy: 0.633686, time: 8.3s\n",
            "  Epoch 487 @ step 762000: Train Loss: 0.974202, Train Accuracy: 0.660625\n",
            "Epoch 487 Test Loss: 1.024664, Test Accuracy: 0.647664, time: 8.8s\n",
            "  Epoch 488 @ step 763000: Train Loss: 0.983107, Train Accuracy: 0.655688\n",
            "  Epoch 488 @ step 764000: Train Loss: 0.980638, Train Accuracy: 0.656969\n",
            "Epoch 488 Test Loss: 1.026554, Test Accuracy: 0.633387, time: 7.9s\n",
            "  Epoch 489 @ step 765000: Train Loss: 0.976801, Train Accuracy: 0.657594\n",
            "Epoch 489 Test Loss: 0.983460, Test Accuracy: 0.655551, time: 8.0s\n",
            "  Epoch 490 @ step 766000: Train Loss: 0.978053, Train Accuracy: 0.657406\n",
            "  Epoch 490 @ step 767000: Train Loss: 0.982042, Train Accuracy: 0.656906\n",
            "Epoch 490 Test Loss: 1.043579, Test Accuracy: 0.632788, time: 7.9s\n",
            "  Epoch 491 @ step 768000: Train Loss: 0.981354, Train Accuracy: 0.659656\n",
            "Epoch 491 Test Loss: 1.056640, Test Accuracy: 0.639677, time: 8.0s\n",
            "  Epoch 492 @ step 769000: Train Loss: 0.979304, Train Accuracy: 0.655125\n",
            "  Epoch 492 @ step 770000: Train Loss: 0.976369, Train Accuracy: 0.658969\n",
            "Epoch 492 Test Loss: 0.999911, Test Accuracy: 0.650160, time: 8.3s\n",
            "  Epoch 493 @ step 771000: Train Loss: 0.974642, Train Accuracy: 0.660875\n",
            "  Epoch 493 @ step 772000: Train Loss: 0.985691, Train Accuracy: 0.654250\n",
            "Epoch 493 Test Loss: 1.000662, Test Accuracy: 0.647364, time: 8.4s\n",
            "  Epoch 494 @ step 773000: Train Loss: 0.974607, Train Accuracy: 0.657813\n",
            "Epoch 494 Test Loss: 1.009390, Test Accuracy: 0.643570, time: 7.9s\n",
            "  Epoch 495 @ step 774000: Train Loss: 0.977311, Train Accuracy: 0.656969\n",
            "  Epoch 495 @ step 775000: Train Loss: 0.982359, Train Accuracy: 0.654469\n",
            "Epoch 495 Test Loss: 1.023800, Test Accuracy: 0.642472, time: 7.9s\n",
            "  Epoch 496 @ step 776000: Train Loss: 0.976856, Train Accuracy: 0.656719\n",
            "Epoch 496 Test Loss: 1.133912, Test Accuracy: 0.599641, time: 8.0s\n",
            "  Epoch 497 @ step 777000: Train Loss: 0.980326, Train Accuracy: 0.656625\n",
            "  Epoch 497 @ step 778000: Train Loss: 0.985675, Train Accuracy: 0.651375\n",
            "Epoch 497 Test Loss: 1.030553, Test Accuracy: 0.642073, time: 9.1s\n",
            "  Epoch 498 @ step 779000: Train Loss: 0.972277, Train Accuracy: 0.659719\n",
            "Epoch 498 Test Loss: 1.029165, Test Accuracy: 0.636182, time: 8.7s\n",
            "  Epoch 499 @ step 780000: Train Loss: 0.980725, Train Accuracy: 0.657875\n",
            "  Epoch 499 @ step 781000: Train Loss: 0.973926, Train Accuracy: 0.659906\n",
            "Epoch 499 Test Loss: 1.025197, Test Accuracy: 0.630292, time: 8.3s\n",
            "  Epoch 500 @ step 782000: Train Loss: 0.980854, Train Accuracy: 0.657500\n",
            "  Epoch 500 @ step 783000: Train Loss: 0.975641, Train Accuracy: 0.658250\n",
            "Epoch 500 Test Loss: 0.986459, Test Accuracy: 0.657648, time: 8.2s\n",
            "  Epoch 501 @ step 784000: Train Loss: 0.977934, Train Accuracy: 0.656906\n",
            "Epoch 501 Test Loss: 0.993745, Test Accuracy: 0.649161, time: 8.0s\n",
            "  Epoch 502 @ step 785000: Train Loss: 0.981339, Train Accuracy: 0.654406\n",
            "  Epoch 502 @ step 786000: Train Loss: 0.980566, Train Accuracy: 0.655688\n",
            "Epoch 502 Test Loss: 1.025441, Test Accuracy: 0.644169, time: 7.9s\n",
            "  Epoch 503 @ step 787000: Train Loss: 0.972482, Train Accuracy: 0.657313\n",
            "Epoch 503 Test Loss: 0.971067, Test Accuracy: 0.661042, time: 8.2s\n",
            "  Epoch 504 @ step 788000: Train Loss: 0.979010, Train Accuracy: 0.657500\n",
            "  Epoch 504 @ step 789000: Train Loss: 0.989472, Train Accuracy: 0.654344\n",
            "Epoch 504 Test Loss: 1.099898, Test Accuracy: 0.613818, time: 8.3s\n",
            "  Epoch 505 @ step 790000: Train Loss: 0.962192, Train Accuracy: 0.663500\n",
            "Epoch 505 Test Loss: 1.000773, Test Accuracy: 0.656749, time: 7.8s\n",
            "  Epoch 506 @ step 791000: Train Loss: 0.986788, Train Accuracy: 0.653844\n",
            "  Epoch 506 @ step 792000: Train Loss: 0.983604, Train Accuracy: 0.656438\n",
            "Epoch 506 Test Loss: 1.077523, Test Accuracy: 0.620607, time: 7.8s\n",
            "  Epoch 507 @ step 793000: Train Loss: 0.965422, Train Accuracy: 0.662438\n",
            "  Epoch 507 @ step 794000: Train Loss: 0.987368, Train Accuracy: 0.657063\n",
            "Epoch 507 Test Loss: 1.036138, Test Accuracy: 0.632488, time: 7.8s\n",
            "  Epoch 508 @ step 795000: Train Loss: 0.980896, Train Accuracy: 0.656000\n",
            "Epoch 508 Test Loss: 1.093633, Test Accuracy: 0.608626, time: 7.8s\n",
            "  Epoch 509 @ step 796000: Train Loss: 0.971875, Train Accuracy: 0.659813\n",
            "  Epoch 509 @ step 797000: Train Loss: 0.980230, Train Accuracy: 0.658750\n",
            "Epoch 509 Test Loss: 1.003510, Test Accuracy: 0.643171, time: 8.6s\n",
            "  Epoch 510 @ step 798000: Train Loss: 0.978045, Train Accuracy: 0.659156\n",
            "Epoch 510 Test Loss: 1.052788, Test Accuracy: 0.629093, time: 7.8s\n",
            "  Epoch 511 @ step 799000: Train Loss: 0.979477, Train Accuracy: 0.657719\n",
            "  Epoch 511 @ step 800000: Train Loss: 0.977306, Train Accuracy: 0.655813\n",
            "Epoch 511 Test Loss: 1.056690, Test Accuracy: 0.619908, time: 7.7s\n",
            "  Epoch 512 @ step 801000: Train Loss: 0.981444, Train Accuracy: 0.656000\n",
            "Epoch 512 Test Loss: 0.992111, Test Accuracy: 0.656050, time: 7.8s\n",
            "  Epoch 513 @ step 802000: Train Loss: 0.976846, Train Accuracy: 0.660063\n",
            "  Epoch 513 @ step 803000: Train Loss: 0.984798, Train Accuracy: 0.653406\n",
            "Epoch 513 Test Loss: 1.118644, Test Accuracy: 0.606530, time: 7.8s\n",
            "  Epoch 514 @ step 804000: Train Loss: 0.969722, Train Accuracy: 0.660688\n",
            "Epoch 514 Test Loss: 1.062951, Test Accuracy: 0.628395, time: 7.7s\n",
            "  Epoch 515 @ step 805000: Train Loss: 0.982872, Train Accuracy: 0.658000\n",
            "  Epoch 515 @ step 806000: Train Loss: 0.977129, Train Accuracy: 0.658719\n",
            "Epoch 515 Test Loss: 1.054530, Test Accuracy: 0.618510, time: 8.7s\n",
            "  Epoch 516 @ step 807000: Train Loss: 0.974736, Train Accuracy: 0.660094\n",
            "  Epoch 516 @ step 808000: Train Loss: 0.986123, Train Accuracy: 0.653688\n",
            "Epoch 516 Test Loss: 0.989416, Test Accuracy: 0.653255, time: 7.7s\n",
            "  Epoch 517 @ step 809000: Train Loss: 0.978446, Train Accuracy: 0.656563\n",
            "Epoch 517 Test Loss: 1.105363, Test Accuracy: 0.604433, time: 7.8s\n",
            "  Epoch 518 @ step 810000: Train Loss: 0.974418, Train Accuracy: 0.658250\n",
            "  Epoch 518 @ step 811000: Train Loss: 0.982722, Train Accuracy: 0.655594\n",
            "Epoch 518 Test Loss: 1.009261, Test Accuracy: 0.640475, time: 8.3s\n",
            "  Epoch 519 @ step 812000: Train Loss: 0.981073, Train Accuracy: 0.654094\n",
            "Epoch 519 Test Loss: 0.985536, Test Accuracy: 0.655451, time: 8.2s\n",
            "  Epoch 520 @ step 813000: Train Loss: 0.981178, Train Accuracy: 0.657719\n",
            "  Epoch 520 @ step 814000: Train Loss: 0.982418, Train Accuracy: 0.655688\n",
            "Epoch 520 Test Loss: 1.040892, Test Accuracy: 0.631490, time: 8.6s\n",
            "  Epoch 521 @ step 815000: Train Loss: 0.969865, Train Accuracy: 0.661313\n",
            "Epoch 521 Test Loss: 1.061756, Test Accuracy: 0.620208, time: 8.1s\n",
            "  Epoch 522 @ step 816000: Train Loss: 0.979810, Train Accuracy: 0.659063\n",
            "  Epoch 522 @ step 817000: Train Loss: 0.982291, Train Accuracy: 0.658219\n",
            "Epoch 522 Test Loss: 1.058664, Test Accuracy: 0.610224, time: 7.9s\n",
            "  Epoch 523 @ step 818000: Train Loss: 0.968618, Train Accuracy: 0.662281\n",
            "  Epoch 523 @ step 819000: Train Loss: 0.987445, Train Accuracy: 0.653750\n",
            "Epoch 523 Test Loss: 0.997321, Test Accuracy: 0.653554, time: 7.8s\n",
            "  Epoch 524 @ step 820000: Train Loss: 0.973722, Train Accuracy: 0.659688\n",
            "Epoch 524 Test Loss: 1.004933, Test Accuracy: 0.649960, time: 7.8s\n",
            "  Epoch 525 @ step 821000: Train Loss: 0.978816, Train Accuracy: 0.658563\n",
            "  Epoch 525 @ step 822000: Train Loss: 0.983957, Train Accuracy: 0.655406\n",
            "Epoch 525 Test Loss: 0.983807, Test Accuracy: 0.661442, time: 7.8s\n",
            "  Epoch 526 @ step 823000: Train Loss: 0.974465, Train Accuracy: 0.658938\n",
            "Epoch 526 Test Loss: 1.036341, Test Accuracy: 0.636482, time: 8.6s\n",
            "  Epoch 527 @ step 824000: Train Loss: 0.977458, Train Accuracy: 0.659500\n",
            "  Epoch 527 @ step 825000: Train Loss: 0.985593, Train Accuracy: 0.656844\n",
            "Epoch 527 Test Loss: 0.989148, Test Accuracy: 0.658646, time: 7.7s\n",
            "  Epoch 528 @ step 826000: Train Loss: 0.968114, Train Accuracy: 0.661063\n",
            "Epoch 528 Test Loss: 1.042130, Test Accuracy: 0.627696, time: 7.7s\n",
            "  Epoch 529 @ step 827000: Train Loss: 0.988877, Train Accuracy: 0.654594\n",
            "  Epoch 529 @ step 828000: Train Loss: 0.972889, Train Accuracy: 0.658688\n",
            "Epoch 529 Test Loss: 1.107545, Test Accuracy: 0.609625, time: 7.7s\n",
            "  Epoch 530 @ step 829000: Train Loss: 0.981155, Train Accuracy: 0.655156\n",
            "Epoch 530 Test Loss: 0.989168, Test Accuracy: 0.653954, time: 7.7s\n",
            "  Epoch 531 @ step 830000: Train Loss: 0.981942, Train Accuracy: 0.655563\n",
            "  Epoch 531 @ step 831000: Train Loss: 0.974740, Train Accuracy: 0.658719\n",
            "Epoch 531 Test Loss: 0.993746, Test Accuracy: 0.651058, time: 7.7s\n",
            "  Epoch 532 @ step 832000: Train Loss: 0.983321, Train Accuracy: 0.657375\n",
            "  Epoch 532 @ step 833000: Train Loss: 0.976450, Train Accuracy: 0.660094\n",
            "Epoch 532 Test Loss: 1.033647, Test Accuracy: 0.623502, time: 8.6s\n",
            "  Epoch 533 @ step 834000: Train Loss: 0.974471, Train Accuracy: 0.656563\n",
            "Epoch 533 Test Loss: 0.964778, Test Accuracy: 0.663339, time: 7.7s\n",
            "  Epoch 534 @ step 835000: Train Loss: 0.975336, Train Accuracy: 0.662563\n",
            "  Epoch 534 @ step 836000: Train Loss: 0.980864, Train Accuracy: 0.657375\n",
            "Epoch 534 Test Loss: 1.086636, Test Accuracy: 0.619708, time: 7.7s\n",
            "  Epoch 535 @ step 837000: Train Loss: 0.982396, Train Accuracy: 0.657344\n",
            "Epoch 535 Test Loss: 1.006692, Test Accuracy: 0.649361, time: 7.7s\n",
            "  Epoch 536 @ step 838000: Train Loss: 0.967762, Train Accuracy: 0.661000\n",
            "  Epoch 536 @ step 839000: Train Loss: 0.982702, Train Accuracy: 0.657469\n",
            "Epoch 536 Test Loss: 1.019951, Test Accuracy: 0.634485, time: 7.7s\n",
            "  Epoch 537 @ step 840000: Train Loss: 0.977373, Train Accuracy: 0.655500\n",
            "Epoch 537 Test Loss: 1.014444, Test Accuracy: 0.646865, time: 8.0s\n",
            "  Epoch 538 @ step 841000: Train Loss: 0.981212, Train Accuracy: 0.656906\n",
            "  Epoch 538 @ step 842000: Train Loss: 0.971649, Train Accuracy: 0.663813\n",
            "Epoch 538 Test Loss: 1.046994, Test Accuracy: 0.636082, time: 8.3s\n",
            "  Epoch 539 @ step 843000: Train Loss: 0.981886, Train Accuracy: 0.655750\n",
            "  Epoch 539 @ step 844000: Train Loss: 0.983927, Train Accuracy: 0.657313\n",
            "Epoch 539 Test Loss: 1.028333, Test Accuracy: 0.641773, time: 7.8s\n",
            "  Epoch 540 @ step 845000: Train Loss: 0.972960, Train Accuracy: 0.659625\n",
            "Epoch 540 Test Loss: 0.975425, Test Accuracy: 0.660942, time: 8.7s\n",
            "  Epoch 541 @ step 846000: Train Loss: 0.976718, Train Accuracy: 0.658594\n",
            "  Epoch 541 @ step 847000: Train Loss: 0.988326, Train Accuracy: 0.654156\n",
            "Epoch 541 Test Loss: 1.120108, Test Accuracy: 0.607129, time: 7.9s\n",
            "  Epoch 542 @ step 848000: Train Loss: 0.970509, Train Accuracy: 0.660375\n",
            "Epoch 542 Test Loss: 1.001266, Test Accuracy: 0.647564, time: 8.0s\n",
            "  Epoch 543 @ step 849000: Train Loss: 0.981604, Train Accuracy: 0.655344\n",
            "  Epoch 543 @ step 850000: Train Loss: 0.985487, Train Accuracy: 0.654750\n",
            "Epoch 543 Test Loss: 1.024611, Test Accuracy: 0.641773, time: 8.6s\n",
            "  Epoch 544 @ step 851000: Train Loss: 0.967210, Train Accuracy: 0.661906\n",
            "Epoch 544 Test Loss: 1.013900, Test Accuracy: 0.637380, time: 7.8s\n",
            "  Epoch 545 @ step 852000: Train Loss: 0.983124, Train Accuracy: 0.657000\n",
            "  Epoch 545 @ step 853000: Train Loss: 0.979155, Train Accuracy: 0.654750\n",
            "Epoch 545 Test Loss: 1.047947, Test Accuracy: 0.629593, time: 7.7s\n",
            "  Epoch 546 @ step 854000: Train Loss: 0.978495, Train Accuracy: 0.658594\n",
            "Epoch 546 Test Loss: 1.053766, Test Accuracy: 0.625799, time: 7.8s\n",
            "  Epoch 547 @ step 855000: Train Loss: 0.977395, Train Accuracy: 0.658688\n",
            "  Epoch 547 @ step 856000: Train Loss: 0.971285, Train Accuracy: 0.661781\n",
            "Epoch 547 Test Loss: 1.024812, Test Accuracy: 0.634485, time: 7.8s\n",
            "  Epoch 548 @ step 857000: Train Loss: 0.975826, Train Accuracy: 0.655438\n",
            "  Epoch 548 @ step 858000: Train Loss: 0.989414, Train Accuracy: 0.652406\n",
            "Epoch 548 Test Loss: 1.018028, Test Accuracy: 0.646466, time: 7.8s\n",
            "  Epoch 549 @ step 859000: Train Loss: 0.969876, Train Accuracy: 0.660750\n",
            "Epoch 549 Test Loss: 1.053704, Test Accuracy: 0.633486, time: 8.6s\n",
            "  Epoch 550 @ step 860000: Train Loss: 0.976090, Train Accuracy: 0.659406\n",
            "  Epoch 550 @ step 861000: Train Loss: 0.982607, Train Accuracy: 0.656000\n",
            "Epoch 550 Test Loss: 0.987105, Test Accuracy: 0.655451, time: 7.9s\n",
            "  Epoch 551 @ step 862000: Train Loss: 0.969062, Train Accuracy: 0.661000\n",
            "Epoch 551 Test Loss: 1.113288, Test Accuracy: 0.600839, time: 7.9s\n",
            "  Epoch 552 @ step 863000: Train Loss: 0.983946, Train Accuracy: 0.656219\n",
            "  Epoch 552 @ step 864000: Train Loss: 0.977718, Train Accuracy: 0.657406\n",
            "Epoch 552 Test Loss: 0.988665, Test Accuracy: 0.650359, time: 7.8s\n",
            "  Epoch 553 @ step 865000: Train Loss: 0.981986, Train Accuracy: 0.656688\n",
            "Epoch 553 Test Loss: 1.039473, Test Accuracy: 0.640875, time: 7.8s\n",
            "  Epoch 554 @ step 866000: Train Loss: 0.980206, Train Accuracy: 0.654438\n",
            "  Epoch 554 @ step 867000: Train Loss: 0.979067, Train Accuracy: 0.657469\n",
            "Epoch 554 Test Loss: 0.983409, Test Accuracy: 0.651857, time: 8.3s\n",
            "  Epoch 555 @ step 868000: Train Loss: 0.969111, Train Accuracy: 0.660188\n",
            "  Epoch 555 @ step 869000: Train Loss: 0.988335, Train Accuracy: 0.652438\n",
            "Epoch 555 Test Loss: 0.977932, Test Accuracy: 0.658247, time: 8.1s\n",
            "  Epoch 556 @ step 870000: Train Loss: 0.974350, Train Accuracy: 0.657125\n",
            "Epoch 556 Test Loss: 0.991596, Test Accuracy: 0.658446, time: 7.8s\n",
            "  Epoch 557 @ step 871000: Train Loss: 0.975444, Train Accuracy: 0.659875\n",
            "  Epoch 557 @ step 872000: Train Loss: 0.980341, Train Accuracy: 0.655969\n",
            "Epoch 557 Test Loss: 1.047745, Test Accuracy: 0.636282, time: 7.8s\n",
            "  Epoch 558 @ step 873000: Train Loss: 0.976874, Train Accuracy: 0.660344\n",
            "Epoch 558 Test Loss: 1.002881, Test Accuracy: 0.648662, time: 7.8s\n",
            "  Epoch 559 @ step 874000: Train Loss: 0.972658, Train Accuracy: 0.659625\n",
            "  Epoch 559 @ step 875000: Train Loss: 0.981918, Train Accuracy: 0.657531\n",
            "Epoch 559 Test Loss: 1.065920, Test Accuracy: 0.626897, time: 7.9s\n",
            "  Epoch 560 @ step 876000: Train Loss: 0.970592, Train Accuracy: 0.660594\n",
            "Epoch 560 Test Loss: 1.058187, Test Accuracy: 0.630891, time: 8.7s\n",
            "  Epoch 561 @ step 877000: Train Loss: 0.980056, Train Accuracy: 0.657844\n",
            "  Epoch 561 @ step 878000: Train Loss: 0.977142, Train Accuracy: 0.655906\n",
            "Epoch 561 Test Loss: 1.098980, Test Accuracy: 0.605631, time: 7.8s\n",
            "  Epoch 562 @ step 879000: Train Loss: 0.981216, Train Accuracy: 0.656594\n",
            "Epoch 562 Test Loss: 0.979832, Test Accuracy: 0.658946, time: 8.8s\n",
            "  Epoch 563 @ step 880000: Train Loss: 0.980921, Train Accuracy: 0.659813\n",
            "  Epoch 563 @ step 881000: Train Loss: 0.971546, Train Accuracy: 0.659781\n",
            "Epoch 563 Test Loss: 1.098525, Test Accuracy: 0.616613, time: 8.3s\n",
            "  Epoch 564 @ step 882000: Train Loss: 0.977049, Train Accuracy: 0.659656\n",
            "  Epoch 564 @ step 883000: Train Loss: 0.984703, Train Accuracy: 0.654563\n",
            "Epoch 564 Test Loss: 1.013085, Test Accuracy: 0.646765, time: 7.8s\n",
            "  Epoch 565 @ step 884000: Train Loss: 0.977422, Train Accuracy: 0.656688\n",
            "Epoch 565 Test Loss: 1.009151, Test Accuracy: 0.648862, time: 7.9s\n",
            "  Epoch 566 @ step 885000: Train Loss: 0.977512, Train Accuracy: 0.659594\n",
            "  Epoch 566 @ step 886000: Train Loss: 0.980308, Train Accuracy: 0.654500\n",
            "Epoch 566 Test Loss: 0.949035, Test Accuracy: 0.670827, time: 8.2s\n",
            "  Epoch 567 @ step 887000: Train Loss: 0.982226, Train Accuracy: 0.657969\n",
            "Epoch 567 Test Loss: 0.981060, Test Accuracy: 0.658247, time: 7.6s\n",
            "  Epoch 568 @ step 888000: Train Loss: 0.976226, Train Accuracy: 0.658531\n",
            "  Epoch 568 @ step 889000: Train Loss: 0.984886, Train Accuracy: 0.653281\n",
            "Epoch 568 Test Loss: 0.984047, Test Accuracy: 0.661841, time: 7.7s\n",
            "  Epoch 569 @ step 890000: Train Loss: 0.974550, Train Accuracy: 0.657188\n",
            "Epoch 569 Test Loss: 1.021555, Test Accuracy: 0.636981, time: 7.7s\n",
            "  Epoch 570 @ step 891000: Train Loss: 0.977684, Train Accuracy: 0.656438\n",
            "  Epoch 570 @ step 892000: Train Loss: 0.982938, Train Accuracy: 0.655469\n",
            "Epoch 570 Test Loss: 1.020170, Test Accuracy: 0.639477, time: 7.6s\n",
            "  Epoch 571 @ step 893000: Train Loss: 0.973904, Train Accuracy: 0.659563\n",
            "  Epoch 571 @ step 894000: Train Loss: 0.983359, Train Accuracy: 0.655656\n",
            "Epoch 571 Test Loss: 1.060910, Test Accuracy: 0.624700, time: 8.4s\n",
            "  Epoch 572 @ step 895000: Train Loss: 0.969401, Train Accuracy: 0.662219\n",
            "Epoch 572 Test Loss: 0.977455, Test Accuracy: 0.659145, time: 7.8s\n",
            "  Epoch 573 @ step 896000: Train Loss: 0.981811, Train Accuracy: 0.656125\n",
            "  Epoch 573 @ step 897000: Train Loss: 0.983731, Train Accuracy: 0.655719\n",
            "Epoch 573 Test Loss: 1.020563, Test Accuracy: 0.639876, time: 7.7s\n",
            "  Epoch 574 @ step 898000: Train Loss: 0.978963, Train Accuracy: 0.656281\n",
            "Epoch 574 Test Loss: 1.005448, Test Accuracy: 0.646466, time: 7.5s\n",
            "  Epoch 575 @ step 899000: Train Loss: 0.976914, Train Accuracy: 0.658000\n",
            "  Epoch 575 @ step 900000: Train Loss: 0.977445, Train Accuracy: 0.657406\n",
            "Epoch 575 Test Loss: 0.968111, Test Accuracy: 0.665635, time: 7.6s\n",
            "  Epoch 576 @ step 901000: Train Loss: 0.975160, Train Accuracy: 0.660250\n",
            "Epoch 576 Test Loss: 1.011408, Test Accuracy: 0.639976, time: 7.8s\n",
            "  Epoch 577 @ step 902000: Train Loss: 0.982932, Train Accuracy: 0.656406\n",
            "  Epoch 577 @ step 903000: Train Loss: 0.977138, Train Accuracy: 0.661469\n",
            "Epoch 577 Test Loss: 1.004338, Test Accuracy: 0.647664, time: 8.6s\n",
            "  Epoch 578 @ step 904000: Train Loss: 0.978110, Train Accuracy: 0.655656\n",
            "Epoch 578 Test Loss: 1.011518, Test Accuracy: 0.641673, time: 7.8s\n",
            "  Epoch 579 @ step 905000: Train Loss: 0.982162, Train Accuracy: 0.657656\n",
            "  Epoch 579 @ step 906000: Train Loss: 0.975386, Train Accuracy: 0.662625\n",
            "Epoch 579 Test Loss: 1.145187, Test Accuracy: 0.586462, time: 7.7s\n",
            "  Epoch 580 @ step 907000: Train Loss: 0.973826, Train Accuracy: 0.657688\n",
            "  Epoch 580 @ step 908000: Train Loss: 0.984485, Train Accuracy: 0.654969\n",
            "Epoch 580 Test Loss: 1.005275, Test Accuracy: 0.654653, time: 7.7s\n",
            "  Epoch 581 @ step 909000: Train Loss: 0.977254, Train Accuracy: 0.658219\n",
            "Epoch 581 Test Loss: 1.009375, Test Accuracy: 0.649661, time: 7.7s\n",
            "  Epoch 582 @ step 910000: Train Loss: 0.975607, Train Accuracy: 0.657406\n",
            "  Epoch 582 @ step 911000: Train Loss: 0.984788, Train Accuracy: 0.654531\n",
            "Epoch 582 Test Loss: 1.003294, Test Accuracy: 0.648462, time: 7.9s\n",
            "  Epoch 583 @ step 912000: Train Loss: 0.974457, Train Accuracy: 0.659563\n",
            "Epoch 583 Test Loss: 1.019536, Test Accuracy: 0.643071, time: 8.6s\n",
            "  Epoch 584 @ step 913000: Train Loss: 0.981531, Train Accuracy: 0.654781\n",
            "  Epoch 584 @ step 914000: Train Loss: 0.983171, Train Accuracy: 0.655156\n",
            "Epoch 584 Test Loss: 1.197539, Test Accuracy: 0.577476, time: 9.1s\n",
            "  Epoch 585 @ step 915000: Train Loss: 0.969658, Train Accuracy: 0.660250\n",
            "Epoch 585 Test Loss: 1.087044, Test Accuracy: 0.608726, time: 7.8s\n",
            "  Epoch 586 @ step 916000: Train Loss: 0.982348, Train Accuracy: 0.655500\n",
            "  Epoch 586 @ step 917000: Train Loss: 0.984589, Train Accuracy: 0.653938\n",
            "Epoch 586 Test Loss: 1.026240, Test Accuracy: 0.636781, time: 7.7s\n",
            "  Epoch 587 @ step 918000: Train Loss: 0.971664, Train Accuracy: 0.658563\n",
            "  Epoch 587 @ step 919000: Train Loss: 0.983068, Train Accuracy: 0.656156\n",
            "Epoch 587 Test Loss: 1.041633, Test Accuracy: 0.626697, time: 7.6s\n",
            "  Epoch 588 @ step 920000: Train Loss: 0.964344, Train Accuracy: 0.661281\n",
            "Epoch 588 Test Loss: 1.042035, Test Accuracy: 0.632688, time: 8.4s\n",
            "  Epoch 589 @ step 921000: Train Loss: 0.989737, Train Accuracy: 0.651094\n",
            "  Epoch 589 @ step 922000: Train Loss: 0.981484, Train Accuracy: 0.656813\n",
            "Epoch 589 Test Loss: 1.027861, Test Accuracy: 0.635483, time: 7.6s\n",
            "  Epoch 590 @ step 923000: Train Loss: 0.978890, Train Accuracy: 0.659281\n",
            "Epoch 590 Test Loss: 1.006431, Test Accuracy: 0.652456, time: 7.6s\n",
            "  Epoch 591 @ step 924000: Train Loss: 0.972703, Train Accuracy: 0.659531\n",
            "  Epoch 591 @ step 925000: Train Loss: 0.981146, Train Accuracy: 0.657813\n",
            "Epoch 591 Test Loss: 1.001683, Test Accuracy: 0.650559, time: 7.6s\n",
            "  Epoch 592 @ step 926000: Train Loss: 0.975704, Train Accuracy: 0.658375\n",
            "Epoch 592 Test Loss: 1.066367, Test Accuracy: 0.619908, time: 7.6s\n",
            "  Epoch 593 @ step 927000: Train Loss: 0.979248, Train Accuracy: 0.658000\n",
            "  Epoch 593 @ step 928000: Train Loss: 0.979982, Train Accuracy: 0.655688\n",
            "Epoch 593 Test Loss: 1.052212, Test Accuracy: 0.624301, time: 7.6s\n",
            "  Epoch 594 @ step 929000: Train Loss: 0.976573, Train Accuracy: 0.658094\n",
            "Epoch 594 Test Loss: 1.013498, Test Accuracy: 0.641374, time: 8.6s\n",
            "  Epoch 595 @ step 930000: Train Loss: 0.982591, Train Accuracy: 0.656344\n",
            "  Epoch 595 @ step 931000: Train Loss: 0.975299, Train Accuracy: 0.655813\n",
            "Epoch 595 Test Loss: 1.032113, Test Accuracy: 0.639677, time: 7.5s\n",
            "  Epoch 596 @ step 932000: Train Loss: 0.976206, Train Accuracy: 0.658906\n",
            "  Epoch 596 @ step 933000: Train Loss: 0.986090, Train Accuracy: 0.654375\n",
            "Epoch 596 Test Loss: 1.034357, Test Accuracy: 0.635982, time: 7.6s\n",
            "  Epoch 597 @ step 934000: Train Loss: 0.971230, Train Accuracy: 0.661500\n",
            "Epoch 597 Test Loss: 1.053016, Test Accuracy: 0.629493, time: 7.7s\n",
            "  Epoch 598 @ step 935000: Train Loss: 0.982159, Train Accuracy: 0.655000\n",
            "  Epoch 598 @ step 936000: Train Loss: 0.979440, Train Accuracy: 0.655844\n",
            "Epoch 598 Test Loss: 0.994685, Test Accuracy: 0.649461, time: 7.7s\n",
            "  Epoch 599 @ step 937000: Train Loss: 0.976430, Train Accuracy: 0.657063\n",
            "Epoch 599 Test Loss: 1.051512, Test Accuracy: 0.628295, time: 7.7s\n",
            "  Epoch 600 @ step 938000: Train Loss: 0.970888, Train Accuracy: 0.660219\n",
            "  Epoch 600 @ step 939000: Train Loss: 0.978150, Train Accuracy: 0.656094\n",
            "Epoch 600 Test Loss: 1.040263, Test Accuracy: 0.623902, time: 8.5s\n",
            "  Epoch 601 @ step 940000: Train Loss: 0.973128, Train Accuracy: 0.656844\n",
            "Epoch 601 Test Loss: 1.044593, Test Accuracy: 0.634285, time: 7.6s\n",
            "  Epoch 602 @ step 941000: Train Loss: 0.989368, Train Accuracy: 0.653156\n",
            "  Epoch 602 @ step 942000: Train Loss: 0.975306, Train Accuracy: 0.657094\n",
            "Epoch 602 Test Loss: 1.138857, Test Accuracy: 0.612121, time: 7.6s\n",
            "  Epoch 603 @ step 943000: Train Loss: 0.981384, Train Accuracy: 0.657688\n",
            "  Epoch 603 @ step 944000: Train Loss: 0.978144, Train Accuracy: 0.657813\n",
            "Epoch 603 Test Loss: 1.051633, Test Accuracy: 0.633287, time: 7.6s\n",
            "  Epoch 604 @ step 945000: Train Loss: 0.975879, Train Accuracy: 0.657031\n",
            "Epoch 604 Test Loss: 1.119322, Test Accuracy: 0.604333, time: 7.5s\n",
            "  Epoch 605 @ step 946000: Train Loss: 0.975701, Train Accuracy: 0.657625\n",
            "  Epoch 605 @ step 947000: Train Loss: 0.984208, Train Accuracy: 0.655000\n",
            "Epoch 605 Test Loss: 1.077323, Test Accuracy: 0.611621, time: 8.0s\n",
            "  Epoch 606 @ step 948000: Train Loss: 0.981490, Train Accuracy: 0.659500\n",
            "Epoch 606 Test Loss: 1.011424, Test Accuracy: 0.642372, time: 8.8s\n",
            "  Epoch 607 @ step 949000: Train Loss: 0.977548, Train Accuracy: 0.655500\n",
            "  Epoch 607 @ step 950000: Train Loss: 0.978328, Train Accuracy: 0.657750\n",
            "Epoch 607 Test Loss: 1.294997, Test Accuracy: 0.563299, time: 7.6s\n",
            "  Epoch 608 @ step 951000: Train Loss: 0.970916, Train Accuracy: 0.660438\n",
            "Epoch 608 Test Loss: 1.054460, Test Accuracy: 0.628295, time: 7.5s\n",
            "  Epoch 609 @ step 952000: Train Loss: 0.985389, Train Accuracy: 0.656594\n",
            "  Epoch 609 @ step 953000: Train Loss: 0.980076, Train Accuracy: 0.656000\n",
            "Epoch 609 Test Loss: 0.991256, Test Accuracy: 0.644269, time: 7.7s\n",
            "  Epoch 610 @ step 954000: Train Loss: 0.976509, Train Accuracy: 0.658438\n",
            "Epoch 610 Test Loss: 0.986161, Test Accuracy: 0.656150, time: 7.5s\n",
            "  Epoch 611 @ step 955000: Train Loss: 0.983055, Train Accuracy: 0.656750\n",
            "  Epoch 611 @ step 956000: Train Loss: 0.971357, Train Accuracy: 0.660094\n",
            "Epoch 611 Test Loss: 1.014312, Test Accuracy: 0.646166, time: 8.4s\n",
            "  Epoch 612 @ step 957000: Train Loss: 0.979820, Train Accuracy: 0.657063\n",
            "  Epoch 612 @ step 958000: Train Loss: 0.983916, Train Accuracy: 0.652594\n",
            "Epoch 612 Test Loss: 1.045847, Test Accuracy: 0.630192, time: 7.6s\n",
            "  Epoch 613 @ step 959000: Train Loss: 0.974736, Train Accuracy: 0.658719\n",
            "Epoch 613 Test Loss: 1.114512, Test Accuracy: 0.613319, time: 7.7s\n",
            "  Epoch 614 @ step 960000: Train Loss: 0.979464, Train Accuracy: 0.657625\n",
            "  Epoch 614 @ step 961000: Train Loss: 0.985608, Train Accuracy: 0.656406\n",
            "Epoch 614 Test Loss: 1.110085, Test Accuracy: 0.598942, time: 7.6s\n",
            "  Epoch 615 @ step 962000: Train Loss: 0.977879, Train Accuracy: 0.656813\n",
            "Epoch 615 Test Loss: 1.044765, Test Accuracy: 0.630990, time: 7.6s\n",
            "  Epoch 616 @ step 963000: Train Loss: 0.980088, Train Accuracy: 0.658000\n",
            "  Epoch 616 @ step 964000: Train Loss: 0.980176, Train Accuracy: 0.655844\n",
            "Epoch 616 Test Loss: 0.992544, Test Accuracy: 0.654852, time: 7.6s\n",
            "  Epoch 617 @ step 965000: Train Loss: 0.977460, Train Accuracy: 0.658094\n",
            "Epoch 617 Test Loss: 1.051091, Test Accuracy: 0.628395, time: 8.6s\n",
            "  Epoch 618 @ step 966000: Train Loss: 0.980232, Train Accuracy: 0.656688\n",
            "  Epoch 618 @ step 967000: Train Loss: 0.979964, Train Accuracy: 0.657375\n",
            "Epoch 618 Test Loss: 1.127622, Test Accuracy: 0.607628, time: 7.7s\n",
            "  Epoch 619 @ step 968000: Train Loss: 0.968698, Train Accuracy: 0.660875\n",
            "  Epoch 619 @ step 969000: Train Loss: 0.982067, Train Accuracy: 0.658250\n",
            "Epoch 619 Test Loss: 1.083188, Test Accuracy: 0.616713, time: 7.6s\n",
            "  Epoch 620 @ step 970000: Train Loss: 0.978978, Train Accuracy: 0.660031\n",
            "Epoch 620 Test Loss: 1.014978, Test Accuracy: 0.644469, time: 7.7s\n",
            "  Epoch 621 @ step 971000: Train Loss: 0.978828, Train Accuracy: 0.657125\n",
            "  Epoch 621 @ step 972000: Train Loss: 0.976260, Train Accuracy: 0.658281\n",
            "Epoch 621 Test Loss: 0.968329, Test Accuracy: 0.666733, time: 7.6s\n",
            "  Epoch 622 @ step 973000: Train Loss: 0.977118, Train Accuracy: 0.657156\n",
            "Epoch 622 Test Loss: 1.038146, Test Accuracy: 0.630691, time: 7.6s\n",
            "  Epoch 623 @ step 974000: Train Loss: 0.977275, Train Accuracy: 0.660656\n",
            "  Epoch 623 @ step 975000: Train Loss: 0.977937, Train Accuracy: 0.655750\n",
            "Epoch 623 Test Loss: 1.013369, Test Accuracy: 0.642372, time: 8.5s\n",
            "  Epoch 624 @ step 976000: Train Loss: 0.973330, Train Accuracy: 0.657344\n",
            "Epoch 624 Test Loss: 1.053805, Test Accuracy: 0.621805, time: 7.6s\n",
            "  Epoch 625 @ step 977000: Train Loss: 0.988894, Train Accuracy: 0.654469\n",
            "  Epoch 625 @ step 978000: Train Loss: 0.973599, Train Accuracy: 0.659594\n",
            "Epoch 625 Test Loss: 1.188783, Test Accuracy: 0.575978, time: 7.5s\n",
            "  Epoch 626 @ step 979000: Train Loss: 0.984064, Train Accuracy: 0.652531\n",
            "  Epoch 626 @ step 980000: Train Loss: 0.979104, Train Accuracy: 0.659125\n",
            "Epoch 626 Test Loss: 1.075369, Test Accuracy: 0.611022, time: 7.5s\n",
            "  Epoch 627 @ step 981000: Train Loss: 0.973032, Train Accuracy: 0.660656\n",
            "Epoch 627 Test Loss: 1.012797, Test Accuracy: 0.644669, time: 7.5s\n",
            "  Epoch 628 @ step 982000: Train Loss: 0.980981, Train Accuracy: 0.660063\n",
            "  Epoch 628 @ step 983000: Train Loss: 0.981945, Train Accuracy: 0.656094\n",
            "Epoch 628 Test Loss: 1.028231, Test Accuracy: 0.640575, time: 8.9s\n",
            "  Epoch 629 @ step 984000: Train Loss: 0.968922, Train Accuracy: 0.659375\n",
            "Epoch 629 Test Loss: 1.009843, Test Accuracy: 0.651258, time: 7.9s\n",
            "  Epoch 630 @ step 985000: Train Loss: 0.976788, Train Accuracy: 0.657656\n",
            "  Epoch 630 @ step 986000: Train Loss: 0.980393, Train Accuracy: 0.656750\n",
            "Epoch 630 Test Loss: 0.976939, Test Accuracy: 0.658147, time: 7.5s\n",
            "  Epoch 631 @ step 987000: Train Loss: 0.976957, Train Accuracy: 0.661125\n",
            "Epoch 631 Test Loss: 0.983787, Test Accuracy: 0.658347, time: 7.6s\n",
            "  Epoch 632 @ step 988000: Train Loss: 0.973443, Train Accuracy: 0.657969\n",
            "  Epoch 632 @ step 989000: Train Loss: 0.980286, Train Accuracy: 0.656563\n",
            "Epoch 632 Test Loss: 0.988758, Test Accuracy: 0.654553, time: 7.5s\n",
            "  Epoch 633 @ step 990000: Train Loss: 0.974766, Train Accuracy: 0.661156\n",
            "Epoch 633 Test Loss: 1.013656, Test Accuracy: 0.646466, time: 7.6s\n",
            "  Epoch 634 @ step 991000: Train Loss: 0.984298, Train Accuracy: 0.654625\n",
            "  Epoch 634 @ step 992000: Train Loss: 0.980081, Train Accuracy: 0.657250\n",
            "Epoch 634 Test Loss: 0.997851, Test Accuracy: 0.652356, time: 8.3s\n",
            "  Epoch 635 @ step 993000: Train Loss: 0.968874, Train Accuracy: 0.661688\n",
            "  Epoch 635 @ step 994000: Train Loss: 0.984844, Train Accuracy: 0.654750\n",
            "Epoch 635 Test Loss: 0.979353, Test Accuracy: 0.662839, time: 7.8s\n",
            "  Epoch 636 @ step 995000: Train Loss: 0.978420, Train Accuracy: 0.659063\n",
            "Epoch 636 Test Loss: 1.057426, Test Accuracy: 0.629992, time: 7.6s\n",
            "  Epoch 637 @ step 996000: Train Loss: 0.967596, Train Accuracy: 0.664438\n",
            "  Epoch 637 @ step 997000: Train Loss: 0.986926, Train Accuracy: 0.655188\n",
            "Epoch 637 Test Loss: 0.990452, Test Accuracy: 0.650559, time: 7.6s\n",
            "  Epoch 638 @ step 998000: Train Loss: 0.977908, Train Accuracy: 0.656719\n",
            "Epoch 638 Test Loss: 1.044626, Test Accuracy: 0.633686, time: 7.5s\n",
            "  Epoch 639 @ step 999000: Train Loss: 0.977964, Train Accuracy: 0.657313\n",
            "  Epoch 639 @ step 1000000: Train Loss: 0.979296, Train Accuracy: 0.659781\n",
            "Epoch 639 Test Loss: 1.080957, Test Accuracy: 0.622304, time: 7.5s\n",
            "  Epoch 640 @ step 1001000: Train Loss: 0.972161, Train Accuracy: 0.658938\n",
            "Epoch 640 Test Loss: 0.969553, Test Accuracy: 0.665236, time: 8.6s\n",
            "  Epoch 641 @ step 1002000: Train Loss: 0.984680, Train Accuracy: 0.656344\n",
            "  Epoch 641 @ step 1003000: Train Loss: 0.975630, Train Accuracy: 0.660250\n",
            "Epoch 641 Test Loss: 1.087539, Test Accuracy: 0.607528, time: 7.7s\n",
            "  Epoch 642 @ step 1004000: Train Loss: 0.977526, Train Accuracy: 0.660281\n",
            "  Epoch 642 @ step 1005000: Train Loss: 0.982410, Train Accuracy: 0.656375\n",
            "Epoch 642 Test Loss: 1.062557, Test Accuracy: 0.620208, time: 7.7s\n",
            "  Epoch 643 @ step 1006000: Train Loss: 0.977606, Train Accuracy: 0.658469\n",
            "Epoch 643 Test Loss: 1.035637, Test Accuracy: 0.633387, time: 7.6s\n",
            "  Epoch 644 @ step 1007000: Train Loss: 0.976293, Train Accuracy: 0.658875\n",
            "  Epoch 644 @ step 1008000: Train Loss: 0.981810, Train Accuracy: 0.656063\n",
            "Epoch 644 Test Loss: 1.000621, Test Accuracy: 0.646565, time: 7.6s\n",
            "  Epoch 645 @ step 1009000: Train Loss: 0.978839, Train Accuracy: 0.656750\n",
            "Epoch 645 Test Loss: 0.999971, Test Accuracy: 0.652855, time: 7.5s\n",
            "  Epoch 646 @ step 1010000: Train Loss: 0.974736, Train Accuracy: 0.658594\n",
            "  Epoch 646 @ step 1011000: Train Loss: 0.985443, Train Accuracy: 0.652938\n",
            "Epoch 646 Test Loss: 1.031173, Test Accuracy: 0.638878, time: 8.6s\n",
            "  Epoch 647 @ step 1012000: Train Loss: 0.971276, Train Accuracy: 0.661844\n",
            "Epoch 647 Test Loss: 1.039334, Test Accuracy: 0.621306, time: 7.6s\n",
            "  Epoch 648 @ step 1013000: Train Loss: 0.982972, Train Accuracy: 0.654531\n",
            "  Epoch 648 @ step 1014000: Train Loss: 0.972905, Train Accuracy: 0.658656\n",
            "Epoch 648 Test Loss: 0.981724, Test Accuracy: 0.660244, time: 7.7s\n",
            "  Epoch 649 @ step 1015000: Train Loss: 0.977610, Train Accuracy: 0.658563\n",
            "Epoch 649 Test Loss: 1.016899, Test Accuracy: 0.649161, time: 7.7s\n",
            "  Epoch 650 @ step 1016000: Train Loss: 0.982543, Train Accuracy: 0.653844\n",
            "  Epoch 650 @ step 1017000: Train Loss: 0.975422, Train Accuracy: 0.659406\n",
            "Epoch 650 Test Loss: 1.036532, Test Accuracy: 0.629892, time: 9.1s\n",
            "  Epoch 651 @ step 1018000: Train Loss: 0.981783, Train Accuracy: 0.657250\n",
            "  Epoch 651 @ step 1019000: Train Loss: 0.980754, Train Accuracy: 0.658375\n",
            "Epoch 651 Test Loss: 1.087380, Test Accuracy: 0.594449, time: 8.1s\n",
            "  Epoch 652 @ step 1020000: Train Loss: 0.973263, Train Accuracy: 0.659781\n",
            "Epoch 652 Test Loss: 1.075706, Test Accuracy: 0.624501, time: 8.1s\n",
            "  Epoch 653 @ step 1021000: Train Loss: 0.983153, Train Accuracy: 0.657406\n",
            "  Epoch 653 @ step 1022000: Train Loss: 0.985941, Train Accuracy: 0.653469\n",
            "Epoch 653 Test Loss: 1.036420, Test Accuracy: 0.638279, time: 7.8s\n",
            "  Epoch 654 @ step 1023000: Train Loss: 0.971954, Train Accuracy: 0.660500\n",
            "Epoch 654 Test Loss: 0.982199, Test Accuracy: 0.656450, time: 7.8s\n",
            "  Epoch 655 @ step 1024000: Train Loss: 0.981557, Train Accuracy: 0.656719\n",
            "  Epoch 655 @ step 1025000: Train Loss: 0.979051, Train Accuracy: 0.654219\n",
            "Epoch 655 Test Loss: 0.993394, Test Accuracy: 0.652556, time: 7.7s\n",
            "  Epoch 656 @ step 1026000: Train Loss: 0.970092, Train Accuracy: 0.659688\n",
            "Epoch 656 Test Loss: 0.985572, Test Accuracy: 0.659145, time: 7.7s\n",
            "  Epoch 657 @ step 1027000: Train Loss: 0.979543, Train Accuracy: 0.656406\n",
            "  Epoch 657 @ step 1028000: Train Loss: 0.971044, Train Accuracy: 0.661344\n",
            "Epoch 657 Test Loss: 1.046119, Test Accuracy: 0.630491, time: 8.5s\n",
            "  Epoch 658 @ step 1029000: Train Loss: 0.984143, Train Accuracy: 0.653969\n",
            "  Epoch 658 @ step 1030000: Train Loss: 0.983690, Train Accuracy: 0.658063\n",
            "Epoch 658 Test Loss: 1.077861, Test Accuracy: 0.622204, time: 7.7s\n",
            "  Epoch 659 @ step 1031000: Train Loss: 0.975351, Train Accuracy: 0.660594\n",
            "Epoch 659 Test Loss: 1.030500, Test Accuracy: 0.638179, time: 7.8s\n",
            "  Epoch 660 @ step 1032000: Train Loss: 0.970991, Train Accuracy: 0.659438\n",
            "  Epoch 660 @ step 1033000: Train Loss: 0.982144, Train Accuracy: 0.655344\n",
            "Epoch 660 Test Loss: 1.044716, Test Accuracy: 0.636881, time: 7.8s\n",
            "  Epoch 661 @ step 1034000: Train Loss: 0.978602, Train Accuracy: 0.656563\n",
            "Epoch 661 Test Loss: 1.005330, Test Accuracy: 0.645667, time: 8.0s\n",
            "  Epoch 662 @ step 1035000: Train Loss: 0.975820, Train Accuracy: 0.659094\n",
            "  Epoch 662 @ step 1036000: Train Loss: 0.976884, Train Accuracy: 0.657688\n",
            "Epoch 662 Test Loss: 1.026544, Test Accuracy: 0.635084, time: 7.8s\n",
            "  Epoch 663 @ step 1037000: Train Loss: 0.980973, Train Accuracy: 0.654531\n",
            "Epoch 663 Test Loss: 0.991854, Test Accuracy: 0.651857, time: 8.7s\n",
            "  Epoch 664 @ step 1038000: Train Loss: 0.982794, Train Accuracy: 0.658156\n",
            "  Epoch 664 @ step 1039000: Train Loss: 0.975012, Train Accuracy: 0.657969\n",
            "Epoch 664 Test Loss: 1.004586, Test Accuracy: 0.636082, time: 7.6s\n",
            "  Epoch 665 @ step 1040000: Train Loss: 0.980716, Train Accuracy: 0.655469\n",
            "Epoch 665 Test Loss: 1.043232, Test Accuracy: 0.625399, time: 7.7s\n",
            "  Epoch 666 @ step 1041000: Train Loss: 0.978586, Train Accuracy: 0.658188\n",
            "  Epoch 666 @ step 1042000: Train Loss: 0.974617, Train Accuracy: 0.659781\n",
            "Epoch 666 Test Loss: 1.047336, Test Accuracy: 0.628395, time: 7.7s\n",
            "  Epoch 667 @ step 1043000: Train Loss: 0.981015, Train Accuracy: 0.655219\n",
            "  Epoch 667 @ step 1044000: Train Loss: 0.981814, Train Accuracy: 0.654750\n",
            "Epoch 667 Test Loss: 0.973107, Test Accuracy: 0.660144, time: 7.7s\n",
            "  Epoch 668 @ step 1045000: Train Loss: 0.976278, Train Accuracy: 0.660000\n",
            "Epoch 668 Test Loss: 1.032298, Test Accuracy: 0.643670, time: 8.0s\n",
            "  Epoch 669 @ step 1046000: Train Loss: 0.972013, Train Accuracy: 0.659406\n",
            "  Epoch 669 @ step 1047000: Train Loss: 0.987688, Train Accuracy: 0.654813\n",
            "Epoch 669 Test Loss: 1.104169, Test Accuracy: 0.625000, time: 8.2s\n",
            "  Epoch 670 @ step 1048000: Train Loss: 0.971925, Train Accuracy: 0.660469\n",
            "Epoch 670 Test Loss: 1.076962, Test Accuracy: 0.615715, time: 7.7s\n",
            "  Epoch 671 @ step 1049000: Train Loss: 0.977777, Train Accuracy: 0.658281\n",
            "  Epoch 671 @ step 1050000: Train Loss: 0.982422, Train Accuracy: 0.654719\n",
            "Epoch 671 Test Loss: 1.019270, Test Accuracy: 0.639677, time: 8.2s\n",
            "  Epoch 672 @ step 1051000: Train Loss: 0.976320, Train Accuracy: 0.657125\n",
            "Epoch 672 Test Loss: 1.097871, Test Accuracy: 0.603035, time: 8.7s\n",
            "  Epoch 673 @ step 1052000: Train Loss: 0.985922, Train Accuracy: 0.655938\n",
            "  Epoch 673 @ step 1053000: Train Loss: 0.977970, Train Accuracy: 0.657281\n",
            "Epoch 673 Test Loss: 1.084589, Test Accuracy: 0.616613, time: 7.6s\n",
            "  Epoch 674 @ step 1054000: Train Loss: 0.980228, Train Accuracy: 0.657156\n",
            "  Epoch 674 @ step 1055000: Train Loss: 0.984279, Train Accuracy: 0.654156\n",
            "Epoch 674 Test Loss: 1.018653, Test Accuracy: 0.636581, time: 8.7s\n",
            "  Epoch 675 @ step 1056000: Train Loss: 0.968579, Train Accuracy: 0.660813\n",
            "Epoch 675 Test Loss: 0.981945, Test Accuracy: 0.655252, time: 7.7s\n",
            "  Epoch 676 @ step 1057000: Train Loss: 0.986805, Train Accuracy: 0.654156\n",
            "  Epoch 676 @ step 1058000: Train Loss: 0.981273, Train Accuracy: 0.656813\n",
            "Epoch 676 Test Loss: 1.041060, Test Accuracy: 0.636681, time: 7.6s\n",
            "  Epoch 677 @ step 1059000: Train Loss: 0.973234, Train Accuracy: 0.657688\n",
            "Epoch 677 Test Loss: 1.020097, Test Accuracy: 0.637181, time: 7.5s\n",
            "  Epoch 678 @ step 1060000: Train Loss: 0.982760, Train Accuracy: 0.654875\n",
            "  Epoch 678 @ step 1061000: Train Loss: 0.976428, Train Accuracy: 0.660938\n",
            "Epoch 678 Test Loss: 1.042909, Test Accuracy: 0.627696, time: 7.6s\n",
            "  Epoch 679 @ step 1062000: Train Loss: 0.973658, Train Accuracy: 0.659094\n",
            "Epoch 679 Test Loss: 1.147324, Test Accuracy: 0.596546, time: 7.6s\n",
            "  Epoch 680 @ step 1063000: Train Loss: 0.984485, Train Accuracy: 0.656750\n",
            "  Epoch 680 @ step 1064000: Train Loss: 0.980583, Train Accuracy: 0.655719\n",
            "Epoch 680 Test Loss: 0.999533, Test Accuracy: 0.640974, time: 8.5s\n",
            "  Epoch 681 @ step 1065000: Train Loss: 0.971660, Train Accuracy: 0.658594\n",
            "Epoch 681 Test Loss: 1.006044, Test Accuracy: 0.640276, time: 7.6s\n",
            "  Epoch 682 @ step 1066000: Train Loss: 0.983030, Train Accuracy: 0.654969\n",
            "  Epoch 682 @ step 1067000: Train Loss: 0.971379, Train Accuracy: 0.659875\n",
            "Epoch 682 Test Loss: 1.069081, Test Accuracy: 0.629992, time: 7.7s\n",
            "  Epoch 683 @ step 1068000: Train Loss: 0.975021, Train Accuracy: 0.657344\n",
            "  Epoch 683 @ step 1069000: Train Loss: 0.990906, Train Accuracy: 0.657594\n",
            "Epoch 683 Test Loss: 1.096910, Test Accuracy: 0.612320, time: 7.8s\n",
            "  Epoch 684 @ step 1070000: Train Loss: 0.974012, Train Accuracy: 0.660500\n",
            "Epoch 684 Test Loss: 1.132374, Test Accuracy: 0.602137, time: 7.7s\n",
            "  Epoch 685 @ step 1071000: Train Loss: 0.984634, Train Accuracy: 0.656688\n",
            "  Epoch 685 @ step 1072000: Train Loss: 0.971234, Train Accuracy: 0.662750\n",
            "Epoch 685 Test Loss: 0.990888, Test Accuracy: 0.654553, time: 7.9s\n",
            "  Epoch 686 @ step 1073000: Train Loss: 0.983487, Train Accuracy: 0.656594\n",
            "Epoch 686 Test Loss: 1.015601, Test Accuracy: 0.636082, time: 8.2s\n",
            "  Epoch 687 @ step 1074000: Train Loss: 0.980190, Train Accuracy: 0.656094\n",
            "  Epoch 687 @ step 1075000: Train Loss: 0.980325, Train Accuracy: 0.654719\n",
            "Epoch 687 Test Loss: 1.001707, Test Accuracy: 0.640076, time: 7.6s\n",
            "  Epoch 688 @ step 1076000: Train Loss: 0.974208, Train Accuracy: 0.661188\n",
            "Epoch 688 Test Loss: 1.092354, Test Accuracy: 0.611522, time: 7.7s\n",
            "  Epoch 689 @ step 1077000: Train Loss: 0.982020, Train Accuracy: 0.654219\n",
            "  Epoch 689 @ step 1078000: Train Loss: 0.973807, Train Accuracy: 0.660000\n",
            "Epoch 689 Test Loss: 1.108707, Test Accuracy: 0.617911, time: 7.7s\n",
            "  Epoch 690 @ step 1079000: Train Loss: 0.981277, Train Accuracy: 0.655781\n",
            "  Epoch 690 @ step 1080000: Train Loss: 0.982400, Train Accuracy: 0.654594\n",
            "Epoch 690 Test Loss: 1.092630, Test Accuracy: 0.610723, time: 7.6s\n",
            "  Epoch 691 @ step 1081000: Train Loss: 0.977392, Train Accuracy: 0.657000\n",
            "Epoch 691 Test Loss: 1.105700, Test Accuracy: 0.611522, time: 8.4s\n",
            "  Epoch 692 @ step 1082000: Train Loss: 0.980822, Train Accuracy: 0.654531\n",
            "  Epoch 692 @ step 1083000: Train Loss: 0.981557, Train Accuracy: 0.656594\n",
            "Epoch 692 Test Loss: 1.031540, Test Accuracy: 0.637181, time: 7.9s\n",
            "  Epoch 693 @ step 1084000: Train Loss: 0.974320, Train Accuracy: 0.658063\n",
            "Epoch 693 Test Loss: 1.053745, Test Accuracy: 0.635982, time: 8.2s\n",
            "  Epoch 694 @ step 1085000: Train Loss: 0.979705, Train Accuracy: 0.656844\n",
            "  Epoch 694 @ step 1086000: Train Loss: 0.977745, Train Accuracy: 0.656844\n",
            "Epoch 694 Test Loss: 1.060536, Test Accuracy: 0.618211, time: 8.7s\n",
            "  Epoch 695 @ step 1087000: Train Loss: 0.976388, Train Accuracy: 0.656625\n",
            "Epoch 695 Test Loss: 1.120217, Test Accuracy: 0.595847, time: 7.6s\n",
            "  Epoch 696 @ step 1088000: Train Loss: 0.980227, Train Accuracy: 0.657344\n",
            "  Epoch 696 @ step 1089000: Train Loss: 0.978337, Train Accuracy: 0.658500\n",
            "Epoch 696 Test Loss: 0.960836, Test Accuracy: 0.664437, time: 7.7s\n",
            "  Epoch 697 @ step 1090000: Train Loss: 0.975580, Train Accuracy: 0.659094\n",
            "Epoch 697 Test Loss: 1.047238, Test Accuracy: 0.630691, time: 8.6s\n",
            "  Epoch 698 @ step 1091000: Train Loss: 0.980916, Train Accuracy: 0.658750\n",
            "  Epoch 698 @ step 1092000: Train Loss: 0.977183, Train Accuracy: 0.658156\n",
            "Epoch 698 Test Loss: 1.242767, Test Accuracy: 0.566993, time: 7.6s\n",
            "  Epoch 699 @ step 1093000: Train Loss: 0.978038, Train Accuracy: 0.656813\n",
            "  Epoch 699 @ step 1094000: Train Loss: 0.979228, Train Accuracy: 0.656469\n",
            "Epoch 699 Test Loss: 1.108161, Test Accuracy: 0.608027, time: 7.6s\n",
            "  Epoch 700 @ step 1095000: Train Loss: 0.978142, Train Accuracy: 0.659094\n",
            "Epoch 700 Test Loss: 1.083346, Test Accuracy: 0.623602, time: 7.6s\n",
            "  Epoch 701 @ step 1096000: Train Loss: 0.976003, Train Accuracy: 0.657969\n",
            "  Epoch 701 @ step 1097000: Train Loss: 0.977946, Train Accuracy: 0.659125\n",
            "Epoch 701 Test Loss: 0.988750, Test Accuracy: 0.655252, time: 7.6s\n",
            "  Epoch 702 @ step 1098000: Train Loss: 0.974021, Train Accuracy: 0.659969\n",
            "Epoch 702 Test Loss: 1.015681, Test Accuracy: 0.645467, time: 7.8s\n",
            "  Epoch 703 @ step 1099000: Train Loss: 0.982979, Train Accuracy: 0.652656\n",
            "  Epoch 703 @ step 1100000: Train Loss: 0.975745, Train Accuracy: 0.659469\n",
            "Epoch 703 Test Loss: 1.157130, Test Accuracy: 0.584165, time: 8.3s\n",
            "  Epoch 704 @ step 1101000: Train Loss: 0.977845, Train Accuracy: 0.659438\n",
            "Epoch 704 Test Loss: 1.004408, Test Accuracy: 0.655052, time: 7.5s\n",
            "  Epoch 705 @ step 1102000: Train Loss: 0.978436, Train Accuracy: 0.657281\n",
            "  Epoch 705 @ step 1103000: Train Loss: 0.973083, Train Accuracy: 0.661094\n",
            "Epoch 705 Test Loss: 1.107844, Test Accuracy: 0.611621, time: 7.5s\n",
            "  Epoch 706 @ step 1104000: Train Loss: 0.987142, Train Accuracy: 0.654063\n",
            "  Epoch 706 @ step 1105000: Train Loss: 0.979599, Train Accuracy: 0.655281\n",
            "Epoch 706 Test Loss: 1.073619, Test Accuracy: 0.617812, time: 7.5s\n",
            "  Epoch 707 @ step 1106000: Train Loss: 0.973190, Train Accuracy: 0.660156\n",
            "Epoch 707 Test Loss: 1.101472, Test Accuracy: 0.607827, time: 7.6s\n",
            "  Epoch 708 @ step 1107000: Train Loss: 0.981439, Train Accuracy: 0.656438\n",
            "  Epoch 708 @ step 1108000: Train Loss: 0.982360, Train Accuracy: 0.656625\n",
            "Epoch 708 Test Loss: 1.037101, Test Accuracy: 0.634185, time: 8.1s\n",
            "  Epoch 709 @ step 1109000: Train Loss: 0.971244, Train Accuracy: 0.657875\n",
            "Epoch 709 Test Loss: 1.002732, Test Accuracy: 0.647464, time: 7.9s\n",
            "  Epoch 710 @ step 1110000: Train Loss: 0.979551, Train Accuracy: 0.658563\n",
            "  Epoch 710 @ step 1111000: Train Loss: 0.979118, Train Accuracy: 0.657031\n",
            "Epoch 710 Test Loss: 1.009906, Test Accuracy: 0.638878, time: 7.6s\n",
            "  Epoch 711 @ step 1112000: Train Loss: 0.975336, Train Accuracy: 0.655438\n",
            "Epoch 711 Test Loss: 0.995225, Test Accuracy: 0.652955, time: 7.6s\n",
            "  Epoch 712 @ step 1113000: Train Loss: 0.983463, Train Accuracy: 0.654031\n",
            "  Epoch 712 @ step 1114000: Train Loss: 0.981649, Train Accuracy: 0.657031\n",
            "Epoch 712 Test Loss: 1.005151, Test Accuracy: 0.651657, time: 7.6s\n",
            "  Epoch 713 @ step 1115000: Train Loss: 0.972756, Train Accuracy: 0.660875\n",
            "Epoch 713 Test Loss: 0.964318, Test Accuracy: 0.668031, time: 7.6s\n",
            "  Epoch 714 @ step 1116000: Train Loss: 0.981954, Train Accuracy: 0.655188\n",
            "  Epoch 714 @ step 1117000: Train Loss: 0.977969, Train Accuracy: 0.657625\n",
            "Epoch 714 Test Loss: 1.100116, Test Accuracy: 0.606230, time: 8.5s\n",
            "  Epoch 715 @ step 1118000: Train Loss: 0.979230, Train Accuracy: 0.654250\n",
            "  Epoch 715 @ step 1119000: Train Loss: 0.977225, Train Accuracy: 0.659344\n",
            "Epoch 715 Test Loss: 1.144803, Test Accuracy: 0.590355, time: 7.8s\n",
            "  Epoch 716 @ step 1120000: Train Loss: 0.980903, Train Accuracy: 0.658188\n",
            "Epoch 716 Test Loss: 1.077997, Test Accuracy: 0.601138, time: 8.7s\n",
            "  Epoch 717 @ step 1121000: Train Loss: 0.972109, Train Accuracy: 0.656563\n",
            "  Epoch 717 @ step 1122000: Train Loss: 0.978366, Train Accuracy: 0.657344\n",
            "Epoch 717 Test Loss: 0.977148, Test Accuracy: 0.660643, time: 7.6s\n",
            "  Epoch 718 @ step 1123000: Train Loss: 0.973808, Train Accuracy: 0.658438\n",
            "Epoch 718 Test Loss: 1.098864, Test Accuracy: 0.613319, time: 7.7s\n",
            "  Epoch 719 @ step 1124000: Train Loss: 0.977883, Train Accuracy: 0.657906\n",
            "  Epoch 719 @ step 1125000: Train Loss: 0.984858, Train Accuracy: 0.656281\n",
            "Epoch 719 Test Loss: 1.031181, Test Accuracy: 0.636881, time: 7.6s\n",
            "  Epoch 720 @ step 1126000: Train Loss: 0.978612, Train Accuracy: 0.657781\n",
            "Epoch 720 Test Loss: 1.012843, Test Accuracy: 0.635583, time: 8.6s\n",
            "  Epoch 721 @ step 1127000: Train Loss: 0.975688, Train Accuracy: 0.657094\n",
            "  Epoch 721 @ step 1128000: Train Loss: 0.977462, Train Accuracy: 0.655344\n",
            "Epoch 721 Test Loss: 0.978004, Test Accuracy: 0.656050, time: 7.6s\n",
            "  Epoch 722 @ step 1129000: Train Loss: 0.980485, Train Accuracy: 0.656281\n",
            "  Epoch 722 @ step 1130000: Train Loss: 0.980970, Train Accuracy: 0.656875\n",
            "Epoch 722 Test Loss: 1.059963, Test Accuracy: 0.626897, time: 7.7s\n",
            "  Epoch 723 @ step 1131000: Train Loss: 0.970035, Train Accuracy: 0.660094\n",
            "Epoch 723 Test Loss: 1.094633, Test Accuracy: 0.605032, time: 7.6s\n",
            "  Epoch 724 @ step 1132000: Train Loss: 0.982654, Train Accuracy: 0.654469\n",
            "  Epoch 724 @ step 1133000: Train Loss: 0.977775, Train Accuracy: 0.658688\n",
            "Epoch 724 Test Loss: 1.067682, Test Accuracy: 0.630691, time: 7.6s\n",
            "  Epoch 725 @ step 1134000: Train Loss: 0.974602, Train Accuracy: 0.659969\n",
            "Epoch 725 Test Loss: 1.061134, Test Accuracy: 0.627097, time: 8.0s\n",
            "  Epoch 726 @ step 1135000: Train Loss: 0.984010, Train Accuracy: 0.656656\n",
            "  Epoch 726 @ step 1136000: Train Loss: 0.979502, Train Accuracy: 0.655125\n",
            "Epoch 726 Test Loss: 1.034655, Test Accuracy: 0.634285, time: 8.2s\n",
            "  Epoch 727 @ step 1137000: Train Loss: 0.975170, Train Accuracy: 0.657813\n",
            "Epoch 727 Test Loss: 1.167747, Test Accuracy: 0.599740, time: 7.6s\n",
            "  Epoch 728 @ step 1138000: Train Loss: 0.980856, Train Accuracy: 0.657344\n",
            "  Epoch 728 @ step 1139000: Train Loss: 0.979487, Train Accuracy: 0.656344\n",
            "Epoch 728 Test Loss: 1.018202, Test Accuracy: 0.635483, time: 7.5s\n",
            "  Epoch 729 @ step 1140000: Train Loss: 0.976078, Train Accuracy: 0.658063\n",
            "Epoch 729 Test Loss: 1.036094, Test Accuracy: 0.631989, time: 7.5s\n",
            "  Epoch 730 @ step 1141000: Train Loss: 0.983564, Train Accuracy: 0.656813\n",
            "  Epoch 730 @ step 1142000: Train Loss: 0.971788, Train Accuracy: 0.658500\n",
            "Epoch 730 Test Loss: 1.081042, Test Accuracy: 0.613119, time: 7.5s\n",
            "  Epoch 731 @ step 1143000: Train Loss: 0.976930, Train Accuracy: 0.661281\n",
            "  Epoch 731 @ step 1144000: Train Loss: 0.984090, Train Accuracy: 0.651031\n",
            "Epoch 731 Test Loss: 1.062690, Test Accuracy: 0.626997, time: 8.3s\n",
            "  Epoch 732 @ step 1145000: Train Loss: 0.974142, Train Accuracy: 0.658125\n",
            "Epoch 732 Test Loss: 1.112733, Test Accuracy: 0.602935, time: 7.7s\n",
            "  Epoch 733 @ step 1146000: Train Loss: 0.979102, Train Accuracy: 0.659063\n",
            "  Epoch 733 @ step 1147000: Train Loss: 0.981896, Train Accuracy: 0.656250\n",
            "Epoch 733 Test Loss: 1.025672, Test Accuracy: 0.640775, time: 7.7s\n",
            "  Epoch 734 @ step 1148000: Train Loss: 0.974982, Train Accuracy: 0.660375\n",
            "Epoch 734 Test Loss: 0.967228, Test Accuracy: 0.665335, time: 7.7s\n",
            "  Epoch 735 @ step 1149000: Train Loss: 0.980330, Train Accuracy: 0.656031\n",
            "  Epoch 735 @ step 1150000: Train Loss: 0.976269, Train Accuracy: 0.655469\n",
            "Epoch 735 Test Loss: 1.068571, Test Accuracy: 0.626897, time: 7.6s\n",
            "  Epoch 736 @ step 1151000: Train Loss: 0.979645, Train Accuracy: 0.653594\n",
            "Epoch 736 Test Loss: 0.971555, Test Accuracy: 0.659345, time: 8.1s\n",
            "  Epoch 737 @ step 1152000: Train Loss: 0.980454, Train Accuracy: 0.656281\n",
            "  Epoch 737 @ step 1153000: Train Loss: 0.973880, Train Accuracy: 0.658500\n",
            "Epoch 737 Test Loss: 1.075853, Test Accuracy: 0.609724, time: 8.6s\n",
            "  Epoch 738 @ step 1154000: Train Loss: 0.976856, Train Accuracy: 0.659469\n",
            "  Epoch 738 @ step 1155000: Train Loss: 0.982913, Train Accuracy: 0.655594\n",
            "Epoch 738 Test Loss: 0.979913, Test Accuracy: 0.659944, time: 8.7s\n",
            "  Epoch 739 @ step 1156000: Train Loss: 0.979324, Train Accuracy: 0.655594\n",
            "Epoch 739 Test Loss: 1.126553, Test Accuracy: 0.605731, time: 7.6s\n",
            "  Epoch 740 @ step 1157000: Train Loss: 0.969301, Train Accuracy: 0.659531\n",
            "  Epoch 740 @ step 1158000: Train Loss: 0.978520, Train Accuracy: 0.655625\n",
            "Epoch 740 Test Loss: 1.065453, Test Accuracy: 0.617712, time: 7.5s\n",
            "  Epoch 741 @ step 1159000: Train Loss: 0.976977, Train Accuracy: 0.658000\n",
            "Epoch 741 Test Loss: 0.964541, Test Accuracy: 0.661741, time: 7.5s\n",
            "  Epoch 742 @ step 1160000: Train Loss: 0.984637, Train Accuracy: 0.655094\n",
            "  Epoch 742 @ step 1161000: Train Loss: 0.981894, Train Accuracy: 0.656875\n",
            "Epoch 742 Test Loss: 1.171513, Test Accuracy: 0.599740, time: 7.6s\n",
            "  Epoch 743 @ step 1162000: Train Loss: 0.972083, Train Accuracy: 0.658594\n",
            "Epoch 743 Test Loss: 1.188133, Test Accuracy: 0.573682, time: 8.3s\n",
            "  Epoch 744 @ step 1163000: Train Loss: 0.983776, Train Accuracy: 0.653125\n",
            "  Epoch 744 @ step 1164000: Train Loss: 0.968004, Train Accuracy: 0.659594\n",
            "Epoch 744 Test Loss: 0.977938, Test Accuracy: 0.657149, time: 7.5s\n",
            "  Epoch 745 @ step 1165000: Train Loss: 0.978820, Train Accuracy: 0.658500\n",
            "Epoch 745 Test Loss: 0.976931, Test Accuracy: 0.659545, time: 7.6s\n",
            "  Epoch 746 @ step 1166000: Train Loss: 0.987841, Train Accuracy: 0.655813\n",
            "  Epoch 746 @ step 1167000: Train Loss: 0.975288, Train Accuracy: 0.659375\n",
            "Epoch 746 Test Loss: 1.017395, Test Accuracy: 0.645168, time: 7.6s\n",
            "  Epoch 747 @ step 1168000: Train Loss: 0.980245, Train Accuracy: 0.655219\n",
            "  Epoch 747 @ step 1169000: Train Loss: 0.982326, Train Accuracy: 0.655031\n",
            "Epoch 747 Test Loss: 1.084146, Test Accuracy: 0.614018, time: 7.6s\n",
            "  Epoch 748 @ step 1170000: Train Loss: 0.970516, Train Accuracy: 0.660094\n",
            "Epoch 748 Test Loss: 1.013050, Test Accuracy: 0.648962, time: 8.0s\n",
            "  Epoch 749 @ step 1171000: Train Loss: 0.981564, Train Accuracy: 0.654844\n",
            "  Epoch 749 @ step 1172000: Train Loss: 0.980079, Train Accuracy: 0.657188\n",
            "Epoch 749 Test Loss: 1.004835, Test Accuracy: 0.646865, time: 8.0s\n",
            "  Epoch 750 @ step 1173000: Train Loss: 0.983745, Train Accuracy: 0.655719\n",
            "Epoch 750 Test Loss: 1.040589, Test Accuracy: 0.630891, time: 7.5s\n",
            "  Epoch 751 @ step 1174000: Train Loss: 0.975502, Train Accuracy: 0.654844\n",
            "  Epoch 751 @ step 1175000: Train Loss: 0.974833, Train Accuracy: 0.659438\n",
            "Epoch 751 Test Loss: 0.991012, Test Accuracy: 0.649161, time: 7.5s\n",
            "  Epoch 752 @ step 1176000: Train Loss: 0.974146, Train Accuracy: 0.657906\n",
            "Epoch 752 Test Loss: 1.045594, Test Accuracy: 0.636482, time: 7.5s\n",
            "  Epoch 753 @ step 1177000: Train Loss: 0.985750, Train Accuracy: 0.655125\n",
            "  Epoch 753 @ step 1178000: Train Loss: 0.977345, Train Accuracy: 0.657906\n",
            "Epoch 753 Test Loss: 0.986072, Test Accuracy: 0.649161, time: 7.6s\n",
            "  Epoch 754 @ step 1179000: Train Loss: 0.973765, Train Accuracy: 0.658000\n",
            "  Epoch 754 @ step 1180000: Train Loss: 0.981200, Train Accuracy: 0.655781\n",
            "Epoch 754 Test Loss: 1.017500, Test Accuracy: 0.639976, time: 8.3s\n",
            "  Epoch 755 @ step 1181000: Train Loss: 0.968943, Train Accuracy: 0.661563\n",
            "Epoch 755 Test Loss: 1.023141, Test Accuracy: 0.639177, time: 7.9s\n",
            "  Epoch 756 @ step 1182000: Train Loss: 0.980002, Train Accuracy: 0.655188\n",
            "  Epoch 756 @ step 1183000: Train Loss: 0.981073, Train Accuracy: 0.657188\n",
            "Epoch 756 Test Loss: 0.990523, Test Accuracy: 0.656550, time: 7.7s\n",
            "  Epoch 757 @ step 1184000: Train Loss: 0.976102, Train Accuracy: 0.657906\n",
            "Epoch 757 Test Loss: 1.005476, Test Accuracy: 0.643470, time: 7.6s\n",
            "  Epoch 758 @ step 1185000: Train Loss: 0.984838, Train Accuracy: 0.655313\n",
            "  Epoch 758 @ step 1186000: Train Loss: 0.973295, Train Accuracy: 0.656875\n",
            "Epoch 758 Test Loss: 1.015683, Test Accuracy: 0.647165, time: 8.1s\n",
            "  Epoch 759 @ step 1187000: Train Loss: 0.983829, Train Accuracy: 0.655031\n",
            "Epoch 759 Test Loss: 0.990333, Test Accuracy: 0.659445, time: 7.6s\n",
            "  Epoch 760 @ step 1188000: Train Loss: 0.979688, Train Accuracy: 0.657281\n",
            "  Epoch 760 @ step 1189000: Train Loss: 0.975289, Train Accuracy: 0.658563\n",
            "Epoch 760 Test Loss: 0.999979, Test Accuracy: 0.651058, time: 8.8s\n",
            "  Epoch 761 @ step 1190000: Train Loss: 0.978317, Train Accuracy: 0.658469\n",
            "  Epoch 761 @ step 1191000: Train Loss: 0.982997, Train Accuracy: 0.656875\n",
            "Epoch 761 Test Loss: 1.013563, Test Accuracy: 0.637380, time: 7.7s\n",
            "  Epoch 762 @ step 1192000: Train Loss: 0.975893, Train Accuracy: 0.656844\n",
            "Epoch 762 Test Loss: 1.072341, Test Accuracy: 0.621506, time: 7.6s\n",
            "  Epoch 763 @ step 1193000: Train Loss: 0.974409, Train Accuracy: 0.657438\n",
            "  Epoch 763 @ step 1194000: Train Loss: 0.979022, Train Accuracy: 0.658250\n",
            "Epoch 763 Test Loss: 1.058476, Test Accuracy: 0.630092, time: 7.7s\n",
            "  Epoch 764 @ step 1195000: Train Loss: 0.979368, Train Accuracy: 0.656938\n",
            "Epoch 764 Test Loss: 1.040263, Test Accuracy: 0.637081, time: 7.7s\n",
            "  Epoch 765 @ step 1196000: Train Loss: 0.980467, Train Accuracy: 0.656281\n",
            "  Epoch 765 @ step 1197000: Train Loss: 0.978422, Train Accuracy: 0.658094\n",
            "Epoch 765 Test Loss: 1.076569, Test Accuracy: 0.618211, time: 7.8s\n",
            "  Epoch 766 @ step 1198000: Train Loss: 0.981945, Train Accuracy: 0.656656\n",
            "Epoch 766 Test Loss: 1.067659, Test Accuracy: 0.626298, time: 8.4s\n",
            "  Epoch 767 @ step 1199000: Train Loss: 0.978308, Train Accuracy: 0.657688\n",
            "  Epoch 767 @ step 1200000: Train Loss: 0.977982, Train Accuracy: 0.661063\n",
            "Epoch 767 Test Loss: 0.988973, Test Accuracy: 0.648462, time: 7.7s\n",
            "  Epoch 768 @ step 1201000: Train Loss: 0.979844, Train Accuracy: 0.658344\n",
            "Epoch 768 Test Loss: 1.004828, Test Accuracy: 0.647364, time: 7.7s\n",
            "  Epoch 769 @ step 1202000: Train Loss: 0.982419, Train Accuracy: 0.655813\n",
            "  Epoch 769 @ step 1203000: Train Loss: 0.974823, Train Accuracy: 0.656313\n",
            "Epoch 769 Test Loss: 0.992610, Test Accuracy: 0.650859, time: 7.7s\n",
            "  Epoch 770 @ step 1204000: Train Loss: 0.980991, Train Accuracy: 0.656656\n",
            "  Epoch 770 @ step 1205000: Train Loss: 0.978823, Train Accuracy: 0.658469\n",
            "Epoch 770 Test Loss: 1.002258, Test Accuracy: 0.647364, time: 7.7s\n",
            "  Epoch 771 @ step 1206000: Train Loss: 0.971045, Train Accuracy: 0.659156\n",
            "Epoch 771 Test Loss: 1.137619, Test Accuracy: 0.606729, time: 8.1s\n",
            "  Epoch 772 @ step 1207000: Train Loss: 0.982239, Train Accuracy: 0.657156\n",
            "  Epoch 772 @ step 1208000: Train Loss: 0.981240, Train Accuracy: 0.657563\n",
            "Epoch 772 Test Loss: 1.049631, Test Accuracy: 0.628794, time: 8.0s\n",
            "  Epoch 773 @ step 1209000: Train Loss: 0.970295, Train Accuracy: 0.660063\n",
            "Epoch 773 Test Loss: 0.982677, Test Accuracy: 0.655451, time: 7.6s\n",
            "  Epoch 774 @ step 1210000: Train Loss: 0.986990, Train Accuracy: 0.653250\n",
            "  Epoch 774 @ step 1211000: Train Loss: 0.978540, Train Accuracy: 0.658813\n",
            "Epoch 774 Test Loss: 1.058804, Test Accuracy: 0.625100, time: 7.7s\n",
            "  Epoch 775 @ step 1212000: Train Loss: 0.979029, Train Accuracy: 0.655656\n",
            "Epoch 775 Test Loss: 1.063387, Test Accuracy: 0.628395, time: 7.7s\n",
            "  Epoch 776 @ step 1213000: Train Loss: 0.979248, Train Accuracy: 0.659531\n",
            "  Epoch 776 @ step 1214000: Train Loss: 0.977131, Train Accuracy: 0.656188\n",
            "Epoch 776 Test Loss: 1.017753, Test Accuracy: 0.639577, time: 7.7s\n",
            "  Epoch 777 @ step 1215000: Train Loss: 0.977483, Train Accuracy: 0.655594\n",
            "  Epoch 777 @ step 1216000: Train Loss: 0.978784, Train Accuracy: 0.659750\n",
            "Epoch 777 Test Loss: 1.027642, Test Accuracy: 0.630491, time: 8.6s\n",
            "  Epoch 778 @ step 1217000: Train Loss: 0.973826, Train Accuracy: 0.658844\n",
            "Epoch 778 Test Loss: 0.993730, Test Accuracy: 0.655252, time: 7.7s\n",
            "  Epoch 779 @ step 1218000: Train Loss: 0.979409, Train Accuracy: 0.656344\n",
            "  Epoch 779 @ step 1219000: Train Loss: 0.985635, Train Accuracy: 0.654031\n",
            "Epoch 779 Test Loss: 1.087728, Test Accuracy: 0.613518, time: 7.7s\n",
            "  Epoch 780 @ step 1220000: Train Loss: 0.970382, Train Accuracy: 0.662594\n",
            "Epoch 780 Test Loss: 0.991743, Test Accuracy: 0.657847, time: 8.2s\n",
            "  Epoch 781 @ step 1221000: Train Loss: 0.982403, Train Accuracy: 0.657688\n",
            "  Epoch 781 @ step 1222000: Train Loss: 0.983286, Train Accuracy: 0.656688\n",
            "Epoch 781 Test Loss: 1.265861, Test Accuracy: 0.545427, time: 7.7s\n",
            "  Epoch 782 @ step 1223000: Train Loss: 0.976832, Train Accuracy: 0.658906\n",
            "Epoch 782 Test Loss: 1.007242, Test Accuracy: 0.651957, time: 8.4s\n",
            "  Epoch 783 @ step 1224000: Train Loss: 0.978479, Train Accuracy: 0.659500\n",
            "  Epoch 783 @ step 1225000: Train Loss: 0.979781, Train Accuracy: 0.656563\n",
            "Epoch 783 Test Loss: 0.984047, Test Accuracy: 0.660244, time: 8.6s\n",
            "  Epoch 784 @ step 1226000: Train Loss: 0.968047, Train Accuracy: 0.659656\n",
            "Epoch 784 Test Loss: 1.030393, Test Accuracy: 0.631290, time: 7.7s\n",
            "  Epoch 785 @ step 1227000: Train Loss: 0.985255, Train Accuracy: 0.654500\n",
            "  Epoch 785 @ step 1228000: Train Loss: 0.978505, Train Accuracy: 0.656594\n",
            "Epoch 785 Test Loss: 1.031299, Test Accuracy: 0.635184, time: 7.6s\n",
            "  Epoch 786 @ step 1229000: Train Loss: 0.975528, Train Accuracy: 0.659250\n",
            "  Epoch 786 @ step 1230000: Train Loss: 0.983514, Train Accuracy: 0.655688\n",
            "Epoch 786 Test Loss: 1.125390, Test Accuracy: 0.610024, time: 7.6s\n",
            "  Epoch 787 @ step 1231000: Train Loss: 0.971951, Train Accuracy: 0.662281\n",
            "Epoch 787 Test Loss: 1.065899, Test Accuracy: 0.625200, time: 7.6s\n",
            "  Epoch 788 @ step 1232000: Train Loss: 0.974221, Train Accuracy: 0.655563\n",
            "  Epoch 788 @ step 1233000: Train Loss: 0.982136, Train Accuracy: 0.655125\n",
            "Epoch 788 Test Loss: 1.040697, Test Accuracy: 0.638379, time: 7.9s\n",
            "  Epoch 789 @ step 1234000: Train Loss: 0.968363, Train Accuracy: 0.661281\n",
            "Epoch 789 Test Loss: 1.032930, Test Accuracy: 0.630092, time: 8.2s\n",
            "  Epoch 790 @ step 1235000: Train Loss: 0.983708, Train Accuracy: 0.656094\n",
            "  Epoch 790 @ step 1236000: Train Loss: 0.979380, Train Accuracy: 0.657375\n",
            "Epoch 790 Test Loss: 1.005902, Test Accuracy: 0.650759, time: 7.6s\n",
            "  Epoch 791 @ step 1237000: Train Loss: 0.974670, Train Accuracy: 0.658750\n",
            "Epoch 791 Test Loss: 1.037307, Test Accuracy: 0.633287, time: 7.5s\n",
            "  Epoch 792 @ step 1238000: Train Loss: 0.980750, Train Accuracy: 0.654375\n",
            "  Epoch 792 @ step 1239000: Train Loss: 0.974567, Train Accuracy: 0.660594\n",
            "Epoch 792 Test Loss: 1.111532, Test Accuracy: 0.603534, time: 7.5s\n",
            "  Epoch 793 @ step 1240000: Train Loss: 0.968705, Train Accuracy: 0.660625\n",
            "  Epoch 793 @ step 1241000: Train Loss: 0.986287, Train Accuracy: 0.652313\n",
            "Epoch 793 Test Loss: 0.971626, Test Accuracy: 0.657149, time: 7.6s\n",
            "  Epoch 794 @ step 1242000: Train Loss: 0.976253, Train Accuracy: 0.660531\n",
            "Epoch 794 Test Loss: 1.036233, Test Accuracy: 0.640675, time: 8.3s\n",
            "  Epoch 795 @ step 1243000: Train Loss: 0.972843, Train Accuracy: 0.656625\n",
            "  Epoch 795 @ step 1244000: Train Loss: 0.985591, Train Accuracy: 0.655844\n",
            "Epoch 795 Test Loss: 1.014594, Test Accuracy: 0.646166, time: 7.9s\n",
            "  Epoch 796 @ step 1245000: Train Loss: 0.968256, Train Accuracy: 0.660125\n",
            "Epoch 796 Test Loss: 1.056237, Test Accuracy: 0.625998, time: 7.6s\n",
            "  Epoch 797 @ step 1246000: Train Loss: 0.984582, Train Accuracy: 0.656750\n",
            "  Epoch 797 @ step 1247000: Train Loss: 0.980547, Train Accuracy: 0.657281\n",
            "Epoch 797 Test Loss: 0.970176, Test Accuracy: 0.665835, time: 7.6s\n",
            "  Epoch 798 @ step 1248000: Train Loss: 0.977843, Train Accuracy: 0.655375\n",
            "Epoch 798 Test Loss: 1.050813, Test Accuracy: 0.629692, time: 7.6s\n",
            "  Epoch 799 @ step 1249000: Train Loss: 0.976275, Train Accuracy: 0.657813\n",
            "  Epoch 799 @ step 1250000: Train Loss: 0.977285, Train Accuracy: 0.657938\n",
            "Epoch 799 Test Loss: 1.080803, Test Accuracy: 0.615715, time: 7.5s\n",
            "  Epoch 800 @ step 1251000: Train Loss: 0.977808, Train Accuracy: 0.656594\n",
            "Epoch 800 Test Loss: 1.000832, Test Accuracy: 0.651258, time: 8.4s\n",
            "  Epoch 801 @ step 1252000: Train Loss: 0.988642, Train Accuracy: 0.653219\n",
            "  Epoch 801 @ step 1253000: Train Loss: 0.973633, Train Accuracy: 0.660344\n",
            "Epoch 801 Test Loss: 1.019337, Test Accuracy: 0.643071, time: 7.6s\n",
            "  Epoch 802 @ step 1254000: Train Loss: 0.981442, Train Accuracy: 0.655563\n",
            "  Epoch 802 @ step 1255000: Train Loss: 0.978774, Train Accuracy: 0.657313\n",
            "Epoch 802 Test Loss: 1.009580, Test Accuracy: 0.645068, time: 7.8s\n",
            "  Epoch 803 @ step 1256000: Train Loss: 0.980864, Train Accuracy: 0.655969\n",
            "Epoch 803 Test Loss: 1.045206, Test Accuracy: 0.626997, time: 7.5s\n",
            "  Epoch 804 @ step 1257000: Train Loss: 0.971992, Train Accuracy: 0.657344\n",
            "  Epoch 804 @ step 1258000: Train Loss: 0.984353, Train Accuracy: 0.654375\n",
            "Epoch 804 Test Loss: 0.977189, Test Accuracy: 0.659545, time: 7.9s\n",
            "  Epoch 805 @ step 1259000: Train Loss: 0.966248, Train Accuracy: 0.660844\n",
            "Epoch 805 Test Loss: 1.108381, Test Accuracy: 0.605931, time: 8.3s\n",
            "  Epoch 806 @ step 1260000: Train Loss: 0.987981, Train Accuracy: 0.652938\n",
            "  Epoch 806 @ step 1261000: Train Loss: 0.978627, Train Accuracy: 0.656219\n",
            "Epoch 806 Test Loss: 1.017048, Test Accuracy: 0.646565, time: 8.6s\n",
            "  Epoch 807 @ step 1262000: Train Loss: 0.978861, Train Accuracy: 0.659813\n",
            "Epoch 807 Test Loss: 1.043219, Test Accuracy: 0.629093, time: 7.6s\n",
            "  Epoch 808 @ step 1263000: Train Loss: 0.982362, Train Accuracy: 0.655094\n",
            "  Epoch 808 @ step 1264000: Train Loss: 0.970657, Train Accuracy: 0.659406\n",
            "Epoch 808 Test Loss: 1.015496, Test Accuracy: 0.642173, time: 7.6s\n",
            "  Epoch 809 @ step 1265000: Train Loss: 0.984347, Train Accuracy: 0.654250\n",
            "  Epoch 809 @ step 1266000: Train Loss: 0.986569, Train Accuracy: 0.654844\n",
            "Epoch 809 Test Loss: 0.996446, Test Accuracy: 0.651857, time: 7.6s\n",
            "  Epoch 810 @ step 1267000: Train Loss: 0.978100, Train Accuracy: 0.658688\n",
            "Epoch 810 Test Loss: 0.980456, Test Accuracy: 0.656550, time: 7.6s\n",
            "  Epoch 811 @ step 1268000: Train Loss: 0.973115, Train Accuracy: 0.660906\n",
            "  Epoch 811 @ step 1269000: Train Loss: 0.984327, Train Accuracy: 0.655188\n",
            "Epoch 811 Test Loss: 1.031951, Test Accuracy: 0.641174, time: 7.8s\n",
            "  Epoch 812 @ step 1270000: Train Loss: 0.978852, Train Accuracy: 0.655250\n",
            "Epoch 812 Test Loss: 1.065780, Test Accuracy: 0.626298, time: 8.3s\n",
            "  Epoch 813 @ step 1271000: Train Loss: 0.979296, Train Accuracy: 0.658906\n",
            "  Epoch 813 @ step 1272000: Train Loss: 0.980495, Train Accuracy: 0.656531\n",
            "Epoch 813 Test Loss: 1.112711, Test Accuracy: 0.603534, time: 7.5s\n",
            "  Epoch 814 @ step 1273000: Train Loss: 0.976297, Train Accuracy: 0.656125\n",
            "Epoch 814 Test Loss: 1.081988, Test Accuracy: 0.605032, time: 7.5s\n",
            "  Epoch 815 @ step 1274000: Train Loss: 0.983387, Train Accuracy: 0.658094\n",
            "  Epoch 815 @ step 1275000: Train Loss: 0.975153, Train Accuracy: 0.660906\n",
            "Epoch 815 Test Loss: 1.003670, Test Accuracy: 0.646865, time: 7.6s\n",
            "  Epoch 816 @ step 1276000: Train Loss: 0.974777, Train Accuracy: 0.655969\n",
            "Epoch 816 Test Loss: 0.970706, Test Accuracy: 0.666234, time: 7.6s\n",
            "  Epoch 817 @ step 1277000: Train Loss: 0.984504, Train Accuracy: 0.654313\n",
            "  Epoch 817 @ step 1278000: Train Loss: 0.973996, Train Accuracy: 0.659719\n",
            "Epoch 817 Test Loss: 1.039719, Test Accuracy: 0.626298, time: 8.1s\n",
            "  Epoch 818 @ step 1279000: Train Loss: 0.979552, Train Accuracy: 0.655938\n",
            "  Epoch 818 @ step 1280000: Train Loss: 0.982934, Train Accuracy: 0.653875\n",
            "Epoch 818 Test Loss: 0.984924, Test Accuracy: 0.655451, time: 8.0s\n",
            "  Epoch 819 @ step 1281000: Train Loss: 0.978397, Train Accuracy: 0.658094\n",
            "Epoch 819 Test Loss: 1.031295, Test Accuracy: 0.622005, time: 7.5s\n",
            "  Epoch 820 @ step 1282000: Train Loss: 0.981597, Train Accuracy: 0.658438\n",
            "  Epoch 820 @ step 1283000: Train Loss: 0.975789, Train Accuracy: 0.657125\n",
            "Epoch 820 Test Loss: 0.974531, Test Accuracy: 0.663039, time: 7.6s\n",
            "  Epoch 821 @ step 1284000: Train Loss: 0.970842, Train Accuracy: 0.661219\n",
            "Epoch 821 Test Loss: 1.037688, Test Accuracy: 0.623702, time: 7.6s\n",
            "  Epoch 822 @ step 1285000: Train Loss: 0.989428, Train Accuracy: 0.649813\n",
            "  Epoch 822 @ step 1286000: Train Loss: 0.984340, Train Accuracy: 0.655844\n",
            "Epoch 822 Test Loss: 1.046432, Test Accuracy: 0.629593, time: 7.5s\n",
            "  Epoch 823 @ step 1287000: Train Loss: 0.963047, Train Accuracy: 0.662031\n",
            "Epoch 823 Test Loss: 1.135342, Test Accuracy: 0.597644, time: 8.4s\n",
            "  Epoch 824 @ step 1288000: Train Loss: 0.985499, Train Accuracy: 0.656063\n",
            "  Epoch 824 @ step 1289000: Train Loss: 0.983717, Train Accuracy: 0.656313\n",
            "Epoch 824 Test Loss: 0.971026, Test Accuracy: 0.666334, time: 7.7s\n",
            "  Epoch 825 @ step 1290000: Train Loss: 0.967228, Train Accuracy: 0.660969\n",
            "  Epoch 825 @ step 1291000: Train Loss: 0.983073, Train Accuracy: 0.655313\n",
            "Epoch 825 Test Loss: 1.014451, Test Accuracy: 0.644469, time: 7.6s\n",
            "  Epoch 826 @ step 1292000: Train Loss: 0.971665, Train Accuracy: 0.659000\n",
            "Epoch 826 Test Loss: 1.047877, Test Accuracy: 0.623702, time: 7.6s\n",
            "  Epoch 827 @ step 1293000: Train Loss: 0.977131, Train Accuracy: 0.658813\n",
            "  Epoch 827 @ step 1294000: Train Loss: 0.988823, Train Accuracy: 0.655469\n",
            "Epoch 827 Test Loss: 1.012534, Test Accuracy: 0.644269, time: 8.7s\n",
            "  Epoch 828 @ step 1295000: Train Loss: 0.974917, Train Accuracy: 0.659719\n",
            "Epoch 828 Test Loss: 1.051345, Test Accuracy: 0.629692, time: 7.5s\n",
            "  Epoch 829 @ step 1296000: Train Loss: 0.982733, Train Accuracy: 0.657906\n",
            "  Epoch 829 @ step 1297000: Train Loss: 0.981559, Train Accuracy: 0.656313\n",
            "Epoch 829 Test Loss: 1.049383, Test Accuracy: 0.629093, time: 8.6s\n",
            "  Epoch 830 @ step 1298000: Train Loss: 0.976681, Train Accuracy: 0.657500\n",
            "Epoch 830 Test Loss: 0.959234, Test Accuracy: 0.671126, time: 7.6s\n",
            "  Epoch 831 @ step 1299000: Train Loss: 0.981062, Train Accuracy: 0.656344\n",
            "  Epoch 831 @ step 1300000: Train Loss: 0.982485, Train Accuracy: 0.657844\n",
            "Epoch 831 Test Loss: 1.011680, Test Accuracy: 0.637181, time: 7.5s\n",
            "  Epoch 832 @ step 1301000: Train Loss: 0.973356, Train Accuracy: 0.658375\n",
            "Epoch 832 Test Loss: 1.001027, Test Accuracy: 0.647863, time: 7.6s\n",
            "  Epoch 833 @ step 1302000: Train Loss: 0.987984, Train Accuracy: 0.653281\n",
            "  Epoch 833 @ step 1303000: Train Loss: 0.973249, Train Accuracy: 0.660625\n",
            "Epoch 833 Test Loss: 0.991638, Test Accuracy: 0.654752, time: 7.5s\n",
            "  Epoch 834 @ step 1304000: Train Loss: 0.981754, Train Accuracy: 0.656344\n",
            "  Epoch 834 @ step 1305000: Train Loss: 0.978657, Train Accuracy: 0.657969\n",
            "Epoch 834 Test Loss: 1.055287, Test Accuracy: 0.626298, time: 7.5s\n",
            "  Epoch 835 @ step 1306000: Train Loss: 0.968382, Train Accuracy: 0.662656\n",
            "Epoch 835 Test Loss: 1.152033, Test Accuracy: 0.595248, time: 8.4s\n",
            "  Epoch 836 @ step 1307000: Train Loss: 0.987599, Train Accuracy: 0.654656\n",
            "  Epoch 836 @ step 1308000: Train Loss: 0.975765, Train Accuracy: 0.656688\n",
            "Epoch 836 Test Loss: 1.067770, Test Accuracy: 0.620008, time: 7.6s\n",
            "  Epoch 837 @ step 1309000: Train Loss: 0.980646, Train Accuracy: 0.656281\n",
            "Epoch 837 Test Loss: 1.012738, Test Accuracy: 0.641174, time: 7.4s\n",
            "  Epoch 838 @ step 1310000: Train Loss: 0.982802, Train Accuracy: 0.654813\n",
            "  Epoch 838 @ step 1311000: Train Loss: 0.978710, Train Accuracy: 0.657719\n",
            "Epoch 838 Test Loss: 1.039283, Test Accuracy: 0.633187, time: 7.5s\n",
            "  Epoch 839 @ step 1312000: Train Loss: 0.975206, Train Accuracy: 0.658000\n",
            "Epoch 839 Test Loss: 1.050004, Test Accuracy: 0.630491, time: 7.4s\n",
            "  Epoch 840 @ step 1313000: Train Loss: 0.979802, Train Accuracy: 0.654844\n",
            "  Epoch 840 @ step 1314000: Train Loss: 0.978032, Train Accuracy: 0.656844\n",
            "Epoch 840 Test Loss: 0.975967, Test Accuracy: 0.656350, time: 7.7s\n",
            "  Epoch 841 @ step 1315000: Train Loss: 0.972520, Train Accuracy: 0.658156\n",
            "  Epoch 841 @ step 1316000: Train Loss: 0.984677, Train Accuracy: 0.653875\n",
            "Epoch 841 Test Loss: 1.056370, Test Accuracy: 0.632688, time: 8.1s\n",
            "  Epoch 842 @ step 1317000: Train Loss: 0.970209, Train Accuracy: 0.659781\n",
            "Epoch 842 Test Loss: 1.017187, Test Accuracy: 0.649261, time: 7.5s\n",
            "  Epoch 843 @ step 1318000: Train Loss: 0.985736, Train Accuracy: 0.654250\n",
            "  Epoch 843 @ step 1319000: Train Loss: 0.983288, Train Accuracy: 0.655969\n",
            "Epoch 843 Test Loss: 1.067277, Test Accuracy: 0.617412, time: 7.4s\n",
            "  Epoch 844 @ step 1320000: Train Loss: 0.977942, Train Accuracy: 0.659438\n",
            "Epoch 844 Test Loss: 1.080955, Test Accuracy: 0.610723, time: 7.3s\n",
            "  Epoch 845 @ step 1321000: Train Loss: 0.974280, Train Accuracy: 0.657063\n",
            "  Epoch 845 @ step 1322000: Train Loss: 0.979425, Train Accuracy: 0.655406\n",
            "Epoch 845 Test Loss: 1.027035, Test Accuracy: 0.639477, time: 7.6s\n",
            "  Epoch 846 @ step 1323000: Train Loss: 0.979182, Train Accuracy: 0.656938\n",
            "Epoch 846 Test Loss: 1.007256, Test Accuracy: 0.653654, time: 8.3s\n",
            "  Epoch 847 @ step 1324000: Train Loss: 0.977614, Train Accuracy: 0.657156\n",
            "  Epoch 847 @ step 1325000: Train Loss: 0.981062, Train Accuracy: 0.657594\n",
            "Epoch 847 Test Loss: 0.971315, Test Accuracy: 0.662540, time: 7.9s\n",
            "  Epoch 848 @ step 1326000: Train Loss: 0.974965, Train Accuracy: 0.657750\n",
            "Epoch 848 Test Loss: 1.071131, Test Accuracy: 0.627296, time: 7.4s\n",
            "  Epoch 849 @ step 1327000: Train Loss: 0.982754, Train Accuracy: 0.652750\n",
            "  Epoch 849 @ step 1328000: Train Loss: 0.971798, Train Accuracy: 0.661281\n",
            "Epoch 849 Test Loss: 1.047336, Test Accuracy: 0.622903, time: 8.3s\n",
            "  Epoch 850 @ step 1329000: Train Loss: 0.975906, Train Accuracy: 0.656344\n",
            "  Epoch 850 @ step 1330000: Train Loss: 0.985414, Train Accuracy: 0.657094\n",
            "Epoch 850 Test Loss: 1.037590, Test Accuracy: 0.633986, time: 7.6s\n",
            "  Epoch 851 @ step 1331000: Train Loss: 0.978289, Train Accuracy: 0.656469\n",
            "Epoch 851 Test Loss: 1.032507, Test Accuracy: 0.635883, time: 7.4s\n",
            "  Epoch 852 @ step 1332000: Train Loss: 0.975658, Train Accuracy: 0.656688\n",
            "  Epoch 852 @ step 1333000: Train Loss: 0.984076, Train Accuracy: 0.654938\n",
            "Epoch 852 Test Loss: 0.974777, Test Accuracy: 0.664337, time: 8.4s\n",
            "  Epoch 853 @ step 1334000: Train Loss: 0.967573, Train Accuracy: 0.661000\n",
            "Epoch 853 Test Loss: 0.973487, Test Accuracy: 0.663538, time: 7.5s\n",
            "  Epoch 854 @ step 1335000: Train Loss: 0.984417, Train Accuracy: 0.655563\n",
            "  Epoch 854 @ step 1336000: Train Loss: 0.980653, Train Accuracy: 0.656625\n",
            "Epoch 854 Test Loss: 0.994532, Test Accuracy: 0.646565, time: 7.4s\n",
            "  Epoch 855 @ step 1337000: Train Loss: 0.980132, Train Accuracy: 0.656344\n",
            "Epoch 855 Test Loss: 1.009970, Test Accuracy: 0.649960, time: 7.4s\n",
            "  Epoch 856 @ step 1338000: Train Loss: 0.977556, Train Accuracy: 0.657781\n",
            "  Epoch 856 @ step 1339000: Train Loss: 0.975985, Train Accuracy: 0.657938\n",
            "Epoch 856 Test Loss: 1.006999, Test Accuracy: 0.635982, time: 7.4s\n",
            "  Epoch 857 @ step 1340000: Train Loss: 0.973025, Train Accuracy: 0.661031\n",
            "  Epoch 857 @ step 1341000: Train Loss: 0.989677, Train Accuracy: 0.649250\n",
            "Epoch 857 Test Loss: 0.988201, Test Accuracy: 0.652556, time: 7.4s\n",
            "  Epoch 858 @ step 1342000: Train Loss: 0.976667, Train Accuracy: 0.660563\n",
            "Epoch 858 Test Loss: 0.982349, Test Accuracy: 0.659545, time: 8.5s\n",
            "  Epoch 859 @ step 1343000: Train Loss: 0.982602, Train Accuracy: 0.654969\n",
            "  Epoch 859 @ step 1344000: Train Loss: 0.981257, Train Accuracy: 0.657781\n",
            "Epoch 859 Test Loss: 1.081072, Test Accuracy: 0.616014, time: 7.3s\n",
            "  Epoch 860 @ step 1345000: Train Loss: 0.967292, Train Accuracy: 0.659906\n",
            "Epoch 860 Test Loss: 0.987898, Test Accuracy: 0.652356, time: 7.4s\n",
            "  Epoch 861 @ step 1346000: Train Loss: 0.982672, Train Accuracy: 0.656313\n",
            "  Epoch 861 @ step 1347000: Train Loss: 0.973949, Train Accuracy: 0.660750\n",
            "Epoch 861 Test Loss: 0.977158, Test Accuracy: 0.655751, time: 7.4s\n",
            "  Epoch 862 @ step 1348000: Train Loss: 0.977018, Train Accuracy: 0.660188\n",
            "Epoch 862 Test Loss: 1.064087, Test Accuracy: 0.622903, time: 7.3s\n",
            "  Epoch 863 @ step 1349000: Train Loss: 0.985836, Train Accuracy: 0.656281\n",
            "  Epoch 863 @ step 1350000: Train Loss: 0.978570, Train Accuracy: 0.659219\n",
            "Epoch 863 Test Loss: 0.998379, Test Accuracy: 0.649661, time: 7.4s\n",
            "  Epoch 864 @ step 1351000: Train Loss: 0.979134, Train Accuracy: 0.656750\n",
            "Epoch 864 Test Loss: 1.066799, Test Accuracy: 0.623303, time: 8.5s\n",
            "  Epoch 865 @ step 1352000: Train Loss: 0.981474, Train Accuracy: 0.660313\n",
            "  Epoch 865 @ step 1353000: Train Loss: 0.973166, Train Accuracy: 0.659844\n",
            "Epoch 865 Test Loss: 1.052376, Test Accuracy: 0.626398, time: 7.4s\n",
            "  Epoch 866 @ step 1354000: Train Loss: 0.978587, Train Accuracy: 0.657781\n",
            "  Epoch 866 @ step 1355000: Train Loss: 0.981704, Train Accuracy: 0.656531\n",
            "Epoch 866 Test Loss: 1.029896, Test Accuracy: 0.637280, time: 7.4s\n",
            "  Epoch 867 @ step 1356000: Train Loss: 0.974947, Train Accuracy: 0.657875\n",
            "Epoch 867 Test Loss: 1.008589, Test Accuracy: 0.645068, time: 7.4s\n",
            "  Epoch 868 @ step 1357000: Train Loss: 0.981512, Train Accuracy: 0.656688\n",
            "  Epoch 868 @ step 1358000: Train Loss: 0.980519, Train Accuracy: 0.653688\n",
            "Epoch 868 Test Loss: 1.011911, Test Accuracy: 0.636681, time: 8.0s\n",
            "  Epoch 869 @ step 1359000: Train Loss: 0.978759, Train Accuracy: 0.660156\n",
            "Epoch 869 Test Loss: 1.001889, Test Accuracy: 0.639577, time: 7.4s\n",
            "  Epoch 870 @ step 1360000: Train Loss: 0.974223, Train Accuracy: 0.655469\n",
            "  Epoch 870 @ step 1361000: Train Loss: 0.975476, Train Accuracy: 0.659000\n",
            "Epoch 870 Test Loss: 0.973896, Test Accuracy: 0.662839, time: 8.5s\n",
            "  Epoch 871 @ step 1362000: Train Loss: 0.973421, Train Accuracy: 0.659500\n",
            "Epoch 871 Test Loss: 0.993900, Test Accuracy: 0.652955, time: 7.4s\n",
            "  Epoch 872 @ step 1363000: Train Loss: 0.986746, Train Accuracy: 0.655313\n",
            "  Epoch 872 @ step 1364000: Train Loss: 0.972232, Train Accuracy: 0.660156\n",
            "Epoch 872 Test Loss: 1.007921, Test Accuracy: 0.641573, time: 8.6s\n",
            "  Epoch 873 @ step 1365000: Train Loss: 0.977787, Train Accuracy: 0.659906\n",
            "  Epoch 873 @ step 1366000: Train Loss: 0.984614, Train Accuracy: 0.654000\n",
            "Epoch 873 Test Loss: 1.070652, Test Accuracy: 0.623003, time: 7.5s\n",
            "  Epoch 874 @ step 1367000: Train Loss: 0.974318, Train Accuracy: 0.659281\n",
            "Epoch 874 Test Loss: 1.047588, Test Accuracy: 0.628295, time: 7.4s\n",
            "  Epoch 875 @ step 1368000: Train Loss: 0.971888, Train Accuracy: 0.659000\n",
            "  Epoch 875 @ step 1369000: Train Loss: 0.987452, Train Accuracy: 0.655906\n",
            "Epoch 875 Test Loss: 1.023459, Test Accuracy: 0.633686, time: 7.8s\n",
            "  Epoch 876 @ step 1370000: Train Loss: 0.978492, Train Accuracy: 0.658313\n",
            "Epoch 876 Test Loss: 0.991174, Test Accuracy: 0.650958, time: 8.1s\n",
            "  Epoch 877 @ step 1371000: Train Loss: 0.982963, Train Accuracy: 0.654813\n",
            "  Epoch 877 @ step 1372000: Train Loss: 0.972308, Train Accuracy: 0.657469\n",
            "Epoch 877 Test Loss: 1.015852, Test Accuracy: 0.643470, time: 7.5s\n",
            "  Epoch 878 @ step 1373000: Train Loss: 0.981011, Train Accuracy: 0.659031\n",
            "Epoch 878 Test Loss: 1.009793, Test Accuracy: 0.643870, time: 7.5s\n",
            "  Epoch 879 @ step 1374000: Train Loss: 0.983247, Train Accuracy: 0.657125\n",
            "  Epoch 879 @ step 1375000: Train Loss: 0.978320, Train Accuracy: 0.656594\n",
            "Epoch 879 Test Loss: 1.087138, Test Accuracy: 0.618211, time: 7.4s\n",
            "  Epoch 880 @ step 1376000: Train Loss: 0.972785, Train Accuracy: 0.658406\n",
            "  Epoch 880 @ step 1377000: Train Loss: 0.985547, Train Accuracy: 0.655000\n",
            "Epoch 880 Test Loss: 1.106366, Test Accuracy: 0.601737, time: 7.5s\n",
            "  Epoch 881 @ step 1378000: Train Loss: 0.979985, Train Accuracy: 0.656125\n",
            "Epoch 881 Test Loss: 1.049307, Test Accuracy: 0.631490, time: 8.0s\n",
            "  Epoch 882 @ step 1379000: Train Loss: 0.972110, Train Accuracy: 0.658938\n",
            "  Epoch 882 @ step 1380000: Train Loss: 0.988803, Train Accuracy: 0.655688\n",
            "Epoch 882 Test Loss: 1.096501, Test Accuracy: 0.606729, time: 7.9s\n",
            "  Epoch 883 @ step 1381000: Train Loss: 0.976293, Train Accuracy: 0.661250\n",
            "Epoch 883 Test Loss: 1.030801, Test Accuracy: 0.634285, time: 7.5s\n",
            "  Epoch 884 @ step 1382000: Train Loss: 0.972414, Train Accuracy: 0.656813\n",
            "  Epoch 884 @ step 1383000: Train Loss: 0.985838, Train Accuracy: 0.657906\n",
            "Epoch 884 Test Loss: 1.050955, Test Accuracy: 0.626997, time: 7.6s\n",
            "  Epoch 885 @ step 1384000: Train Loss: 0.976508, Train Accuracy: 0.657219\n",
            "Epoch 885 Test Loss: 0.998190, Test Accuracy: 0.650659, time: 7.6s\n",
            "  Epoch 886 @ step 1385000: Train Loss: 0.983054, Train Accuracy: 0.653969\n",
            "  Epoch 886 @ step 1386000: Train Loss: 0.971994, Train Accuracy: 0.659813\n",
            "Epoch 886 Test Loss: 1.092475, Test Accuracy: 0.608926, time: 7.5s\n",
            "  Epoch 887 @ step 1387000: Train Loss: 0.985279, Train Accuracy: 0.653594\n",
            "Epoch 887 Test Loss: 1.011951, Test Accuracy: 0.636482, time: 8.3s\n",
            "  Epoch 888 @ step 1388000: Train Loss: 0.981079, Train Accuracy: 0.657969\n",
            "  Epoch 888 @ step 1389000: Train Loss: 0.973798, Train Accuracy: 0.658750\n",
            "Epoch 888 Test Loss: 1.030438, Test Accuracy: 0.639077, time: 7.6s\n",
            "  Epoch 889 @ step 1390000: Train Loss: 0.975607, Train Accuracy: 0.659688\n",
            "  Epoch 889 @ step 1391000: Train Loss: 0.982328, Train Accuracy: 0.657906\n",
            "Epoch 889 Test Loss: 1.033918, Test Accuracy: 0.634285, time: 7.4s\n",
            "  Epoch 890 @ step 1392000: Train Loss: 0.973261, Train Accuracy: 0.656625\n",
            "Epoch 890 Test Loss: 0.975820, Test Accuracy: 0.659245, time: 8.1s\n",
            "  Epoch 891 @ step 1393000: Train Loss: 0.976842, Train Accuracy: 0.658125\n",
            "  Epoch 891 @ step 1394000: Train Loss: 0.981123, Train Accuracy: 0.654844\n",
            "Epoch 891 Test Loss: 1.053737, Test Accuracy: 0.623003, time: 7.5s\n",
            "  Epoch 892 @ step 1395000: Train Loss: 0.975617, Train Accuracy: 0.657531\n",
            "Epoch 892 Test Loss: 1.105962, Test Accuracy: 0.611022, time: 7.6s\n",
            "  Epoch 893 @ step 1396000: Train Loss: 0.978697, Train Accuracy: 0.657875\n",
            "  Epoch 893 @ step 1397000: Train Loss: 0.977953, Train Accuracy: 0.659156\n",
            "Epoch 893 Test Loss: 1.069626, Test Accuracy: 0.623502, time: 8.6s\n",
            "  Epoch 894 @ step 1398000: Train Loss: 0.975691, Train Accuracy: 0.658469\n",
            "Epoch 894 Test Loss: 1.047458, Test Accuracy: 0.626697, time: 8.5s\n",
            "  Epoch 895 @ step 1399000: Train Loss: 0.983153, Train Accuracy: 0.655844\n",
            "  Epoch 895 @ step 1400000: Train Loss: 0.972407, Train Accuracy: 0.658750\n",
            "Epoch 895 Test Loss: 1.046484, Test Accuracy: 0.635583, time: 7.6s\n",
            "  Epoch 896 @ step 1401000: Train Loss: 0.977117, Train Accuracy: 0.658156\n",
            "  Epoch 896 @ step 1402000: Train Loss: 0.989106, Train Accuracy: 0.653688\n",
            "Epoch 896 Test Loss: 1.025161, Test Accuracy: 0.641474, time: 7.5s\n",
            "  Epoch 897 @ step 1403000: Train Loss: 0.967654, Train Accuracy: 0.663313\n",
            "Epoch 897 Test Loss: 1.010820, Test Accuracy: 0.646765, time: 7.6s\n",
            "  Epoch 898 @ step 1404000: Train Loss: 0.988544, Train Accuracy: 0.653844\n",
            "  Epoch 898 @ step 1405000: Train Loss: 0.980890, Train Accuracy: 0.651219\n",
            "Epoch 898 Test Loss: 1.038867, Test Accuracy: 0.624401, time: 7.7s\n",
            "  Epoch 899 @ step 1406000: Train Loss: 0.977846, Train Accuracy: 0.657750\n",
            "Epoch 899 Test Loss: 1.042047, Test Accuracy: 0.635683, time: 8.6s\n",
            "  Epoch 900 @ step 1407000: Train Loss: 0.974279, Train Accuracy: 0.659656\n",
            "  Epoch 900 @ step 1408000: Train Loss: 0.975990, Train Accuracy: 0.657813\n",
            "Epoch 900 Test Loss: 1.053414, Test Accuracy: 0.631090, time: 7.7s\n",
            "  Epoch 901 @ step 1409000: Train Loss: 0.979770, Train Accuracy: 0.657750\n",
            "Epoch 901 Test Loss: 1.145062, Test Accuracy: 0.587859, time: 7.6s\n",
            "  Epoch 902 @ step 1410000: Train Loss: 0.985783, Train Accuracy: 0.654656\n",
            "  Epoch 902 @ step 1411000: Train Loss: 0.981938, Train Accuracy: 0.657656\n",
            "Epoch 902 Test Loss: 1.067669, Test Accuracy: 0.628594, time: 7.5s\n",
            "  Epoch 903 @ step 1412000: Train Loss: 0.971278, Train Accuracy: 0.661781\n",
            "Epoch 903 Test Loss: 1.067122, Test Accuracy: 0.629393, time: 7.5s\n",
            "  Epoch 904 @ step 1413000: Train Loss: 0.987814, Train Accuracy: 0.655281\n",
            "  Epoch 904 @ step 1414000: Train Loss: 0.981457, Train Accuracy: 0.654344\n",
            "Epoch 904 Test Loss: 1.022744, Test Accuracy: 0.633786, time: 7.9s\n",
            "  Epoch 905 @ step 1415000: Train Loss: 0.969987, Train Accuracy: 0.660750\n",
            "  Epoch 905 @ step 1416000: Train Loss: 0.983595, Train Accuracy: 0.655969\n",
            "Epoch 905 Test Loss: 0.980561, Test Accuracy: 0.660044, time: 8.2s\n",
            "  Epoch 906 @ step 1417000: Train Loss: 0.981291, Train Accuracy: 0.655563\n",
            "Epoch 906 Test Loss: 1.049410, Test Accuracy: 0.628295, time: 7.6s\n",
            "  Epoch 907 @ step 1418000: Train Loss: 0.974841, Train Accuracy: 0.660813\n",
            "  Epoch 907 @ step 1419000: Train Loss: 0.984017, Train Accuracy: 0.655906\n",
            "Epoch 907 Test Loss: 1.038197, Test Accuracy: 0.634685, time: 7.6s\n",
            "  Epoch 908 @ step 1420000: Train Loss: 0.975474, Train Accuracy: 0.655625\n",
            "Epoch 908 Test Loss: 1.054445, Test Accuracy: 0.631789, time: 7.5s\n",
            "  Epoch 909 @ step 1421000: Train Loss: 0.975748, Train Accuracy: 0.659750\n",
            "  Epoch 909 @ step 1422000: Train Loss: 0.975866, Train Accuracy: 0.657281\n",
            "Epoch 909 Test Loss: 1.022740, Test Accuracy: 0.636082, time: 7.6s\n",
            "  Epoch 910 @ step 1423000: Train Loss: 0.976871, Train Accuracy: 0.657313\n",
            "Epoch 910 Test Loss: 1.072391, Test Accuracy: 0.615016, time: 8.1s\n",
            "  Epoch 911 @ step 1424000: Train Loss: 0.988923, Train Accuracy: 0.655563\n",
            "  Epoch 911 @ step 1425000: Train Loss: 0.971366, Train Accuracy: 0.658688\n",
            "Epoch 911 Test Loss: 1.064127, Test Accuracy: 0.622704, time: 8.0s\n",
            "  Epoch 912 @ step 1426000: Train Loss: 0.981566, Train Accuracy: 0.657375\n",
            "  Epoch 912 @ step 1427000: Train Loss: 0.986041, Train Accuracy: 0.655031\n",
            "Epoch 912 Test Loss: 1.040530, Test Accuracy: 0.635084, time: 7.9s\n",
            "  Epoch 913 @ step 1428000: Train Loss: 0.968907, Train Accuracy: 0.660563\n",
            "Epoch 913 Test Loss: 0.997343, Test Accuracy: 0.649261, time: 7.5s\n",
            "  Epoch 914 @ step 1429000: Train Loss: 0.980825, Train Accuracy: 0.658906\n",
            "  Epoch 914 @ step 1430000: Train Loss: 0.985267, Train Accuracy: 0.655313\n",
            "Epoch 914 Test Loss: 1.039823, Test Accuracy: 0.642971, time: 7.5s\n",
            "  Epoch 915 @ step 1431000: Train Loss: 0.973675, Train Accuracy: 0.660563\n",
            "Epoch 915 Test Loss: 1.159596, Test Accuracy: 0.589257, time: 7.8s\n",
            "  Epoch 916 @ step 1432000: Train Loss: 0.980008, Train Accuracy: 0.655969\n",
            "  Epoch 916 @ step 1433000: Train Loss: 0.974733, Train Accuracy: 0.657906\n",
            "Epoch 916 Test Loss: 1.077685, Test Accuracy: 0.618910, time: 8.7s\n",
            "  Epoch 917 @ step 1434000: Train Loss: 0.976805, Train Accuracy: 0.658594\n",
            "Epoch 917 Test Loss: 1.004079, Test Accuracy: 0.655351, time: 8.3s\n",
            "  Epoch 918 @ step 1435000: Train Loss: 0.985278, Train Accuracy: 0.656094\n",
            "  Epoch 918 @ step 1436000: Train Loss: 0.978754, Train Accuracy: 0.658375\n",
            "Epoch 918 Test Loss: 1.042464, Test Accuracy: 0.632987, time: 8.0s\n",
            "  Epoch 919 @ step 1437000: Train Loss: 0.975767, Train Accuracy: 0.657250\n",
            "Epoch 919 Test Loss: 1.009105, Test Accuracy: 0.646166, time: 7.7s\n",
            "  Epoch 920 @ step 1438000: Train Loss: 0.976775, Train Accuracy: 0.657688\n",
            "  Epoch 920 @ step 1439000: Train Loss: 0.978123, Train Accuracy: 0.658750\n",
            "Epoch 920 Test Loss: 1.036944, Test Accuracy: 0.633486, time: 7.6s\n",
            "  Epoch 921 @ step 1440000: Train Loss: 0.974226, Train Accuracy: 0.658500\n",
            "  Epoch 921 @ step 1441000: Train Loss: 0.982884, Train Accuracy: 0.657438\n",
            "Epoch 921 Test Loss: 1.009226, Test Accuracy: 0.649760, time: 7.5s\n",
            "  Epoch 922 @ step 1442000: Train Loss: 0.971382, Train Accuracy: 0.660563\n",
            "Epoch 922 Test Loss: 1.021142, Test Accuracy: 0.635982, time: 8.6s\n",
            "  Epoch 923 @ step 1443000: Train Loss: 0.979912, Train Accuracy: 0.656625\n",
            "  Epoch 923 @ step 1444000: Train Loss: 0.986431, Train Accuracy: 0.654781\n",
            "Epoch 923 Test Loss: 0.983424, Test Accuracy: 0.656749, time: 7.8s\n",
            "  Epoch 924 @ step 1445000: Train Loss: 0.970528, Train Accuracy: 0.659406\n",
            "Epoch 924 Test Loss: 1.025780, Test Accuracy: 0.644169, time: 8.1s\n",
            "  Epoch 925 @ step 1446000: Train Loss: 0.986688, Train Accuracy: 0.654031\n",
            "  Epoch 925 @ step 1447000: Train Loss: 0.977522, Train Accuracy: 0.656375\n",
            "Epoch 925 Test Loss: 0.999953, Test Accuracy: 0.657947, time: 7.7s\n",
            "  Epoch 926 @ step 1448000: Train Loss: 0.966464, Train Accuracy: 0.663031\n",
            "Epoch 926 Test Loss: 0.973876, Test Accuracy: 0.664736, time: 8.1s\n",
            "  Epoch 927 @ step 1449000: Train Loss: 0.987449, Train Accuracy: 0.653375\n",
            "  Epoch 927 @ step 1450000: Train Loss: 0.972788, Train Accuracy: 0.660531\n",
            "Epoch 927 Test Loss: 1.009025, Test Accuracy: 0.641973, time: 8.1s\n",
            "  Epoch 928 @ step 1451000: Train Loss: 0.981840, Train Accuracy: 0.655469\n",
            "  Epoch 928 @ step 1452000: Train Loss: 0.981370, Train Accuracy: 0.652938\n",
            "Epoch 928 Test Loss: 1.147924, Test Accuracy: 0.589956, time: 8.1s\n",
            "  Epoch 929 @ step 1453000: Train Loss: 0.970195, Train Accuracy: 0.661281\n",
            "Epoch 929 Test Loss: 1.000404, Test Accuracy: 0.642971, time: 7.6s\n",
            "  Epoch 930 @ step 1454000: Train Loss: 0.986507, Train Accuracy: 0.652688\n",
            "  Epoch 930 @ step 1455000: Train Loss: 0.978130, Train Accuracy: 0.657719\n",
            "Epoch 930 Test Loss: 1.103812, Test Accuracy: 0.603435, time: 7.5s\n",
            "  Epoch 931 @ step 1456000: Train Loss: 0.978494, Train Accuracy: 0.658656\n",
            "Epoch 931 Test Loss: 1.001648, Test Accuracy: 0.648962, time: 7.6s\n",
            "  Epoch 932 @ step 1457000: Train Loss: 0.974525, Train Accuracy: 0.659938\n",
            "  Epoch 932 @ step 1458000: Train Loss: 0.977044, Train Accuracy: 0.657250\n",
            "Epoch 932 Test Loss: 1.042664, Test Accuracy: 0.631390, time: 7.5s\n",
            "  Epoch 933 @ step 1459000: Train Loss: 0.974848, Train Accuracy: 0.658469\n",
            "Epoch 933 Test Loss: 1.034298, Test Accuracy: 0.640475, time: 8.5s\n",
            "  Epoch 934 @ step 1460000: Train Loss: 0.983138, Train Accuracy: 0.654969\n",
            "  Epoch 934 @ step 1461000: Train Loss: 0.977740, Train Accuracy: 0.658813\n",
            "Epoch 934 Test Loss: 1.020938, Test Accuracy: 0.632188, time: 7.4s\n",
            "  Epoch 935 @ step 1462000: Train Loss: 0.974292, Train Accuracy: 0.660875\n",
            "Epoch 935 Test Loss: 0.987679, Test Accuracy: 0.658946, time: 7.5s\n",
            "  Epoch 936 @ step 1463000: Train Loss: 0.982408, Train Accuracy: 0.654188\n",
            "  Epoch 936 @ step 1464000: Train Loss: 0.975204, Train Accuracy: 0.659750\n",
            "Epoch 936 Test Loss: 1.005626, Test Accuracy: 0.651158, time: 7.4s\n",
            "  Epoch 937 @ step 1465000: Train Loss: 0.973940, Train Accuracy: 0.656875\n",
            "  Epoch 937 @ step 1466000: Train Loss: 0.985544, Train Accuracy: 0.654688\n",
            "Epoch 937 Test Loss: 0.988979, Test Accuracy: 0.652955, time: 7.4s\n",
            "  Epoch 938 @ step 1467000: Train Loss: 0.979117, Train Accuracy: 0.659313\n",
            "Epoch 938 Test Loss: 1.007636, Test Accuracy: 0.641673, time: 7.7s\n",
            "  Epoch 939 @ step 1468000: Train Loss: 0.977113, Train Accuracy: 0.657656\n",
            "  Epoch 939 @ step 1469000: Train Loss: 0.976584, Train Accuracy: 0.658281\n",
            "Epoch 939 Test Loss: 1.106676, Test Accuracy: 0.603834, time: 8.8s\n",
            "  Epoch 940 @ step 1470000: Train Loss: 0.982307, Train Accuracy: 0.656813\n",
            "Epoch 940 Test Loss: 1.033567, Test Accuracy: 0.640875, time: 7.5s\n",
            "  Epoch 941 @ step 1471000: Train Loss: 0.968414, Train Accuracy: 0.660688\n",
            "  Epoch 941 @ step 1472000: Train Loss: 0.980507, Train Accuracy: 0.655563\n",
            "Epoch 941 Test Loss: 1.049822, Test Accuracy: 0.628295, time: 7.5s\n",
            "  Epoch 942 @ step 1473000: Train Loss: 0.975031, Train Accuracy: 0.656156\n",
            "Epoch 942 Test Loss: 1.000471, Test Accuracy: 0.655451, time: 7.5s\n",
            "  Epoch 943 @ step 1474000: Train Loss: 0.979834, Train Accuracy: 0.656750\n",
            "  Epoch 943 @ step 1475000: Train Loss: 0.978067, Train Accuracy: 0.658906\n",
            "Epoch 943 Test Loss: 1.105225, Test Accuracy: 0.618710, time: 7.6s\n",
            "  Epoch 944 @ step 1476000: Train Loss: 0.970869, Train Accuracy: 0.659344\n",
            "  Epoch 944 @ step 1477000: Train Loss: 0.985720, Train Accuracy: 0.653781\n",
            "Epoch 944 Test Loss: 1.154526, Test Accuracy: 0.595747, time: 7.6s\n",
            "  Epoch 945 @ step 1478000: Train Loss: 0.975473, Train Accuracy: 0.659313\n",
            "Epoch 945 Test Loss: 1.077829, Test Accuracy: 0.618411, time: 8.5s\n",
            "  Epoch 946 @ step 1479000: Train Loss: 0.986405, Train Accuracy: 0.656750\n",
            "  Epoch 946 @ step 1480000: Train Loss: 0.973223, Train Accuracy: 0.659719\n",
            "Epoch 946 Test Loss: 0.985871, Test Accuracy: 0.649561, time: 7.7s\n",
            "  Epoch 947 @ step 1481000: Train Loss: 0.974500, Train Accuracy: 0.657313\n",
            "Epoch 947 Test Loss: 1.127222, Test Accuracy: 0.600539, time: 7.6s\n",
            "  Epoch 948 @ step 1482000: Train Loss: 0.982241, Train Accuracy: 0.655469\n",
            "  Epoch 948 @ step 1483000: Train Loss: 0.984273, Train Accuracy: 0.652750\n",
            "Epoch 948 Test Loss: 1.054776, Test Accuracy: 0.627097, time: 7.6s\n",
            "  Epoch 949 @ step 1484000: Train Loss: 0.974695, Train Accuracy: 0.659969\n",
            "Epoch 949 Test Loss: 1.139511, Test Accuracy: 0.590954, time: 7.6s\n",
            "  Epoch 950 @ step 1485000: Train Loss: 0.978169, Train Accuracy: 0.658094\n",
            "  Epoch 950 @ step 1486000: Train Loss: 0.981132, Train Accuracy: 0.657813\n",
            "Epoch 950 Test Loss: 0.996381, Test Accuracy: 0.652556, time: 7.9s\n",
            "  Epoch 951 @ step 1487000: Train Loss: 0.981246, Train Accuracy: 0.656313\n",
            "Epoch 951 Test Loss: 0.988941, Test Accuracy: 0.652955, time: 8.1s\n",
            "  Epoch 952 @ step 1488000: Train Loss: 0.979506, Train Accuracy: 0.656625\n",
            "  Epoch 952 @ step 1489000: Train Loss: 0.979918, Train Accuracy: 0.655563\n",
            "Epoch 952 Test Loss: 0.977612, Test Accuracy: 0.655551, time: 7.6s\n",
            "  Epoch 953 @ step 1490000: Train Loss: 0.972716, Train Accuracy: 0.660750\n",
            "  Epoch 953 @ step 1491000: Train Loss: 0.984484, Train Accuracy: 0.655031\n",
            "Epoch 953 Test Loss: 1.107102, Test Accuracy: 0.601937, time: 7.6s\n",
            "  Epoch 954 @ step 1492000: Train Loss: 0.966683, Train Accuracy: 0.662063\n",
            "Epoch 954 Test Loss: 1.001842, Test Accuracy: 0.647464, time: 7.7s\n",
            "  Epoch 955 @ step 1493000: Train Loss: 0.987587, Train Accuracy: 0.651438\n",
            "  Epoch 955 @ step 1494000: Train Loss: 0.978502, Train Accuracy: 0.657094\n",
            "Epoch 955 Test Loss: 1.036244, Test Accuracy: 0.636881, time: 8.2s\n",
            "  Epoch 956 @ step 1495000: Train Loss: 0.973081, Train Accuracy: 0.661219\n",
            "Epoch 956 Test Loss: 1.063915, Test Accuracy: 0.620507, time: 8.5s\n",
            "  Epoch 957 @ step 1496000: Train Loss: 0.980001, Train Accuracy: 0.657906\n",
            "  Epoch 957 @ step 1497000: Train Loss: 0.983238, Train Accuracy: 0.655219\n",
            "Epoch 957 Test Loss: 1.030638, Test Accuracy: 0.642272, time: 7.6s\n",
            "  Epoch 958 @ step 1498000: Train Loss: 0.973136, Train Accuracy: 0.659219\n",
            "Epoch 958 Test Loss: 1.078382, Test Accuracy: 0.609026, time: 7.5s\n",
            "  Epoch 959 @ step 1499000: Train Loss: 0.983466, Train Accuracy: 0.656563\n",
            "  Epoch 959 @ step 1500000: Train Loss: 0.979401, Train Accuracy: 0.657219\n",
            "Epoch 959 Test Loss: 1.008923, Test Accuracy: 0.646466, time: 7.6s\n",
            "  Epoch 960 @ step 1501000: Train Loss: 0.966308, Train Accuracy: 0.664063\n",
            "  Epoch 960 @ step 1502000: Train Loss: 0.990018, Train Accuracy: 0.653469\n",
            "Epoch 960 Test Loss: 1.061992, Test Accuracy: 0.623802, time: 7.6s\n",
            "  Epoch 961 @ step 1503000: Train Loss: 0.982995, Train Accuracy: 0.657344\n",
            "Epoch 961 Test Loss: 1.087292, Test Accuracy: 0.616913, time: 8.6s\n",
            "  Epoch 962 @ step 1504000: Train Loss: 0.972324, Train Accuracy: 0.662000\n",
            "  Epoch 962 @ step 1505000: Train Loss: 0.983481, Train Accuracy: 0.655250\n",
            "Epoch 962 Test Loss: 1.060992, Test Accuracy: 0.620607, time: 8.5s\n",
            "  Epoch 963 @ step 1506000: Train Loss: 0.970737, Train Accuracy: 0.659969\n",
            "Epoch 963 Test Loss: 1.085792, Test Accuracy: 0.615615, time: 7.7s\n",
            "  Epoch 964 @ step 1507000: Train Loss: 0.984000, Train Accuracy: 0.656063\n",
            "  Epoch 964 @ step 1508000: Train Loss: 0.980760, Train Accuracy: 0.656969\n",
            "Epoch 964 Test Loss: 0.996916, Test Accuracy: 0.649661, time: 7.6s\n",
            "  Epoch 965 @ step 1509000: Train Loss: 0.975964, Train Accuracy: 0.657156\n",
            "Epoch 965 Test Loss: 1.019509, Test Accuracy: 0.637979, time: 7.6s\n",
            "  Epoch 966 @ step 1510000: Train Loss: 0.981452, Train Accuracy: 0.658125\n",
            "  Epoch 966 @ step 1511000: Train Loss: 0.975446, Train Accuracy: 0.660406\n",
            "Epoch 966 Test Loss: 0.985226, Test Accuracy: 0.657448, time: 7.7s\n",
            "  Epoch 967 @ step 1512000: Train Loss: 0.977452, Train Accuracy: 0.660094\n",
            "Epoch 967 Test Loss: 0.973097, Test Accuracy: 0.664936, time: 7.9s\n",
            "  Epoch 968 @ step 1513000: Train Loss: 0.984936, Train Accuracy: 0.658156\n",
            "  Epoch 968 @ step 1514000: Train Loss: 0.971427, Train Accuracy: 0.659438\n",
            "Epoch 968 Test Loss: 0.975331, Test Accuracy: 0.662340, time: 8.2s\n",
            "  Epoch 969 @ step 1515000: Train Loss: 0.980481, Train Accuracy: 0.658719\n",
            "  Epoch 969 @ step 1516000: Train Loss: 0.982578, Train Accuracy: 0.658250\n",
            "Epoch 969 Test Loss: 1.127079, Test Accuracy: 0.588758, time: 7.7s\n",
            "  Epoch 970 @ step 1517000: Train Loss: 0.971776, Train Accuracy: 0.655813\n",
            "Epoch 970 Test Loss: 1.007930, Test Accuracy: 0.640375, time: 7.6s\n",
            "  Epoch 971 @ step 1518000: Train Loss: 0.981893, Train Accuracy: 0.654469\n",
            "  Epoch 971 @ step 1519000: Train Loss: 0.979589, Train Accuracy: 0.657625\n",
            "Epoch 971 Test Loss: 1.061487, Test Accuracy: 0.627496, time: 7.5s\n",
            "  Epoch 972 @ step 1520000: Train Loss: 0.973338, Train Accuracy: 0.660625\n",
            "Epoch 972 Test Loss: 1.068589, Test Accuracy: 0.627097, time: 7.6s\n",
            "  Epoch 973 @ step 1521000: Train Loss: 0.978918, Train Accuracy: 0.658938\n",
            "  Epoch 973 @ step 1522000: Train Loss: 0.983290, Train Accuracy: 0.653813\n",
            "Epoch 973 Test Loss: 1.035745, Test Accuracy: 0.639976, time: 8.3s\n",
            "  Epoch 974 @ step 1523000: Train Loss: 0.978738, Train Accuracy: 0.659781\n",
            "Epoch 974 Test Loss: 1.044157, Test Accuracy: 0.632388, time: 7.8s\n",
            "  Epoch 975 @ step 1524000: Train Loss: 0.982682, Train Accuracy: 0.654031\n",
            "  Epoch 975 @ step 1525000: Train Loss: 0.981884, Train Accuracy: 0.654125\n",
            "Epoch 975 Test Loss: 0.989540, Test Accuracy: 0.656250, time: 7.6s\n",
            "  Epoch 976 @ step 1526000: Train Loss: 0.969923, Train Accuracy: 0.658625\n",
            "  Epoch 976 @ step 1527000: Train Loss: 0.984971, Train Accuracy: 0.652250\n",
            "Epoch 976 Test Loss: 1.062173, Test Accuracy: 0.621805, time: 7.6s\n",
            "  Epoch 977 @ step 1528000: Train Loss: 0.973386, Train Accuracy: 0.659531\n",
            "Epoch 977 Test Loss: 1.016864, Test Accuracy: 0.637780, time: 8.1s\n",
            "  Epoch 978 @ step 1529000: Train Loss: 0.981495, Train Accuracy: 0.654563\n",
            "  Epoch 978 @ step 1530000: Train Loss: 0.987608, Train Accuracy: 0.656000\n",
            "Epoch 978 Test Loss: 1.005047, Test Accuracy: 0.646565, time: 7.7s\n",
            "  Epoch 979 @ step 1531000: Train Loss: 0.967600, Train Accuracy: 0.659563\n",
            "Epoch 979 Test Loss: 1.102677, Test Accuracy: 0.593450, time: 8.6s\n",
            "  Epoch 980 @ step 1532000: Train Loss: 0.980505, Train Accuracy: 0.658250\n",
            "  Epoch 980 @ step 1533000: Train Loss: 0.981586, Train Accuracy: 0.657000\n",
            "Epoch 980 Test Loss: 1.024651, Test Accuracy: 0.636382, time: 7.7s\n",
            "  Epoch 981 @ step 1534000: Train Loss: 0.970262, Train Accuracy: 0.658906\n",
            "Epoch 981 Test Loss: 1.029773, Test Accuracy: 0.635883, time: 7.7s\n",
            "  Epoch 982 @ step 1535000: Train Loss: 0.984855, Train Accuracy: 0.656375\n",
            "  Epoch 982 @ step 1536000: Train Loss: 0.977968, Train Accuracy: 0.656406\n",
            "Epoch 982 Test Loss: 1.003068, Test Accuracy: 0.648263, time: 7.8s\n",
            "  Epoch 983 @ step 1537000: Train Loss: 0.977945, Train Accuracy: 0.657375\n",
            "Epoch 983 Test Loss: 1.112135, Test Accuracy: 0.596246, time: 8.5s\n",
            "  Epoch 984 @ step 1538000: Train Loss: 0.980305, Train Accuracy: 0.656875\n",
            "  Epoch 984 @ step 1539000: Train Loss: 0.979886, Train Accuracy: 0.658969\n",
            "Epoch 984 Test Loss: 1.142941, Test Accuracy: 0.589657, time: 7.9s\n",
            "  Epoch 985 @ step 1540000: Train Loss: 0.971758, Train Accuracy: 0.659813\n",
            "  Epoch 985 @ step 1541000: Train Loss: 0.986020, Train Accuracy: 0.656219\n",
            "Epoch 985 Test Loss: 1.491077, Test Accuracy: 0.521865, time: 8.3s\n",
            "  Epoch 986 @ step 1542000: Train Loss: 0.973555, Train Accuracy: 0.659750\n",
            "Epoch 986 Test Loss: 1.170192, Test Accuracy: 0.579073, time: 7.7s\n",
            "  Epoch 987 @ step 1543000: Train Loss: 0.980656, Train Accuracy: 0.656250\n",
            "  Epoch 987 @ step 1544000: Train Loss: 0.979735, Train Accuracy: 0.658250\n",
            "Epoch 987 Test Loss: 0.987908, Test Accuracy: 0.657947, time: 7.8s\n",
            "  Epoch 988 @ step 1545000: Train Loss: 0.979109, Train Accuracy: 0.659875\n",
            "Epoch 988 Test Loss: 0.974879, Test Accuracy: 0.658746, time: 7.7s\n",
            "  Epoch 989 @ step 1546000: Train Loss: 0.973486, Train Accuracy: 0.657500\n",
            "  Epoch 989 @ step 1547000: Train Loss: 0.981564, Train Accuracy: 0.654000\n",
            "Epoch 989 Test Loss: 1.064342, Test Accuracy: 0.629593, time: 7.7s\n",
            "  Epoch 990 @ step 1548000: Train Loss: 0.974510, Train Accuracy: 0.661281\n",
            "Epoch 990 Test Loss: 1.106848, Test Accuracy: 0.606530, time: 8.3s\n",
            "  Epoch 991 @ step 1549000: Train Loss: 0.986210, Train Accuracy: 0.653844\n",
            "  Epoch 991 @ step 1550000: Train Loss: 0.978843, Train Accuracy: 0.655500\n",
            "Epoch 991 Test Loss: 1.023127, Test Accuracy: 0.634285, time: 8.0s\n",
            "  Epoch 992 @ step 1551000: Train Loss: 0.972505, Train Accuracy: 0.659250\n",
            "  Epoch 992 @ step 1552000: Train Loss: 0.986307, Train Accuracy: 0.652031\n",
            "Epoch 992 Test Loss: 1.001876, Test Accuracy: 0.644369, time: 7.7s\n",
            "  Epoch 993 @ step 1553000: Train Loss: 0.972928, Train Accuracy: 0.661406\n",
            "Epoch 993 Test Loss: 1.089801, Test Accuracy: 0.619708, time: 7.7s\n",
            "  Epoch 994 @ step 1554000: Train Loss: 0.978165, Train Accuracy: 0.656625\n",
            "  Epoch 994 @ step 1555000: Train Loss: 0.979715, Train Accuracy: 0.658063\n",
            "Epoch 994 Test Loss: 1.008107, Test Accuracy: 0.642472, time: 7.6s\n",
            "  Epoch 995 @ step 1556000: Train Loss: 0.977035, Train Accuracy: 0.656594\n",
            "Epoch 995 Test Loss: 1.079514, Test Accuracy: 0.621006, time: 7.6s\n",
            "  Epoch 996 @ step 1557000: Train Loss: 0.977845, Train Accuracy: 0.658875\n",
            "  Epoch 996 @ step 1558000: Train Loss: 0.983435, Train Accuracy: 0.655594\n",
            "Epoch 996 Test Loss: 0.991058, Test Accuracy: 0.650359, time: 8.5s\n",
            "  Epoch 997 @ step 1559000: Train Loss: 0.976159, Train Accuracy: 0.653281\n",
            "Epoch 997 Test Loss: 1.053683, Test Accuracy: 0.628594, time: 7.6s\n",
            "  Epoch 998 @ step 1560000: Train Loss: 0.980534, Train Accuracy: 0.656469\n",
            "  Epoch 998 @ step 1561000: Train Loss: 0.977881, Train Accuracy: 0.659281\n",
            "Epoch 998 Test Loss: 1.174178, Test Accuracy: 0.581869, time: 8.0s\n",
            "  Epoch 999 @ step 1562000: Train Loss: 0.973746, Train Accuracy: 0.659406\n",
            "Epoch 999 Test Loss: 1.079038, Test Accuracy: 0.617812, time: 7.6s\n",
            "  Epoch 1000 @ step 1563000: Train Loss: 0.988135, Train Accuracy: 0.651094\n",
            "  Epoch 1000 @ step 1564000: Train Loss: 0.974646, Train Accuracy: 0.657781\n",
            "Epoch 1000 Test Loss: 0.997845, Test Accuracy: 0.645567, time: 7.5s\n",
            "  Epoch 1001 @ step 1565000: Train Loss: 0.977253, Train Accuracy: 0.655156\n",
            "  Epoch 1001 @ step 1566000: Train Loss: 0.983322, Train Accuracy: 0.656406\n",
            "Epoch 1001 Test Loss: 1.047339, Test Accuracy: 0.628295, time: 7.6s\n",
            "  Epoch 1002 @ step 1567000: Train Loss: 0.976920, Train Accuracy: 0.658906\n",
            "Epoch 1002 Test Loss: 1.047460, Test Accuracy: 0.629393, time: 8.5s\n",
            "  Epoch 1003 @ step 1568000: Train Loss: 0.976394, Train Accuracy: 0.659844\n",
            "  Epoch 1003 @ step 1569000: Train Loss: 0.981815, Train Accuracy: 0.655406\n",
            "Epoch 1003 Test Loss: 0.971593, Test Accuracy: 0.663239, time: 7.5s\n",
            "  Epoch 1004 @ step 1570000: Train Loss: 0.973386, Train Accuracy: 0.660469\n",
            "Epoch 1004 Test Loss: 1.014367, Test Accuracy: 0.640575, time: 7.7s\n",
            "  Epoch 1005 @ step 1571000: Train Loss: 0.978826, Train Accuracy: 0.655500\n",
            "  Epoch 1005 @ step 1572000: Train Loss: 0.985336, Train Accuracy: 0.653719\n",
            "Epoch 1005 Test Loss: 0.991804, Test Accuracy: 0.650559, time: 8.6s\n",
            "  Epoch 1006 @ step 1573000: Train Loss: 0.967646, Train Accuracy: 0.665188\n",
            "Epoch 1006 Test Loss: 1.017461, Test Accuracy: 0.645966, time: 7.6s\n",
            "  Epoch 1007 @ step 1574000: Train Loss: 0.983354, Train Accuracy: 0.653594\n",
            "  Epoch 1007 @ step 1575000: Train Loss: 0.978076, Train Accuracy: 0.658594\n",
            "Epoch 1007 Test Loss: 1.003438, Test Accuracy: 0.649261, time: 7.9s\n",
            "  Epoch 1008 @ step 1576000: Train Loss: 0.981236, Train Accuracy: 0.656531\n",
            "  Epoch 1008 @ step 1577000: Train Loss: 0.982927, Train Accuracy: 0.655313\n",
            "Epoch 1008 Test Loss: 1.005624, Test Accuracy: 0.654752, time: 8.1s\n",
            "  Epoch 1009 @ step 1578000: Train Loss: 0.971810, Train Accuracy: 0.660344\n",
            "Epoch 1009 Test Loss: 1.063680, Test Accuracy: 0.625000, time: 7.6s\n",
            "  Epoch 1010 @ step 1579000: Train Loss: 0.985485, Train Accuracy: 0.652875\n",
            "  Epoch 1010 @ step 1580000: Train Loss: 0.977884, Train Accuracy: 0.659938\n",
            "Epoch 1010 Test Loss: 0.993866, Test Accuracy: 0.652456, time: 7.6s\n",
            "  Epoch 1011 @ step 1581000: Train Loss: 0.977674, Train Accuracy: 0.657656\n",
            "Epoch 1011 Test Loss: 1.214385, Test Accuracy: 0.576478, time: 7.7s\n",
            "  Epoch 1012 @ step 1582000: Train Loss: 0.974961, Train Accuracy: 0.656469\n",
            "  Epoch 1012 @ step 1583000: Train Loss: 0.987358, Train Accuracy: 0.652750\n",
            "Epoch 1012 Test Loss: 0.976976, Test Accuracy: 0.657049, time: 7.8s\n",
            "  Epoch 1013 @ step 1584000: Train Loss: 0.967090, Train Accuracy: 0.661406\n",
            "Epoch 1013 Test Loss: 1.028720, Test Accuracy: 0.640974, time: 8.1s\n",
            "  Epoch 1014 @ step 1585000: Train Loss: 0.985247, Train Accuracy: 0.656875\n",
            "  Epoch 1014 @ step 1586000: Train Loss: 0.978084, Train Accuracy: 0.657938\n",
            "Epoch 1014 Test Loss: 1.006302, Test Accuracy: 0.651058, time: 7.7s\n",
            "  Epoch 1015 @ step 1587000: Train Loss: 0.979194, Train Accuracy: 0.658625\n",
            "  Epoch 1015 @ step 1588000: Train Loss: 0.982404, Train Accuracy: 0.655594\n",
            "Epoch 1015 Test Loss: 1.094719, Test Accuracy: 0.615315, time: 8.6s\n",
            "  Epoch 1016 @ step 1589000: Train Loss: 0.975389, Train Accuracy: 0.658906\n",
            "Epoch 1016 Test Loss: 1.018257, Test Accuracy: 0.635583, time: 7.6s\n",
            "  Epoch 1017 @ step 1590000: Train Loss: 0.971411, Train Accuracy: 0.656469\n",
            "  Epoch 1017 @ step 1591000: Train Loss: 0.985801, Train Accuracy: 0.654344\n",
            "Epoch 1017 Test Loss: 1.012409, Test Accuracy: 0.638878, time: 7.5s\n",
            "  Epoch 1018 @ step 1592000: Train Loss: 0.972547, Train Accuracy: 0.660375\n",
            "Epoch 1018 Test Loss: 1.069968, Test Accuracy: 0.618111, time: 7.5s\n",
            "  Epoch 1019 @ step 1593000: Train Loss: 0.981753, Train Accuracy: 0.656594\n",
            "  Epoch 1019 @ step 1594000: Train Loss: 0.975886, Train Accuracy: 0.657469\n",
            "Epoch 1019 Test Loss: 0.960574, Test Accuracy: 0.669229, time: 7.5s\n",
            "  Epoch 1020 @ step 1595000: Train Loss: 0.981462, Train Accuracy: 0.656875\n",
            "Epoch 1020 Test Loss: 1.012782, Test Accuracy: 0.637280, time: 7.7s\n",
            "  Epoch 1021 @ step 1596000: Train Loss: 0.978848, Train Accuracy: 0.655969\n",
            "  Epoch 1021 @ step 1597000: Train Loss: 0.976002, Train Accuracy: 0.659781\n",
            "Epoch 1021 Test Loss: 1.011522, Test Accuracy: 0.643770, time: 8.3s\n",
            "  Epoch 1022 @ step 1598000: Train Loss: 0.981659, Train Accuracy: 0.657250\n",
            "Epoch 1022 Test Loss: 0.991885, Test Accuracy: 0.654253, time: 7.5s\n",
            "  Epoch 1023 @ step 1599000: Train Loss: 0.980600, Train Accuracy: 0.655094\n",
            "  Epoch 1023 @ step 1600000: Train Loss: 0.974780, Train Accuracy: 0.659375\n",
            "Epoch 1023 Test Loss: 1.061876, Test Accuracy: 0.631290, time: 7.6s\n",
            "  Epoch 1024 @ step 1601000: Train Loss: 0.977355, Train Accuracy: 0.657625\n",
            "  Epoch 1024 @ step 1602000: Train Loss: 0.984377, Train Accuracy: 0.655781\n",
            "Epoch 1024 Test Loss: 1.111929, Test Accuracy: 0.605731, time: 7.6s\n",
            "  Epoch 1025 @ step 1603000: Train Loss: 0.968965, Train Accuracy: 0.662375\n",
            "Epoch 1025 Test Loss: 1.092552, Test Accuracy: 0.602835, time: 7.6s\n",
            "  Epoch 1026 @ step 1604000: Train Loss: 0.982515, Train Accuracy: 0.658625\n",
            "  Epoch 1026 @ step 1605000: Train Loss: 0.985300, Train Accuracy: 0.654969\n",
            "Epoch 1026 Test Loss: 1.047507, Test Accuracy: 0.622704, time: 8.0s\n",
            "  Epoch 1027 @ step 1606000: Train Loss: 0.971636, Train Accuracy: 0.660750\n",
            "Epoch 1027 Test Loss: 0.985415, Test Accuracy: 0.656050, time: 8.0s\n",
            "  Epoch 1028 @ step 1607000: Train Loss: 0.979113, Train Accuracy: 0.657938\n",
            "  Epoch 1028 @ step 1608000: Train Loss: 0.985192, Train Accuracy: 0.653625\n",
            "Epoch 1028 Test Loss: 1.072391, Test Accuracy: 0.614617, time: 7.7s\n",
            "  Epoch 1029 @ step 1609000: Train Loss: 0.972565, Train Accuracy: 0.659719\n",
            "Epoch 1029 Test Loss: 1.123304, Test Accuracy: 0.598542, time: 7.6s\n",
            "  Epoch 1030 @ step 1610000: Train Loss: 0.982983, Train Accuracy: 0.653063\n",
            "  Epoch 1030 @ step 1611000: Train Loss: 0.983286, Train Accuracy: 0.658250\n",
            "Epoch 1030 Test Loss: 1.002936, Test Accuracy: 0.649860, time: 7.6s\n",
            "  Epoch 1031 @ step 1612000: Train Loss: 0.973640, Train Accuracy: 0.657688\n",
            "  Epoch 1031 @ step 1613000: Train Loss: 0.981315, Train Accuracy: 0.657031\n",
            "Epoch 1031 Test Loss: 1.035801, Test Accuracy: 0.640375, time: 7.6s\n",
            "  Epoch 1032 @ step 1614000: Train Loss: 0.978519, Train Accuracy: 0.659688\n",
            "Epoch 1032 Test Loss: 1.088732, Test Accuracy: 0.612520, time: 8.4s\n",
            "  Epoch 1033 @ step 1615000: Train Loss: 0.974493, Train Accuracy: 0.660125\n",
            "  Epoch 1033 @ step 1616000: Train Loss: 0.981801, Train Accuracy: 0.656000\n",
            "Epoch 1033 Test Loss: 0.981309, Test Accuracy: 0.663239, time: 7.7s\n",
            "  Epoch 1034 @ step 1617000: Train Loss: 0.976989, Train Accuracy: 0.656906\n",
            "Epoch 1034 Test Loss: 1.020207, Test Accuracy: 0.636681, time: 7.6s\n",
            "  Epoch 1035 @ step 1618000: Train Loss: 0.974132, Train Accuracy: 0.657219\n",
            "  Epoch 1035 @ step 1619000: Train Loss: 0.979077, Train Accuracy: 0.657375\n",
            "Epoch 1035 Test Loss: 1.010975, Test Accuracy: 0.642572, time: 7.7s\n",
            "  Epoch 1036 @ step 1620000: Train Loss: 0.981356, Train Accuracy: 0.659844\n",
            "Epoch 1036 Test Loss: 1.064373, Test Accuracy: 0.624201, time: 8.5s\n",
            "  Epoch 1037 @ step 1621000: Train Loss: 0.973938, Train Accuracy: 0.656563\n",
            "  Epoch 1037 @ step 1622000: Train Loss: 0.976224, Train Accuracy: 0.659969\n",
            "Epoch 1037 Test Loss: 1.012111, Test Accuracy: 0.641673, time: 7.6s\n",
            "  Epoch 1038 @ step 1623000: Train Loss: 0.980328, Train Accuracy: 0.654844\n",
            "Epoch 1038 Test Loss: 1.075289, Test Accuracy: 0.620008, time: 8.5s\n",
            "  Epoch 1039 @ step 1624000: Train Loss: 0.977069, Train Accuracy: 0.659375\n",
            "  Epoch 1039 @ step 1625000: Train Loss: 0.971967, Train Accuracy: 0.661031\n",
            "Epoch 1039 Test Loss: 1.013631, Test Accuracy: 0.630791, time: 7.5s\n",
            "  Epoch 1040 @ step 1626000: Train Loss: 0.971386, Train Accuracy: 0.659875\n",
            "  Epoch 1040 @ step 1627000: Train Loss: 0.990470, Train Accuracy: 0.654656\n",
            "Epoch 1040 Test Loss: 1.006060, Test Accuracy: 0.653954, time: 7.6s\n",
            "  Epoch 1041 @ step 1628000: Train Loss: 0.978558, Train Accuracy: 0.658438\n",
            "Epoch 1041 Test Loss: 1.003753, Test Accuracy: 0.646865, time: 7.6s\n",
            "  Epoch 1042 @ step 1629000: Train Loss: 0.977174, Train Accuracy: 0.659219\n",
            "  Epoch 1042 @ step 1630000: Train Loss: 0.980514, Train Accuracy: 0.657094\n",
            "Epoch 1042 Test Loss: 1.077231, Test Accuracy: 0.619209, time: 7.5s\n",
            "  Epoch 1043 @ step 1631000: Train Loss: 0.981289, Train Accuracy: 0.654938\n",
            "Epoch 1043 Test Loss: 1.157673, Test Accuracy: 0.597544, time: 7.7s\n",
            "  Epoch 1044 @ step 1632000: Train Loss: 0.976591, Train Accuracy: 0.658344\n",
            "  Epoch 1044 @ step 1633000: Train Loss: 0.971425, Train Accuracy: 0.657125\n",
            "Epoch 1044 Test Loss: 1.054581, Test Accuracy: 0.630192, time: 8.4s\n",
            "  Epoch 1045 @ step 1634000: Train Loss: 0.984332, Train Accuracy: 0.655938\n",
            "Epoch 1045 Test Loss: 1.003576, Test Accuracy: 0.646266, time: 7.5s\n",
            "  Epoch 1046 @ step 1635000: Train Loss: 0.982557, Train Accuracy: 0.653469\n",
            "  Epoch 1046 @ step 1636000: Train Loss: 0.973470, Train Accuracy: 0.657250\n",
            "Epoch 1046 Test Loss: 1.064035, Test Accuracy: 0.622204, time: 7.6s\n",
            "  Epoch 1047 @ step 1637000: Train Loss: 0.978113, Train Accuracy: 0.659594\n",
            "  Epoch 1047 @ step 1638000: Train Loss: 0.985256, Train Accuracy: 0.651906\n",
            "Epoch 1047 Test Loss: 0.981976, Test Accuracy: 0.656150, time: 7.6s\n",
            "  Epoch 1048 @ step 1639000: Train Loss: 0.969352, Train Accuracy: 0.662406\n",
            "Epoch 1048 Test Loss: 0.992325, Test Accuracy: 0.656450, time: 7.6s\n",
            "  Epoch 1049 @ step 1640000: Train Loss: 0.987927, Train Accuracy: 0.654250\n",
            "  Epoch 1049 @ step 1641000: Train Loss: 0.977053, Train Accuracy: 0.657281\n",
            "Epoch 1049 Test Loss: 0.988541, Test Accuracy: 0.652556, time: 8.0s\n",
            "  Epoch 1050 @ step 1642000: Train Loss: 0.974259, Train Accuracy: 0.658500\n",
            "Epoch 1050 Test Loss: 1.033039, Test Accuracy: 0.636681, time: 7.9s\n",
            "  Epoch 1051 @ step 1643000: Train Loss: 0.982235, Train Accuracy: 0.656688\n",
            "  Epoch 1051 @ step 1644000: Train Loss: 0.975738, Train Accuracy: 0.657406\n",
            "Epoch 1051 Test Loss: 0.974343, Test Accuracy: 0.661841, time: 7.5s\n",
            "  Epoch 1052 @ step 1645000: Train Loss: 0.972188, Train Accuracy: 0.659938\n",
            "Epoch 1052 Test Loss: 1.003769, Test Accuracy: 0.641673, time: 7.5s\n",
            "  Epoch 1053 @ step 1646000: Train Loss: 0.987647, Train Accuracy: 0.653313\n",
            "  Epoch 1053 @ step 1647000: Train Loss: 0.973295, Train Accuracy: 0.658250\n",
            "Epoch 1053 Test Loss: 0.985380, Test Accuracy: 0.652057, time: 7.5s\n",
            "  Epoch 1054 @ step 1648000: Train Loss: 0.974786, Train Accuracy: 0.658906\n",
            "Epoch 1054 Test Loss: 1.111346, Test Accuracy: 0.603634, time: 7.4s\n",
            "  Epoch 1055 @ step 1649000: Train Loss: 0.990531, Train Accuracy: 0.655375\n",
            "  Epoch 1055 @ step 1650000: Train Loss: 0.976883, Train Accuracy: 0.656656\n",
            "Epoch 1055 Test Loss: 1.074639, Test Accuracy: 0.620308, time: 8.3s\n",
            "  Epoch 1056 @ step 1651000: Train Loss: 0.972068, Train Accuracy: 0.660719\n",
            "  Epoch 1056 @ step 1652000: Train Loss: 0.982067, Train Accuracy: 0.658938\n",
            "Epoch 1056 Test Loss: 1.015087, Test Accuracy: 0.644069, time: 7.7s\n",
            "  Epoch 1057 @ step 1653000: Train Loss: 0.974941, Train Accuracy: 0.659406\n",
            "Epoch 1057 Test Loss: 1.067045, Test Accuracy: 0.624002, time: 8.0s\n",
            "  Epoch 1058 @ step 1654000: Train Loss: 0.979004, Train Accuracy: 0.658281\n",
            "  Epoch 1058 @ step 1655000: Train Loss: 0.977522, Train Accuracy: 0.659688\n",
            "Epoch 1058 Test Loss: 0.986242, Test Accuracy: 0.658147, time: 8.7s\n",
            "  Epoch 1059 @ step 1656000: Train Loss: 0.980197, Train Accuracy: 0.657125\n",
            "Epoch 1059 Test Loss: 0.997828, Test Accuracy: 0.650759, time: 7.6s\n",
            "  Epoch 1060 @ step 1657000: Train Loss: 0.976813, Train Accuracy: 0.660906\n",
            "  Epoch 1060 @ step 1658000: Train Loss: 0.979095, Train Accuracy: 0.655250\n",
            "Epoch 1060 Test Loss: 1.043011, Test Accuracy: 0.631290, time: 7.6s\n",
            "  Epoch 1061 @ step 1659000: Train Loss: 0.980863, Train Accuracy: 0.658063\n",
            "Epoch 1061 Test Loss: 1.023967, Test Accuracy: 0.638878, time: 8.6s\n",
            "  Epoch 1062 @ step 1660000: Train Loss: 0.978570, Train Accuracy: 0.656281\n",
            "  Epoch 1062 @ step 1661000: Train Loss: 0.976923, Train Accuracy: 0.656594\n",
            "Epoch 1062 Test Loss: 1.028125, Test Accuracy: 0.637380, time: 7.6s\n",
            "  Epoch 1063 @ step 1662000: Train Loss: 0.979916, Train Accuracy: 0.657125\n",
            "  Epoch 1063 @ step 1663000: Train Loss: 0.982806, Train Accuracy: 0.656875\n",
            "Epoch 1063 Test Loss: 1.062987, Test Accuracy: 0.622804, time: 7.6s\n",
            "  Epoch 1064 @ step 1664000: Train Loss: 0.970987, Train Accuracy: 0.658531\n",
            "Epoch 1064 Test Loss: 1.001539, Test Accuracy: 0.653854, time: 7.6s\n",
            "  Epoch 1065 @ step 1665000: Train Loss: 0.985472, Train Accuracy: 0.658000\n",
            "  Epoch 1065 @ step 1666000: Train Loss: 0.978856, Train Accuracy: 0.659625\n",
            "Epoch 1065 Test Loss: 1.007761, Test Accuracy: 0.637580, time: 7.5s\n",
            "  Epoch 1066 @ step 1667000: Train Loss: 0.978469, Train Accuracy: 0.656281\n",
            "Epoch 1066 Test Loss: 0.995853, Test Accuracy: 0.653255, time: 7.6s\n",
            "  Epoch 1067 @ step 1668000: Train Loss: 0.978418, Train Accuracy: 0.653063\n",
            "  Epoch 1067 @ step 1669000: Train Loss: 0.970140, Train Accuracy: 0.659188\n",
            "Epoch 1067 Test Loss: 1.001612, Test Accuracy: 0.651957, time: 8.5s\n",
            "  Epoch 1068 @ step 1670000: Train Loss: 0.978856, Train Accuracy: 0.658469\n",
            "Epoch 1068 Test Loss: 1.085391, Test Accuracy: 0.605431, time: 7.7s\n",
            "  Epoch 1069 @ step 1671000: Train Loss: 0.983412, Train Accuracy: 0.653375\n",
            "  Epoch 1069 @ step 1672000: Train Loss: 0.982478, Train Accuracy: 0.657250\n",
            "Epoch 1069 Test Loss: 1.026567, Test Accuracy: 0.634784, time: 7.5s\n",
            "  Epoch 1070 @ step 1673000: Train Loss: 0.969009, Train Accuracy: 0.663156\n",
            "Epoch 1070 Test Loss: 1.027051, Test Accuracy: 0.635284, time: 7.6s\n",
            "  Epoch 1071 @ step 1674000: Train Loss: 0.988121, Train Accuracy: 0.654844\n",
            "  Epoch 1071 @ step 1675000: Train Loss: 0.978122, Train Accuracy: 0.656906\n",
            "Epoch 1071 Test Loss: 1.007262, Test Accuracy: 0.646865, time: 7.6s\n",
            "  Epoch 1072 @ step 1676000: Train Loss: 0.976285, Train Accuracy: 0.661063\n",
            "  Epoch 1072 @ step 1677000: Train Loss: 0.979349, Train Accuracy: 0.656250\n",
            "Epoch 1072 Test Loss: 0.997092, Test Accuracy: 0.649161, time: 8.0s\n",
            "  Epoch 1073 @ step 1678000: Train Loss: 0.977292, Train Accuracy: 0.655156\n",
            "Epoch 1073 Test Loss: 1.000657, Test Accuracy: 0.646266, time: 8.1s\n",
            "  Epoch 1074 @ step 1679000: Train Loss: 0.974076, Train Accuracy: 0.661000\n",
            "  Epoch 1074 @ step 1680000: Train Loss: 0.977496, Train Accuracy: 0.657406\n",
            "Epoch 1074 Test Loss: 1.022887, Test Accuracy: 0.639677, time: 7.6s\n",
            "  Epoch 1075 @ step 1681000: Train Loss: 0.978721, Train Accuracy: 0.657844\n",
            "Epoch 1075 Test Loss: 0.983767, Test Accuracy: 0.657248, time: 7.6s\n",
            "  Epoch 1076 @ step 1682000: Train Loss: 0.976445, Train Accuracy: 0.655719\n",
            "  Epoch 1076 @ step 1683000: Train Loss: 0.987179, Train Accuracy: 0.652844\n",
            "Epoch 1076 Test Loss: 0.977767, Test Accuracy: 0.657049, time: 7.6s\n",
            "  Epoch 1077 @ step 1684000: Train Loss: 0.973979, Train Accuracy: 0.660063\n",
            "Epoch 1077 Test Loss: 1.062934, Test Accuracy: 0.621605, time: 7.6s\n",
            "  Epoch 1078 @ step 1685000: Train Loss: 0.979952, Train Accuracy: 0.656031\n",
            "  Epoch 1078 @ step 1686000: Train Loss: 0.975596, Train Accuracy: 0.657000\n",
            "Epoch 1078 Test Loss: 1.051111, Test Accuracy: 0.627396, time: 8.2s\n",
            "  Epoch 1079 @ step 1687000: Train Loss: 0.973740, Train Accuracy: 0.659781\n",
            "  Epoch 1079 @ step 1688000: Train Loss: 0.987228, Train Accuracy: 0.651719\n",
            "Epoch 1079 Test Loss: 1.047085, Test Accuracy: 0.636881, time: 8.4s\n",
            "  Epoch 1080 @ step 1689000: Train Loss: 0.972187, Train Accuracy: 0.660906\n",
            "Epoch 1080 Test Loss: 1.065842, Test Accuracy: 0.624900, time: 8.7s\n",
            "  Epoch 1081 @ step 1690000: Train Loss: 0.979477, Train Accuracy: 0.659938\n",
            "  Epoch 1081 @ step 1691000: Train Loss: 0.981942, Train Accuracy: 0.656781\n",
            "Epoch 1081 Test Loss: 1.096467, Test Accuracy: 0.612420, time: 7.7s\n",
            "  Epoch 1082 @ step 1692000: Train Loss: 0.971381, Train Accuracy: 0.661594\n",
            "Epoch 1082 Test Loss: 1.050035, Test Accuracy: 0.627196, time: 7.7s\n",
            "  Epoch 1083 @ step 1693000: Train Loss: 0.980912, Train Accuracy: 0.658469\n",
            "  Epoch 1083 @ step 1694000: Train Loss: 0.978391, Train Accuracy: 0.655063\n",
            "Epoch 1083 Test Loss: 0.997176, Test Accuracy: 0.657049, time: 7.7s\n",
            "  Epoch 1084 @ step 1695000: Train Loss: 0.972407, Train Accuracy: 0.660250\n",
            "Epoch 1084 Test Loss: 1.030863, Test Accuracy: 0.636182, time: 8.5s\n",
            "  Epoch 1085 @ step 1696000: Train Loss: 0.984923, Train Accuracy: 0.654406\n",
            "  Epoch 1085 @ step 1697000: Train Loss: 0.974250, Train Accuracy: 0.659625\n",
            "Epoch 1085 Test Loss: 1.023084, Test Accuracy: 0.643271, time: 7.6s\n",
            "  Epoch 1086 @ step 1698000: Train Loss: 0.978154, Train Accuracy: 0.656094\n",
            "Epoch 1086 Test Loss: 1.012550, Test Accuracy: 0.642372, time: 7.5s\n",
            "  Epoch 1087 @ step 1699000: Train Loss: 0.982871, Train Accuracy: 0.653844\n",
            "  Epoch 1087 @ step 1700000: Train Loss: 0.984505, Train Accuracy: 0.655313\n",
            "Epoch 1087 Test Loss: 1.022434, Test Accuracy: 0.642073, time: 7.6s\n",
            "  Epoch 1088 @ step 1701000: Train Loss: 0.965151, Train Accuracy: 0.661500\n",
            "  Epoch 1088 @ step 1702000: Train Loss: 0.979833, Train Accuracy: 0.657094\n",
            "Epoch 1088 Test Loss: 1.085415, Test Accuracy: 0.626597, time: 7.5s\n",
            "  Epoch 1089 @ step 1703000: Train Loss: 0.974972, Train Accuracy: 0.657781\n",
            "Epoch 1089 Test Loss: 1.080518, Test Accuracy: 0.622804, time: 7.7s\n",
            "  Epoch 1090 @ step 1704000: Train Loss: 0.980607, Train Accuracy: 0.659281\n",
            "  Epoch 1090 @ step 1705000: Train Loss: 0.986221, Train Accuracy: 0.655344\n",
            "Epoch 1090 Test Loss: 1.006499, Test Accuracy: 0.650260, time: 8.4s\n",
            "  Epoch 1091 @ step 1706000: Train Loss: 0.971970, Train Accuracy: 0.659188\n",
            "Epoch 1091 Test Loss: 1.009900, Test Accuracy: 0.651158, time: 7.6s\n",
            "  Epoch 1092 @ step 1707000: Train Loss: 0.987061, Train Accuracy: 0.654531\n",
            "  Epoch 1092 @ step 1708000: Train Loss: 0.977060, Train Accuracy: 0.658219\n",
            "Epoch 1092 Test Loss: 0.967358, Test Accuracy: 0.669329, time: 7.6s\n",
            "  Epoch 1093 @ step 1709000: Train Loss: 0.972114, Train Accuracy: 0.658000\n",
            "Epoch 1093 Test Loss: 0.984916, Test Accuracy: 0.655950, time: 7.6s\n",
            "  Epoch 1094 @ step 1710000: Train Loss: 0.982956, Train Accuracy: 0.658125\n",
            "  Epoch 1094 @ step 1711000: Train Loss: 0.975526, Train Accuracy: 0.660031\n",
            "Epoch 1094 Test Loss: 1.019295, Test Accuracy: 0.638878, time: 7.6s\n",
            "  Epoch 1095 @ step 1712000: Train Loss: 0.988624, Train Accuracy: 0.652313\n",
            "  Epoch 1095 @ step 1713000: Train Loss: 0.978011, Train Accuracy: 0.657125\n",
            "Epoch 1095 Test Loss: 1.001289, Test Accuracy: 0.653854, time: 8.0s\n",
            "  Epoch 1096 @ step 1714000: Train Loss: 0.969851, Train Accuracy: 0.656688\n",
            "Epoch 1096 Test Loss: 1.012905, Test Accuracy: 0.647264, time: 8.0s\n",
            "  Epoch 1097 @ step 1715000: Train Loss: 0.981069, Train Accuracy: 0.657688\n",
            "  Epoch 1097 @ step 1716000: Train Loss: 0.977475, Train Accuracy: 0.657438\n",
            "Epoch 1097 Test Loss: 0.970523, Test Accuracy: 0.664137, time: 7.6s\n",
            "  Epoch 1098 @ step 1717000: Train Loss: 0.975819, Train Accuracy: 0.658156\n",
            "Epoch 1098 Test Loss: 0.973046, Test Accuracy: 0.661042, time: 7.7s\n",
            "  Epoch 1099 @ step 1718000: Train Loss: 0.978957, Train Accuracy: 0.660063\n",
            "  Epoch 1099 @ step 1719000: Train Loss: 0.980301, Train Accuracy: 0.657219\n",
            "Epoch 1099 Test Loss: 1.031233, Test Accuracy: 0.637879, time: 7.6s\n",
            "  Epoch 1100 @ step 1720000: Train Loss: 0.983088, Train Accuracy: 0.657469\n",
            "Epoch 1100 Test Loss: 1.024690, Test Accuracy: 0.644968, time: 7.6s\n",
            "  Epoch 1101 @ step 1721000: Train Loss: 0.973397, Train Accuracy: 0.657094\n",
            "  Epoch 1101 @ step 1722000: Train Loss: 0.977634, Train Accuracy: 0.657875\n",
            "Epoch 1101 Test Loss: 1.010025, Test Accuracy: 0.643171, time: 8.6s\n",
            "  Epoch 1102 @ step 1723000: Train Loss: 0.980078, Train Accuracy: 0.657125\n",
            "Epoch 1102 Test Loss: 1.016363, Test Accuracy: 0.644169, time: 8.7s\n",
            "  Epoch 1103 @ step 1724000: Train Loss: 0.982455, Train Accuracy: 0.653563\n",
            "  Epoch 1103 @ step 1725000: Train Loss: 0.970747, Train Accuracy: 0.665000\n",
            "Epoch 1103 Test Loss: 0.995324, Test Accuracy: 0.652157, time: 7.6s\n",
            "  Epoch 1104 @ step 1726000: Train Loss: 0.980845, Train Accuracy: 0.656125\n",
            "  Epoch 1104 @ step 1727000: Train Loss: 0.979876, Train Accuracy: 0.657500\n",
            "Epoch 1104 Test Loss: 0.991510, Test Accuracy: 0.650659, time: 7.6s\n",
            "  Epoch 1105 @ step 1728000: Train Loss: 0.981445, Train Accuracy: 0.657156\n",
            "Epoch 1105 Test Loss: 0.969192, Test Accuracy: 0.658846, time: 7.6s\n",
            "  Epoch 1106 @ step 1729000: Train Loss: 0.971646, Train Accuracy: 0.659625\n",
            "  Epoch 1106 @ step 1730000: Train Loss: 0.977970, Train Accuracy: 0.654000\n",
            "Epoch 1106 Test Loss: 1.021396, Test Accuracy: 0.635583, time: 7.6s\n",
            "  Epoch 1107 @ step 1731000: Train Loss: 0.982523, Train Accuracy: 0.655281\n",
            "Epoch 1107 Test Loss: 0.984839, Test Accuracy: 0.658946, time: 8.6s\n",
            "  Epoch 1108 @ step 1732000: Train Loss: 0.979713, Train Accuracy: 0.655344\n",
            "  Epoch 1108 @ step 1733000: Train Loss: 0.977703, Train Accuracy: 0.658344\n",
            "Epoch 1108 Test Loss: 1.060383, Test Accuracy: 0.630990, time: 7.6s\n",
            "  Epoch 1109 @ step 1734000: Train Loss: 0.975371, Train Accuracy: 0.660094\n",
            "Epoch 1109 Test Loss: 1.061606, Test Accuracy: 0.623802, time: 7.5s\n",
            "  Epoch 1110 @ step 1735000: Train Loss: 0.986820, Train Accuracy: 0.653594\n",
            "  Epoch 1110 @ step 1736000: Train Loss: 0.974402, Train Accuracy: 0.656594\n",
            "Epoch 1110 Test Loss: 1.082837, Test Accuracy: 0.620008, time: 7.5s\n",
            "  Epoch 1111 @ step 1737000: Train Loss: 0.976191, Train Accuracy: 0.657344\n",
            "  Epoch 1111 @ step 1738000: Train Loss: 0.985215, Train Accuracy: 0.656406\n",
            "Epoch 1111 Test Loss: 1.002578, Test Accuracy: 0.652656, time: 7.7s\n",
            "  Epoch 1112 @ step 1739000: Train Loss: 0.973177, Train Accuracy: 0.658781\n",
            "Epoch 1112 Test Loss: 1.114778, Test Accuracy: 0.618411, time: 7.6s\n",
            "  Epoch 1113 @ step 1740000: Train Loss: 0.975744, Train Accuracy: 0.660594\n",
            "  Epoch 1113 @ step 1741000: Train Loss: 0.985303, Train Accuracy: 0.655969\n",
            "Epoch 1113 Test Loss: 0.988323, Test Accuracy: 0.658546, time: 8.5s\n",
            "  Epoch 1114 @ step 1742000: Train Loss: 0.979545, Train Accuracy: 0.655406\n",
            "Epoch 1114 Test Loss: 1.178745, Test Accuracy: 0.585763, time: 7.6s\n",
            "  Epoch 1115 @ step 1743000: Train Loss: 0.978656, Train Accuracy: 0.658125\n",
            "  Epoch 1115 @ step 1744000: Train Loss: 0.974834, Train Accuracy: 0.659438\n",
            "Epoch 1115 Test Loss: 1.150653, Test Accuracy: 0.596945, time: 7.6s\n",
            "  Epoch 1116 @ step 1745000: Train Loss: 0.978678, Train Accuracy: 0.656469\n",
            "Epoch 1116 Test Loss: 1.101050, Test Accuracy: 0.607428, time: 7.6s\n",
            "  Epoch 1117 @ step 1746000: Train Loss: 0.982710, Train Accuracy: 0.653656\n",
            "  Epoch 1117 @ step 1747000: Train Loss: 0.976951, Train Accuracy: 0.660906\n",
            "Epoch 1117 Test Loss: 1.042813, Test Accuracy: 0.624700, time: 7.6s\n",
            "  Epoch 1118 @ step 1748000: Train Loss: 0.977049, Train Accuracy: 0.658000\n",
            "Epoch 1118 Test Loss: 1.000694, Test Accuracy: 0.654353, time: 8.0s\n",
            "  Epoch 1119 @ step 1749000: Train Loss: 0.979449, Train Accuracy: 0.657438\n",
            "  Epoch 1119 @ step 1750000: Train Loss: 0.970562, Train Accuracy: 0.661125\n",
            "Epoch 1119 Test Loss: 1.182712, Test Accuracy: 0.592951, time: 8.0s\n",
            "  Epoch 1120 @ step 1751000: Train Loss: 0.976962, Train Accuracy: 0.658250\n",
            "  Epoch 1120 @ step 1752000: Train Loss: 0.984370, Train Accuracy: 0.656156\n",
            "Epoch 1120 Test Loss: 1.052677, Test Accuracy: 0.630192, time: 7.7s\n",
            "  Epoch 1121 @ step 1753000: Train Loss: 0.983330, Train Accuracy: 0.653188\n",
            "Epoch 1121 Test Loss: 1.025412, Test Accuracy: 0.636781, time: 7.5s\n",
            "  Epoch 1122 @ step 1754000: Train Loss: 0.974396, Train Accuracy: 0.659031\n",
            "  Epoch 1122 @ step 1755000: Train Loss: 0.979163, Train Accuracy: 0.657125\n",
            "Epoch 1122 Test Loss: 0.984304, Test Accuracy: 0.660343, time: 7.6s\n",
            "  Epoch 1123 @ step 1756000: Train Loss: 0.973085, Train Accuracy: 0.660438\n",
            "Epoch 1123 Test Loss: 1.123508, Test Accuracy: 0.595547, time: 8.1s\n",
            "  Epoch 1124 @ step 1757000: Train Loss: 0.977810, Train Accuracy: 0.658188\n",
            "  Epoch 1124 @ step 1758000: Train Loss: 0.981827, Train Accuracy: 0.657406\n",
            "Epoch 1124 Test Loss: 1.119957, Test Accuracy: 0.597943, time: 8.4s\n",
            "  Epoch 1125 @ step 1759000: Train Loss: 0.969681, Train Accuracy: 0.660781\n",
            "Epoch 1125 Test Loss: 1.034039, Test Accuracy: 0.639976, time: 8.0s\n",
            "  Epoch 1126 @ step 1760000: Train Loss: 0.985201, Train Accuracy: 0.652313\n",
            "  Epoch 1126 @ step 1761000: Train Loss: 0.976275, Train Accuracy: 0.658313\n",
            "Epoch 1126 Test Loss: 1.029909, Test Accuracy: 0.644768, time: 7.7s\n",
            "  Epoch 1127 @ step 1762000: Train Loss: 0.972917, Train Accuracy: 0.657750\n",
            "  Epoch 1127 @ step 1763000: Train Loss: 0.986336, Train Accuracy: 0.652781\n",
            "Epoch 1127 Test Loss: 1.102158, Test Accuracy: 0.610124, time: 7.7s\n",
            "  Epoch 1128 @ step 1764000: Train Loss: 0.969546, Train Accuracy: 0.662438\n",
            "Epoch 1128 Test Loss: 0.976865, Test Accuracy: 0.659145, time: 7.8s\n",
            "  Epoch 1129 @ step 1765000: Train Loss: 0.980473, Train Accuracy: 0.656281\n",
            "  Epoch 1129 @ step 1766000: Train Loss: 0.987941, Train Accuracy: 0.656219\n",
            "Epoch 1129 Test Loss: 1.069355, Test Accuracy: 0.623003, time: 7.8s\n",
            "  Epoch 1130 @ step 1767000: Train Loss: 0.971993, Train Accuracy: 0.658844\n",
            "Epoch 1130 Test Loss: 1.014956, Test Accuracy: 0.635184, time: 8.6s\n",
            "  Epoch 1131 @ step 1768000: Train Loss: 0.980205, Train Accuracy: 0.654438\n",
            "  Epoch 1131 @ step 1769000: Train Loss: 0.980521, Train Accuracy: 0.653906\n",
            "Epoch 1131 Test Loss: 1.042453, Test Accuracy: 0.641773, time: 7.6s\n",
            "  Epoch 1132 @ step 1770000: Train Loss: 0.976317, Train Accuracy: 0.657219\n",
            "Epoch 1132 Test Loss: 1.031186, Test Accuracy: 0.626098, time: 7.6s\n",
            "  Epoch 1133 @ step 1771000: Train Loss: 0.979652, Train Accuracy: 0.653656\n",
            "  Epoch 1133 @ step 1772000: Train Loss: 0.976942, Train Accuracy: 0.658438\n",
            "Epoch 1133 Test Loss: 1.050617, Test Accuracy: 0.632188, time: 7.6s\n",
            "  Epoch 1134 @ step 1773000: Train Loss: 0.970135, Train Accuracy: 0.661125\n",
            "  Epoch 1134 @ step 1774000: Train Loss: 0.985639, Train Accuracy: 0.656250\n",
            "Epoch 1134 Test Loss: 1.076154, Test Accuracy: 0.615915, time: 7.6s\n",
            "  Epoch 1135 @ step 1775000: Train Loss: 0.978128, Train Accuracy: 0.657719\n",
            "Epoch 1135 Test Loss: 1.056259, Test Accuracy: 0.621805, time: 7.7s\n",
            "  Epoch 1136 @ step 1776000: Train Loss: 0.976717, Train Accuracy: 0.656094\n",
            "  Epoch 1136 @ step 1777000: Train Loss: 0.978910, Train Accuracy: 0.658094\n",
            "Epoch 1136 Test Loss: 1.008846, Test Accuracy: 0.648363, time: 8.4s\n",
            "  Epoch 1137 @ step 1778000: Train Loss: 0.970012, Train Accuracy: 0.660750\n",
            "Epoch 1137 Test Loss: 1.051149, Test Accuracy: 0.620208, time: 7.6s\n",
            "  Epoch 1138 @ step 1779000: Train Loss: 0.979948, Train Accuracy: 0.655875\n",
            "  Epoch 1138 @ step 1780000: Train Loss: 0.982483, Train Accuracy: 0.655031\n",
            "Epoch 1138 Test Loss: 1.197943, Test Accuracy: 0.574181, time: 7.5s\n",
            "  Epoch 1139 @ step 1781000: Train Loss: 0.974660, Train Accuracy: 0.657969\n",
            "Epoch 1139 Test Loss: 1.084575, Test Accuracy: 0.614317, time: 7.5s\n",
            "  Epoch 1140 @ step 1782000: Train Loss: 0.983622, Train Accuracy: 0.657344\n",
            "  Epoch 1140 @ step 1783000: Train Loss: 0.980896, Train Accuracy: 0.658156\n",
            "Epoch 1140 Test Loss: 1.059521, Test Accuracy: 0.626597, time: 7.6s\n",
            "  Epoch 1141 @ step 1784000: Train Loss: 0.969415, Train Accuracy: 0.660375\n",
            "Epoch 1141 Test Loss: 1.067980, Test Accuracy: 0.618211, time: 7.9s\n",
            "  Epoch 1142 @ step 1785000: Train Loss: 0.988909, Train Accuracy: 0.654938\n",
            "  Epoch 1142 @ step 1786000: Train Loss: 0.966841, Train Accuracy: 0.661031\n",
            "Epoch 1142 Test Loss: 1.076573, Test Accuracy: 0.604133, time: 8.1s\n",
            "  Epoch 1143 @ step 1787000: Train Loss: 0.981215, Train Accuracy: 0.653906\n",
            "  Epoch 1143 @ step 1788000: Train Loss: 0.985280, Train Accuracy: 0.656750\n",
            "Epoch 1143 Test Loss: 1.074544, Test Accuracy: 0.622304, time: 7.4s\n",
            "  Epoch 1144 @ step 1789000: Train Loss: 0.974548, Train Accuracy: 0.657469\n",
            "Epoch 1144 Test Loss: 1.123080, Test Accuracy: 0.606330, time: 7.5s\n",
            "  Epoch 1145 @ step 1790000: Train Loss: 0.976109, Train Accuracy: 0.655219\n",
            "  Epoch 1145 @ step 1791000: Train Loss: 0.983605, Train Accuracy: 0.657750\n",
            "Epoch 1145 Test Loss: 1.018362, Test Accuracy: 0.646565, time: 8.0s\n",
            "  Epoch 1146 @ step 1792000: Train Loss: 0.973244, Train Accuracy: 0.660344\n",
            "Epoch 1146 Test Loss: 1.049734, Test Accuracy: 0.628295, time: 7.8s\n",
            "  Epoch 1147 @ step 1793000: Train Loss: 0.976806, Train Accuracy: 0.658031\n",
            "  Epoch 1147 @ step 1794000: Train Loss: 0.981826, Train Accuracy: 0.654969\n",
            "Epoch 1147 Test Loss: 0.994481, Test Accuracy: 0.656350, time: 9.1s\n",
            "  Epoch 1148 @ step 1795000: Train Loss: 0.984153, Train Accuracy: 0.657438\n",
            "Epoch 1148 Test Loss: 0.967579, Test Accuracy: 0.662740, time: 7.5s\n",
            "  Epoch 1149 @ step 1796000: Train Loss: 0.975025, Train Accuracy: 0.658594\n",
            "  Epoch 1149 @ step 1797000: Train Loss: 0.977355, Train Accuracy: 0.657000\n",
            "Epoch 1149 Test Loss: 1.020753, Test Accuracy: 0.642372, time: 7.6s\n",
            "  Epoch 1150 @ step 1798000: Train Loss: 0.973749, Train Accuracy: 0.659594\n",
            "  Epoch 1150 @ step 1799000: Train Loss: 0.983160, Train Accuracy: 0.655063\n",
            "Epoch 1150 Test Loss: 1.056780, Test Accuracy: 0.625200, time: 7.5s\n",
            "  Epoch 1151 @ step 1800000: Train Loss: 0.974293, Train Accuracy: 0.659094\n",
            "Epoch 1151 Test Loss: 1.036116, Test Accuracy: 0.640276, time: 7.5s\n",
            "  Epoch 1152 @ step 1801000: Train Loss: 0.974715, Train Accuracy: 0.658500\n",
            "  Epoch 1152 @ step 1802000: Train Loss: 0.983276, Train Accuracy: 0.655625\n",
            "Epoch 1152 Test Loss: 1.128867, Test Accuracy: 0.600539, time: 7.5s\n",
            "  Epoch 1153 @ step 1803000: Train Loss: 0.975798, Train Accuracy: 0.659281\n",
            "Epoch 1153 Test Loss: 1.072100, Test Accuracy: 0.611921, time: 8.5s\n",
            "  Epoch 1154 @ step 1804000: Train Loss: 0.976114, Train Accuracy: 0.658625\n",
            "  Epoch 1154 @ step 1805000: Train Loss: 0.983002, Train Accuracy: 0.652844\n",
            "Epoch 1154 Test Loss: 1.032006, Test Accuracy: 0.631689, time: 7.5s\n",
            "  Epoch 1155 @ step 1806000: Train Loss: 0.976052, Train Accuracy: 0.657063\n",
            "Epoch 1155 Test Loss: 0.963552, Test Accuracy: 0.666234, time: 7.5s\n",
            "  Epoch 1156 @ step 1807000: Train Loss: 0.986090, Train Accuracy: 0.655906\n",
            "  Epoch 1156 @ step 1808000: Train Loss: 0.976826, Train Accuracy: 0.657031\n",
            "Epoch 1156 Test Loss: 1.008626, Test Accuracy: 0.652157, time: 7.5s\n",
            "  Epoch 1157 @ step 1809000: Train Loss: 0.970704, Train Accuracy: 0.658781\n",
            "Epoch 1157 Test Loss: 1.042495, Test Accuracy: 0.632288, time: 7.5s\n",
            "  Epoch 1158 @ step 1810000: Train Loss: 0.985533, Train Accuracy: 0.655625\n",
            "  Epoch 1158 @ step 1811000: Train Loss: 0.980708, Train Accuracy: 0.654250\n",
            "Epoch 1158 Test Loss: 1.036860, Test Accuracy: 0.628494, time: 7.6s\n",
            "  Epoch 1159 @ step 1812000: Train Loss: 0.978827, Train Accuracy: 0.660250\n",
            "  Epoch 1159 @ step 1813000: Train Loss: 0.983323, Train Accuracy: 0.655563\n",
            "Epoch 1159 Test Loss: 0.985869, Test Accuracy: 0.654153, time: 8.4s\n",
            "  Epoch 1160 @ step 1814000: Train Loss: 0.975769, Train Accuracy: 0.658250\n",
            "Epoch 1160 Test Loss: 0.977007, Test Accuracy: 0.662240, time: 7.5s\n",
            "  Epoch 1161 @ step 1815000: Train Loss: 0.974818, Train Accuracy: 0.658219\n",
            "  Epoch 1161 @ step 1816000: Train Loss: 0.983797, Train Accuracy: 0.654500\n",
            "Epoch 1161 Test Loss: 1.000967, Test Accuracy: 0.646366, time: 7.5s\n",
            "  Epoch 1162 @ step 1817000: Train Loss: 0.977460, Train Accuracy: 0.655531\n",
            "Epoch 1162 Test Loss: 1.057546, Test Accuracy: 0.621306, time: 7.5s\n",
            "  Epoch 1163 @ step 1818000: Train Loss: 0.975490, Train Accuracy: 0.660000\n",
            "  Epoch 1163 @ step 1819000: Train Loss: 0.976769, Train Accuracy: 0.660375\n",
            "Epoch 1163 Test Loss: 1.061803, Test Accuracy: 0.626398, time: 7.5s\n",
            "  Epoch 1164 @ step 1820000: Train Loss: 0.977891, Train Accuracy: 0.656750\n",
            "Epoch 1164 Test Loss: 1.029152, Test Accuracy: 0.628594, time: 7.8s\n",
            "  Epoch 1165 @ step 1821000: Train Loss: 0.982411, Train Accuracy: 0.653531\n",
            "  Epoch 1165 @ step 1822000: Train Loss: 0.979474, Train Accuracy: 0.656656\n",
            "Epoch 1165 Test Loss: 1.017036, Test Accuracy: 0.640475, time: 8.1s\n",
            "  Epoch 1166 @ step 1823000: Train Loss: 0.981775, Train Accuracy: 0.657219\n",
            "  Epoch 1166 @ step 1824000: Train Loss: 0.980358, Train Accuracy: 0.659000\n",
            "Epoch 1166 Test Loss: 1.018509, Test Accuracy: 0.642272, time: 7.7s\n",
            "  Epoch 1167 @ step 1825000: Train Loss: 0.976538, Train Accuracy: 0.658938\n",
            "Epoch 1167 Test Loss: 0.976643, Test Accuracy: 0.664637, time: 7.8s\n",
            "  Epoch 1168 @ step 1826000: Train Loss: 0.979375, Train Accuracy: 0.655094\n",
            "  Epoch 1168 @ step 1827000: Train Loss: 0.979284, Train Accuracy: 0.659156\n",
            "Epoch 1168 Test Loss: 0.988462, Test Accuracy: 0.662041, time: 7.5s\n",
            "  Epoch 1169 @ step 1828000: Train Loss: 0.978063, Train Accuracy: 0.658438\n",
            "Epoch 1169 Test Loss: 0.991420, Test Accuracy: 0.650260, time: 8.7s\n",
            "  Epoch 1170 @ step 1829000: Train Loss: 0.975318, Train Accuracy: 0.655531\n",
            "  Epoch 1170 @ step 1830000: Train Loss: 0.984630, Train Accuracy: 0.656969\n",
            "Epoch 1170 Test Loss: 1.011422, Test Accuracy: 0.644369, time: 8.5s\n",
            "  Epoch 1171 @ step 1831000: Train Loss: 0.976388, Train Accuracy: 0.655938\n",
            "Epoch 1171 Test Loss: 1.163824, Test Accuracy: 0.591354, time: 7.5s\n",
            "  Epoch 1172 @ step 1832000: Train Loss: 0.984372, Train Accuracy: 0.656125\n",
            "  Epoch 1172 @ step 1833000: Train Loss: 0.972279, Train Accuracy: 0.659875\n",
            "Epoch 1172 Test Loss: 1.022901, Test Accuracy: 0.641673, time: 7.5s\n",
            "  Epoch 1173 @ step 1834000: Train Loss: 0.981052, Train Accuracy: 0.655781\n",
            "Epoch 1173 Test Loss: 1.045382, Test Accuracy: 0.621506, time: 7.5s\n",
            "  Epoch 1174 @ step 1835000: Train Loss: 0.983447, Train Accuracy: 0.657719\n",
            "  Epoch 1174 @ step 1836000: Train Loss: 0.977364, Train Accuracy: 0.657781\n",
            "Epoch 1174 Test Loss: 1.008302, Test Accuracy: 0.639876, time: 7.5s\n",
            "  Epoch 1175 @ step 1837000: Train Loss: 0.969398, Train Accuracy: 0.659531\n",
            "  Epoch 1175 @ step 1838000: Train Loss: 0.989281, Train Accuracy: 0.653313\n",
            "Epoch 1175 Test Loss: 0.975694, Test Accuracy: 0.662240, time: 7.6s\n",
            "  Epoch 1176 @ step 1839000: Train Loss: 0.968140, Train Accuracy: 0.660406\n",
            "Epoch 1176 Test Loss: 1.236055, Test Accuracy: 0.563798, time: 8.5s\n",
            "  Epoch 1177 @ step 1840000: Train Loss: 0.983835, Train Accuracy: 0.654938\n",
            "  Epoch 1177 @ step 1841000: Train Loss: 0.985866, Train Accuracy: 0.652344\n",
            "Epoch 1177 Test Loss: 1.030497, Test Accuracy: 0.631889, time: 7.6s\n",
            "  Epoch 1178 @ step 1842000: Train Loss: 0.970296, Train Accuracy: 0.659969\n",
            "Epoch 1178 Test Loss: 1.068262, Test Accuracy: 0.612520, time: 7.5s\n",
            "  Epoch 1179 @ step 1843000: Train Loss: 0.977179, Train Accuracy: 0.656563\n",
            "  Epoch 1179 @ step 1844000: Train Loss: 0.978937, Train Accuracy: 0.659094\n",
            "Epoch 1179 Test Loss: 1.002232, Test Accuracy: 0.643670, time: 7.5s\n",
            "  Epoch 1180 @ step 1845000: Train Loss: 0.978249, Train Accuracy: 0.658313\n",
            "Epoch 1180 Test Loss: 0.962656, Test Accuracy: 0.666034, time: 7.5s\n",
            "  Epoch 1181 @ step 1846000: Train Loss: 0.982315, Train Accuracy: 0.655594\n",
            "  Epoch 1181 @ step 1847000: Train Loss: 0.976982, Train Accuracy: 0.657125\n",
            "Epoch 1181 Test Loss: 0.996584, Test Accuracy: 0.655851, time: 7.6s\n",
            "  Epoch 1182 @ step 1848000: Train Loss: 0.970606, Train Accuracy: 0.659281\n",
            "  Epoch 1182 @ step 1849000: Train Loss: 0.991582, Train Accuracy: 0.649344\n",
            "Epoch 1182 Test Loss: 1.041375, Test Accuracy: 0.627995, time: 8.5s\n",
            "  Epoch 1183 @ step 1850000: Train Loss: 0.973940, Train Accuracy: 0.660813\n",
            "Epoch 1183 Test Loss: 1.122609, Test Accuracy: 0.599441, time: 7.5s\n",
            "  Epoch 1184 @ step 1851000: Train Loss: 0.981927, Train Accuracy: 0.655531\n",
            "  Epoch 1184 @ step 1852000: Train Loss: 0.978195, Train Accuracy: 0.657625\n",
            "Epoch 1184 Test Loss: 1.083942, Test Accuracy: 0.621406, time: 7.6s\n",
            "  Epoch 1185 @ step 1853000: Train Loss: 0.978081, Train Accuracy: 0.656188\n",
            "Epoch 1185 Test Loss: 1.018275, Test Accuracy: 0.642173, time: 7.5s\n",
            "  Epoch 1186 @ step 1854000: Train Loss: 0.985007, Train Accuracy: 0.656156\n",
            "  Epoch 1186 @ step 1855000: Train Loss: 0.985191, Train Accuracy: 0.655406\n",
            "Epoch 1186 Test Loss: 0.982983, Test Accuracy: 0.657548, time: 7.5s\n",
            "  Epoch 1187 @ step 1856000: Train Loss: 0.972445, Train Accuracy: 0.661813\n",
            "Epoch 1187 Test Loss: 1.145704, Test Accuracy: 0.593750, time: 7.7s\n",
            "  Epoch 1188 @ step 1857000: Train Loss: 0.982914, Train Accuracy: 0.655125\n",
            "  Epoch 1188 @ step 1858000: Train Loss: 0.977321, Train Accuracy: 0.656875\n",
            "Epoch 1188 Test Loss: 0.977246, Test Accuracy: 0.660843, time: 8.7s\n",
            "  Epoch 1189 @ step 1859000: Train Loss: 0.974083, Train Accuracy: 0.657438\n",
            "Epoch 1189 Test Loss: 1.036640, Test Accuracy: 0.641573, time: 7.5s\n",
            "  Epoch 1190 @ step 1860000: Train Loss: 0.986924, Train Accuracy: 0.653438\n",
            "  Epoch 1190 @ step 1861000: Train Loss: 0.978564, Train Accuracy: 0.659906\n",
            "Epoch 1190 Test Loss: 0.990658, Test Accuracy: 0.650659, time: 7.5s\n",
            "  Epoch 1191 @ step 1862000: Train Loss: 0.971376, Train Accuracy: 0.663344\n",
            "  Epoch 1191 @ step 1863000: Train Loss: 0.977666, Train Accuracy: 0.656531\n",
            "Epoch 1191 Test Loss: 1.071735, Test Accuracy: 0.619109, time: 8.6s\n",
            "  Epoch 1192 @ step 1864000: Train Loss: 0.979316, Train Accuracy: 0.654469\n",
            "Epoch 1192 Test Loss: 1.050852, Test Accuracy: 0.632488, time: 7.6s\n",
            "  Epoch 1193 @ step 1865000: Train Loss: 0.972957, Train Accuracy: 0.662656\n",
            "  Epoch 1193 @ step 1866000: Train Loss: 0.983121, Train Accuracy: 0.656250\n",
            "Epoch 1193 Test Loss: 0.984222, Test Accuracy: 0.656050, time: 8.4s\n",
            "  Epoch 1194 @ step 1867000: Train Loss: 0.967746, Train Accuracy: 0.660906\n",
            "Epoch 1194 Test Loss: 1.111103, Test Accuracy: 0.606330, time: 7.6s\n",
            "  Epoch 1195 @ step 1868000: Train Loss: 0.989131, Train Accuracy: 0.654125\n",
            "  Epoch 1195 @ step 1869000: Train Loss: 0.981981, Train Accuracy: 0.657188\n",
            "Epoch 1195 Test Loss: 1.006170, Test Accuracy: 0.640675, time: 7.5s\n",
            "  Epoch 1196 @ step 1870000: Train Loss: 0.969502, Train Accuracy: 0.660969\n",
            "Epoch 1196 Test Loss: 1.017911, Test Accuracy: 0.640775, time: 7.5s\n",
            "  Epoch 1197 @ step 1871000: Train Loss: 0.985356, Train Accuracy: 0.653219\n",
            "  Epoch 1197 @ step 1872000: Train Loss: 0.977215, Train Accuracy: 0.657125\n",
            "Epoch 1197 Test Loss: 1.053037, Test Accuracy: 0.629992, time: 7.6s\n",
            "  Epoch 1198 @ step 1873000: Train Loss: 0.974100, Train Accuracy: 0.661719\n",
            "  Epoch 1198 @ step 1874000: Train Loss: 0.984907, Train Accuracy: 0.652656\n",
            "Epoch 1198 Test Loss: 1.046446, Test Accuracy: 0.636881, time: 7.5s\n",
            "  Epoch 1199 @ step 1875000: Train Loss: 0.980086, Train Accuracy: 0.654719\n",
            "Epoch 1199 Test Loss: 0.957485, Test Accuracy: 0.668630, time: 8.5s\n",
            "  Epoch 1200 @ step 1876000: Train Loss: 0.974909, Train Accuracy: 0.657781\n",
            "  Epoch 1200 @ step 1877000: Train Loss: 0.980626, Train Accuracy: 0.656313\n",
            "Epoch 1200 Test Loss: 1.039341, Test Accuracy: 0.624900, time: 7.5s\n",
            "  Epoch 1201 @ step 1878000: Train Loss: 0.972359, Train Accuracy: 0.658563\n",
            "Epoch 1201 Test Loss: 0.991057, Test Accuracy: 0.652756, time: 7.5s\n",
            "  Epoch 1202 @ step 1879000: Train Loss: 0.979112, Train Accuracy: 0.654906\n",
            "  Epoch 1202 @ step 1880000: Train Loss: 0.980911, Train Accuracy: 0.656250\n",
            "Epoch 1202 Test Loss: 1.040597, Test Accuracy: 0.634585, time: 7.5s\n",
            "  Epoch 1203 @ step 1881000: Train Loss: 0.975345, Train Accuracy: 0.662063\n",
            "Epoch 1203 Test Loss: 1.050092, Test Accuracy: 0.626597, time: 7.5s\n",
            "  Epoch 1204 @ step 1882000: Train Loss: 0.977181, Train Accuracy: 0.657406\n",
            "  Epoch 1204 @ step 1883000: Train Loss: 0.982558, Train Accuracy: 0.655594\n",
            "Epoch 1204 Test Loss: 1.046360, Test Accuracy: 0.629093, time: 7.6s\n",
            "  Epoch 1205 @ step 1884000: Train Loss: 0.983061, Train Accuracy: 0.654563\n",
            "Epoch 1205 Test Loss: 0.988816, Test Accuracy: 0.657548, time: 8.6s\n",
            "  Epoch 1206 @ step 1885000: Train Loss: 0.976229, Train Accuracy: 0.658594\n",
            "  Epoch 1206 @ step 1886000: Train Loss: 0.974957, Train Accuracy: 0.657313\n",
            "Epoch 1206 Test Loss: 0.999138, Test Accuracy: 0.653155, time: 7.6s\n",
            "  Epoch 1207 @ step 1887000: Train Loss: 0.974629, Train Accuracy: 0.659281\n",
            "  Epoch 1207 @ step 1888000: Train Loss: 0.981248, Train Accuracy: 0.655469\n",
            "Epoch 1207 Test Loss: 0.997950, Test Accuracy: 0.654353, time: 7.5s\n",
            "  Epoch 1208 @ step 1889000: Train Loss: 0.978265, Train Accuracy: 0.657594\n",
            "Epoch 1208 Test Loss: 0.997517, Test Accuracy: 0.649361, time: 7.4s\n",
            "  Epoch 1209 @ step 1890000: Train Loss: 0.971481, Train Accuracy: 0.661906\n",
            "  Epoch 1209 @ step 1891000: Train Loss: 0.981426, Train Accuracy: 0.657906\n",
            "Epoch 1209 Test Loss: 1.019847, Test Accuracy: 0.637280, time: 7.5s\n",
            "  Epoch 1210 @ step 1892000: Train Loss: 0.973039, Train Accuracy: 0.661531\n",
            "Epoch 1210 Test Loss: 1.114367, Test Accuracy: 0.608127, time: 8.2s\n",
            "  Epoch 1211 @ step 1893000: Train Loss: 0.983981, Train Accuracy: 0.653688\n",
            "  Epoch 1211 @ step 1894000: Train Loss: 0.979264, Train Accuracy: 0.655500\n",
            "Epoch 1211 Test Loss: 1.073664, Test Accuracy: 0.615016, time: 8.2s\n",
            "  Epoch 1212 @ step 1895000: Train Loss: 0.974261, Train Accuracy: 0.662719\n",
            "Epoch 1212 Test Loss: 1.021782, Test Accuracy: 0.636981, time: 7.5s\n",
            "  Epoch 1213 @ step 1896000: Train Loss: 0.982659, Train Accuracy: 0.653969\n",
            "  Epoch 1213 @ step 1897000: Train Loss: 0.974336, Train Accuracy: 0.660969\n",
            "Epoch 1213 Test Loss: 1.141525, Test Accuracy: 0.603235, time: 8.5s\n",
            "  Epoch 1214 @ step 1898000: Train Loss: 0.980019, Train Accuracy: 0.659281\n",
            "  Epoch 1214 @ step 1899000: Train Loss: 0.982018, Train Accuracy: 0.656688\n",
            "Epoch 1214 Test Loss: 0.969416, Test Accuracy: 0.662839, time: 7.6s\n",
            "  Epoch 1215 @ step 1900000: Train Loss: 0.975729, Train Accuracy: 0.659250\n",
            "Epoch 1215 Test Loss: 1.021452, Test Accuracy: 0.649161, time: 7.5s\n",
            "  Epoch 1216 @ step 1901000: Train Loss: 0.972703, Train Accuracy: 0.656938\n",
            "  Epoch 1216 @ step 1902000: Train Loss: 0.985577, Train Accuracy: 0.654438\n",
            "Epoch 1216 Test Loss: 1.157349, Test Accuracy: 0.601737, time: 8.3s\n",
            "  Epoch 1217 @ step 1903000: Train Loss: 0.975822, Train Accuracy: 0.658250\n",
            "Epoch 1217 Test Loss: 0.991133, Test Accuracy: 0.658047, time: 7.5s\n",
            "  Epoch 1218 @ step 1904000: Train Loss: 0.978425, Train Accuracy: 0.657438\n",
            "  Epoch 1218 @ step 1905000: Train Loss: 0.978473, Train Accuracy: 0.657625\n",
            "Epoch 1218 Test Loss: 1.035253, Test Accuracy: 0.648363, time: 7.5s\n",
            "  Epoch 1219 @ step 1906000: Train Loss: 0.974101, Train Accuracy: 0.658906\n",
            "Epoch 1219 Test Loss: 1.036246, Test Accuracy: 0.632288, time: 7.5s\n",
            "  Epoch 1220 @ step 1907000: Train Loss: 0.979724, Train Accuracy: 0.656344\n",
            "  Epoch 1220 @ step 1908000: Train Loss: 0.981429, Train Accuracy: 0.657344\n",
            "Epoch 1220 Test Loss: 0.998740, Test Accuracy: 0.650659, time: 7.4s\n",
            "  Epoch 1221 @ step 1909000: Train Loss: 0.981822, Train Accuracy: 0.657469\n",
            "Epoch 1221 Test Loss: 1.047847, Test Accuracy: 0.626697, time: 7.4s\n",
            "  Epoch 1222 @ step 1910000: Train Loss: 0.977498, Train Accuracy: 0.657875\n",
            "  Epoch 1222 @ step 1911000: Train Loss: 0.971946, Train Accuracy: 0.659563\n",
            "Epoch 1222 Test Loss: 1.010764, Test Accuracy: 0.639077, time: 8.4s\n",
            "  Epoch 1223 @ step 1912000: Train Loss: 0.989078, Train Accuracy: 0.653594\n",
            "  Epoch 1223 @ step 1913000: Train Loss: 0.977405, Train Accuracy: 0.656313\n",
            "Epoch 1223 Test Loss: 1.111438, Test Accuracy: 0.594050, time: 7.4s\n",
            "  Epoch 1224 @ step 1914000: Train Loss: 0.973325, Train Accuracy: 0.659469\n",
            "Epoch 1224 Test Loss: 1.014113, Test Accuracy: 0.650859, time: 7.4s\n",
            "  Epoch 1225 @ step 1915000: Train Loss: 0.979659, Train Accuracy: 0.657844\n",
            "  Epoch 1225 @ step 1916000: Train Loss: 0.981424, Train Accuracy: 0.654375\n",
            "Epoch 1225 Test Loss: 1.080574, Test Accuracy: 0.617312, time: 7.5s\n",
            "  Epoch 1226 @ step 1917000: Train Loss: 0.982565, Train Accuracy: 0.652906\n",
            "Epoch 1226 Test Loss: 1.010207, Test Accuracy: 0.644768, time: 7.5s\n",
            "  Epoch 1227 @ step 1918000: Train Loss: 0.975945, Train Accuracy: 0.658531\n",
            "  Epoch 1227 @ step 1919000: Train Loss: 0.981418, Train Accuracy: 0.657406\n",
            "Epoch 1227 Test Loss: 1.007343, Test Accuracy: 0.647863, time: 7.4s\n",
            "  Epoch 1228 @ step 1920000: Train Loss: 0.968858, Train Accuracy: 0.663875\n",
            "Epoch 1228 Test Loss: 1.073060, Test Accuracy: 0.617911, time: 8.5s\n",
            "  Epoch 1229 @ step 1921000: Train Loss: 0.990217, Train Accuracy: 0.653250\n",
            "  Epoch 1229 @ step 1922000: Train Loss: 0.971131, Train Accuracy: 0.659938\n",
            "Epoch 1229 Test Loss: 1.023218, Test Accuracy: 0.643470, time: 7.4s\n",
            "  Epoch 1230 @ step 1923000: Train Loss: 0.986297, Train Accuracy: 0.652781\n",
            "  Epoch 1230 @ step 1924000: Train Loss: 0.980049, Train Accuracy: 0.656156\n",
            "Epoch 1230 Test Loss: 0.993185, Test Accuracy: 0.657947, time: 7.4s\n",
            "  Epoch 1231 @ step 1925000: Train Loss: 0.974364, Train Accuracy: 0.660063\n",
            "Epoch 1231 Test Loss: 1.069426, Test Accuracy: 0.622005, time: 7.5s\n",
            "  Epoch 1232 @ step 1926000: Train Loss: 0.977821, Train Accuracy: 0.656656\n",
            "  Epoch 1232 @ step 1927000: Train Loss: 0.981411, Train Accuracy: 0.658813\n",
            "Epoch 1232 Test Loss: 1.039532, Test Accuracy: 0.641873, time: 8.0s\n",
            "  Epoch 1233 @ step 1928000: Train Loss: 0.974424, Train Accuracy: 0.656094\n",
            "Epoch 1233 Test Loss: 0.983356, Test Accuracy: 0.660743, time: 7.4s\n",
            "  Epoch 1234 @ step 1929000: Train Loss: 0.981115, Train Accuracy: 0.658563\n",
            "  Epoch 1234 @ step 1930000: Train Loss: 0.981283, Train Accuracy: 0.655344\n",
            "Epoch 1234 Test Loss: 1.017713, Test Accuracy: 0.640276, time: 8.5s\n",
            "  Epoch 1235 @ step 1931000: Train Loss: 0.974521, Train Accuracy: 0.658250\n",
            "Epoch 1235 Test Loss: 0.991090, Test Accuracy: 0.651857, time: 7.7s\n",
            "  Epoch 1236 @ step 1932000: Train Loss: 0.984046, Train Accuracy: 0.656813\n",
            "  Epoch 1236 @ step 1933000: Train Loss: 0.978387, Train Accuracy: 0.656563\n",
            "Epoch 1236 Test Loss: 1.046472, Test Accuracy: 0.624700, time: 8.3s\n",
            "  Epoch 1237 @ step 1934000: Train Loss: 0.971542, Train Accuracy: 0.661563\n",
            "Epoch 1237 Test Loss: 1.084844, Test Accuracy: 0.617412, time: 7.4s\n",
            "  Epoch 1238 @ step 1935000: Train Loss: 0.986533, Train Accuracy: 0.653594\n",
            "  Epoch 1238 @ step 1936000: Train Loss: 0.972224, Train Accuracy: 0.661594\n",
            "Epoch 1238 Test Loss: 1.182060, Test Accuracy: 0.591254, time: 7.4s\n",
            "  Epoch 1239 @ step 1937000: Train Loss: 0.979686, Train Accuracy: 0.653906\n",
            "  Epoch 1239 @ step 1938000: Train Loss: 0.985505, Train Accuracy: 0.655594\n",
            "Epoch 1239 Test Loss: 0.986097, Test Accuracy: 0.658446, time: 7.8s\n",
            "  Epoch 1240 @ step 1939000: Train Loss: 0.975655, Train Accuracy: 0.656563\n",
            "Epoch 1240 Test Loss: 1.008621, Test Accuracy: 0.643470, time: 8.0s\n",
            "  Epoch 1241 @ step 1940000: Train Loss: 0.972130, Train Accuracy: 0.659031\n",
            "  Epoch 1241 @ step 1941000: Train Loss: 0.986391, Train Accuracy: 0.656594\n",
            "Epoch 1241 Test Loss: 1.010751, Test Accuracy: 0.642372, time: 7.4s\n",
            "  Epoch 1242 @ step 1942000: Train Loss: 0.980981, Train Accuracy: 0.655719\n",
            "Epoch 1242 Test Loss: 1.130646, Test Accuracy: 0.598942, time: 7.5s\n",
            "  Epoch 1243 @ step 1943000: Train Loss: 0.973925, Train Accuracy: 0.658625\n",
            "  Epoch 1243 @ step 1944000: Train Loss: 0.974402, Train Accuracy: 0.658281\n",
            "Epoch 1243 Test Loss: 1.018205, Test Accuracy: 0.640875, time: 7.5s\n",
            "  Epoch 1244 @ step 1945000: Train Loss: 0.978296, Train Accuracy: 0.654750\n",
            "Epoch 1244 Test Loss: 1.114704, Test Accuracy: 0.599641, time: 7.5s\n",
            "  Epoch 1245 @ step 1946000: Train Loss: 0.984546, Train Accuracy: 0.654469\n",
            "  Epoch 1245 @ step 1947000: Train Loss: 0.971698, Train Accuracy: 0.661750\n",
            "Epoch 1245 Test Loss: 1.170580, Test Accuracy: 0.578474, time: 8.1s\n",
            "  Epoch 1246 @ step 1948000: Train Loss: 0.972667, Train Accuracy: 0.658469\n",
            "  Epoch 1246 @ step 1949000: Train Loss: 0.986274, Train Accuracy: 0.654656\n",
            "Epoch 1246 Test Loss: 1.099047, Test Accuracy: 0.619010, time: 7.8s\n",
            "  Epoch 1247 @ step 1950000: Train Loss: 0.972419, Train Accuracy: 0.657781\n",
            "Epoch 1247 Test Loss: 1.021077, Test Accuracy: 0.637280, time: 7.5s\n",
            "  Epoch 1248 @ step 1951000: Train Loss: 0.979548, Train Accuracy: 0.655969\n",
            "  Epoch 1248 @ step 1952000: Train Loss: 0.980865, Train Accuracy: 0.656594\n",
            "Epoch 1248 Test Loss: 1.098773, Test Accuracy: 0.614816, time: 7.5s\n",
            "  Epoch 1249 @ step 1953000: Train Loss: 0.976607, Train Accuracy: 0.657625\n",
            "Epoch 1249 Test Loss: 0.996847, Test Accuracy: 0.644669, time: 7.4s\n",
            "  Epoch 1250 @ step 1954000: Train Loss: 0.981817, Train Accuracy: 0.659406\n",
            "  Epoch 1250 @ step 1955000: Train Loss: 0.977925, Train Accuracy: 0.656844\n",
            "Epoch 1250 Test Loss: 0.987851, Test Accuracy: 0.647963, time: 7.6s\n",
            "  Epoch 1251 @ step 1956000: Train Loss: 0.974803, Train Accuracy: 0.657281\n",
            "Epoch 1251 Test Loss: 0.992273, Test Accuracy: 0.649161, time: 8.4s\n",
            "  Epoch 1252 @ step 1957000: Train Loss: 0.977930, Train Accuracy: 0.658000\n",
            "  Epoch 1252 @ step 1958000: Train Loss: 0.973115, Train Accuracy: 0.657813\n",
            "Epoch 1252 Test Loss: 1.085060, Test Accuracy: 0.612520, time: 7.6s\n",
            "  Epoch 1253 @ step 1959000: Train Loss: 0.982298, Train Accuracy: 0.659188\n",
            "  Epoch 1253 @ step 1960000: Train Loss: 0.983109, Train Accuracy: 0.654844\n",
            "Epoch 1253 Test Loss: 1.085773, Test Accuracy: 0.623103, time: 7.6s\n",
            "  Epoch 1254 @ step 1961000: Train Loss: 0.976689, Train Accuracy: 0.656844\n",
            "Epoch 1254 Test Loss: 1.118465, Test Accuracy: 0.600839, time: 8.1s\n",
            "  Epoch 1255 @ step 1962000: Train Loss: 0.973861, Train Accuracy: 0.658781\n",
            "  Epoch 1255 @ step 1963000: Train Loss: 0.981782, Train Accuracy: 0.657250\n",
            "Epoch 1255 Test Loss: 1.133319, Test Accuracy: 0.589157, time: 7.5s\n",
            "  Epoch 1256 @ step 1964000: Train Loss: 0.966536, Train Accuracy: 0.661031\n",
            "Epoch 1256 Test Loss: 1.057327, Test Accuracy: 0.630891, time: 7.4s\n",
            "  Epoch 1257 @ step 1965000: Train Loss: 0.985752, Train Accuracy: 0.654125\n",
            "  Epoch 1257 @ step 1966000: Train Loss: 0.981096, Train Accuracy: 0.655156\n",
            "Epoch 1257 Test Loss: 1.287127, Test Accuracy: 0.543031, time: 8.5s\n",
            "  Epoch 1258 @ step 1967000: Train Loss: 0.965260, Train Accuracy: 0.663844\n",
            "Epoch 1258 Test Loss: 1.005333, Test Accuracy: 0.646665, time: 8.6s\n",
            "  Epoch 1259 @ step 1968000: Train Loss: 0.997214, Train Accuracy: 0.649438\n",
            "  Epoch 1259 @ step 1969000: Train Loss: 0.980006, Train Accuracy: 0.657781\n",
            "Epoch 1259 Test Loss: 0.991560, Test Accuracy: 0.652855, time: 7.4s\n",
            "  Epoch 1260 @ step 1970000: Train Loss: 0.975928, Train Accuracy: 0.656563\n",
            "Epoch 1260 Test Loss: 0.995757, Test Accuracy: 0.648862, time: 7.4s\n",
            "  Epoch 1261 @ step 1971000: Train Loss: 0.977281, Train Accuracy: 0.658063\n",
            "  Epoch 1261 @ step 1972000: Train Loss: 0.970397, Train Accuracy: 0.660469\n",
            "Epoch 1261 Test Loss: 1.016577, Test Accuracy: 0.645667, time: 7.4s\n",
            "  Epoch 1262 @ step 1973000: Train Loss: 0.981789, Train Accuracy: 0.656719\n",
            "  Epoch 1262 @ step 1974000: Train Loss: 0.983863, Train Accuracy: 0.654563\n",
            "Epoch 1262 Test Loss: 1.050068, Test Accuracy: 0.630491, time: 7.7s\n",
            "  Epoch 1263 @ step 1975000: Train Loss: 0.970766, Train Accuracy: 0.659688\n",
            "Epoch 1263 Test Loss: 0.988533, Test Accuracy: 0.655252, time: 8.3s\n",
            "  Epoch 1264 @ step 1976000: Train Loss: 0.976363, Train Accuracy: 0.660875\n",
            "  Epoch 1264 @ step 1977000: Train Loss: 0.985802, Train Accuracy: 0.654125\n",
            "Epoch 1264 Test Loss: 1.026637, Test Accuracy: 0.637580, time: 7.4s\n",
            "  Epoch 1265 @ step 1978000: Train Loss: 0.976016, Train Accuracy: 0.660219\n",
            "Epoch 1265 Test Loss: 1.001075, Test Accuracy: 0.654752, time: 7.5s\n",
            "  Epoch 1266 @ step 1979000: Train Loss: 0.976325, Train Accuracy: 0.658531\n",
            "  Epoch 1266 @ step 1980000: Train Loss: 0.982834, Train Accuracy: 0.653594\n",
            "Epoch 1266 Test Loss: 1.054449, Test Accuracy: 0.628994, time: 7.5s\n",
            "  Epoch 1267 @ step 1981000: Train Loss: 0.977618, Train Accuracy: 0.661000\n",
            "Epoch 1267 Test Loss: 1.008045, Test Accuracy: 0.646565, time: 7.4s\n",
            "  Epoch 1268 @ step 1982000: Train Loss: 0.979858, Train Accuracy: 0.655406\n",
            "  Epoch 1268 @ step 1983000: Train Loss: 0.976805, Train Accuracy: 0.655906\n",
            "Epoch 1268 Test Loss: 0.990342, Test Accuracy: 0.658846, time: 7.8s\n",
            "  Epoch 1269 @ step 1984000: Train Loss: 0.977962, Train Accuracy: 0.658969\n",
            "  Epoch 1269 @ step 1985000: Train Loss: 0.983970, Train Accuracy: 0.654438\n",
            "Epoch 1269 Test Loss: 0.963891, Test Accuracy: 0.665835, time: 8.0s\n",
            "  Epoch 1270 @ step 1986000: Train Loss: 0.971169, Train Accuracy: 0.659844\n",
            "Epoch 1270 Test Loss: 0.983044, Test Accuracy: 0.659844, time: 7.4s\n",
            "  Epoch 1271 @ step 1987000: Train Loss: 0.986100, Train Accuracy: 0.654500\n",
            "  Epoch 1271 @ step 1988000: Train Loss: 0.978337, Train Accuracy: 0.658250\n",
            "Epoch 1271 Test Loss: 1.093475, Test Accuracy: 0.624800, time: 7.4s\n",
            "  Epoch 1272 @ step 1989000: Train Loss: 0.965935, Train Accuracy: 0.660250\n",
            "Epoch 1272 Test Loss: 1.001934, Test Accuracy: 0.648263, time: 7.5s\n",
            "  Epoch 1273 @ step 1990000: Train Loss: 0.985805, Train Accuracy: 0.655781\n",
            "  Epoch 1273 @ step 1991000: Train Loss: 0.980091, Train Accuracy: 0.656125\n",
            "Epoch 1273 Test Loss: 1.021281, Test Accuracy: 0.643670, time: 7.4s\n",
            "  Epoch 1274 @ step 1992000: Train Loss: 0.975130, Train Accuracy: 0.657063\n",
            "Epoch 1274 Test Loss: 1.037704, Test Accuracy: 0.627895, time: 8.1s\n",
            "  Epoch 1275 @ step 1993000: Train Loss: 0.986361, Train Accuracy: 0.659344\n",
            "  Epoch 1275 @ step 1994000: Train Loss: 0.977492, Train Accuracy: 0.657094\n",
            "Epoch 1275 Test Loss: 1.214901, Test Accuracy: 0.566793, time: 7.7s\n",
            "  Epoch 1276 @ step 1995000: Train Loss: 0.976182, Train Accuracy: 0.658375\n",
            "Epoch 1276 Test Loss: 1.007012, Test Accuracy: 0.641474, time: 8.0s\n",
            "  Epoch 1277 @ step 1996000: Train Loss: 0.980404, Train Accuracy: 0.655938\n",
            "  Epoch 1277 @ step 1997000: Train Loss: 0.974378, Train Accuracy: 0.657250\n",
            "Epoch 1277 Test Loss: 0.992278, Test Accuracy: 0.653155, time: 7.4s\n",
            "  Epoch 1278 @ step 1998000: Train Loss: 0.978971, Train Accuracy: 0.656531\n",
            "  Epoch 1278 @ step 1999000: Train Loss: 0.981443, Train Accuracy: 0.658969\n",
            "Epoch 1278 Test Loss: 0.971607, Test Accuracy: 0.663139, time: 7.5s\n",
            "  Epoch 1279 @ step 2000000: Train Loss: 0.970597, Train Accuracy: 0.659875\n",
            "Epoch 1279 Test Loss: 1.052565, Test Accuracy: 0.631390, time: 7.5s\n",
            "  Epoch 1280 @ step 2001000: Train Loss: 0.986817, Train Accuracy: 0.655594\n",
            "  Epoch 1280 @ step 2002000: Train Loss: 0.974646, Train Accuracy: 0.657750\n",
            "Epoch 1280 Test Loss: 1.050944, Test Accuracy: 0.631989, time: 8.4s\n",
            "  Epoch 1281 @ step 2003000: Train Loss: 0.980559, Train Accuracy: 0.658000\n",
            "Epoch 1281 Test Loss: 1.028572, Test Accuracy: 0.642472, time: 7.9s\n",
            "  Epoch 1282 @ step 2004000: Train Loss: 0.981166, Train Accuracy: 0.654219\n",
            "  Epoch 1282 @ step 2005000: Train Loss: 0.981343, Train Accuracy: 0.658719\n",
            "Epoch 1282 Test Loss: 1.025684, Test Accuracy: 0.640375, time: 7.4s\n",
            "  Epoch 1283 @ step 2006000: Train Loss: 0.970524, Train Accuracy: 0.656156\n",
            "Epoch 1283 Test Loss: 1.056879, Test Accuracy: 0.632288, time: 7.5s\n",
            "  Epoch 1284 @ step 2007000: Train Loss: 0.982830, Train Accuracy: 0.655031\n",
            "  Epoch 1284 @ step 2008000: Train Loss: 0.979226, Train Accuracy: 0.659563\n",
            "Epoch 1284 Test Loss: 1.060626, Test Accuracy: 0.611022, time: 7.5s\n",
            "  Epoch 1285 @ step 2009000: Train Loss: 0.973041, Train Accuracy: 0.658500\n",
            "  Epoch 1285 @ step 2010000: Train Loss: 0.987034, Train Accuracy: 0.656531\n",
            "Epoch 1285 Test Loss: 0.983933, Test Accuracy: 0.653255, time: 7.5s\n",
            "  Epoch 1286 @ step 2011000: Train Loss: 0.976064, Train Accuracy: 0.660406\n",
            "Epoch 1286 Test Loss: 0.971438, Test Accuracy: 0.661941, time: 8.5s\n",
            "  Epoch 1287 @ step 2012000: Train Loss: 0.976381, Train Accuracy: 0.659156\n",
            "  Epoch 1287 @ step 2013000: Train Loss: 0.981694, Train Accuracy: 0.658875\n",
            "Epoch 1287 Test Loss: 1.018874, Test Accuracy: 0.645767, time: 7.5s\n",
            "  Epoch 1288 @ step 2014000: Train Loss: 0.968350, Train Accuracy: 0.664281\n",
            "Epoch 1288 Test Loss: 0.962405, Test Accuracy: 0.664437, time: 7.4s\n",
            "  Epoch 1289 @ step 2015000: Train Loss: 0.986799, Train Accuracy: 0.652656\n",
            "  Epoch 1289 @ step 2016000: Train Loss: 0.981587, Train Accuracy: 0.657313\n",
            "Epoch 1289 Test Loss: 0.987096, Test Accuracy: 0.657847, time: 7.4s\n",
            "  Epoch 1290 @ step 2017000: Train Loss: 0.969581, Train Accuracy: 0.661844\n",
            "Epoch 1290 Test Loss: 0.987375, Test Accuracy: 0.653055, time: 7.4s\n",
            "  Epoch 1291 @ step 2018000: Train Loss: 0.985805, Train Accuracy: 0.654406\n",
            "  Epoch 1291 @ step 2019000: Train Loss: 0.975660, Train Accuracy: 0.656500\n",
            "Epoch 1291 Test Loss: 1.018791, Test Accuracy: 0.646266, time: 7.5s\n",
            "  Epoch 1292 @ step 2020000: Train Loss: 0.974733, Train Accuracy: 0.656563\n",
            "Epoch 1292 Test Loss: 0.983067, Test Accuracy: 0.655751, time: 8.5s\n",
            "  Epoch 1293 @ step 2021000: Train Loss: 0.989851, Train Accuracy: 0.653156\n",
            "  Epoch 1293 @ step 2022000: Train Loss: 0.974068, Train Accuracy: 0.655719\n",
            "Epoch 1293 Test Loss: 0.984144, Test Accuracy: 0.660343, time: 7.5s\n",
            "  Epoch 1294 @ step 2023000: Train Loss: 0.981004, Train Accuracy: 0.653344\n",
            "  Epoch 1294 @ step 2024000: Train Loss: 0.980834, Train Accuracy: 0.655344\n",
            "Epoch 1294 Test Loss: 0.975407, Test Accuracy: 0.657947, time: 7.5s\n",
            "  Epoch 1295 @ step 2025000: Train Loss: 0.969732, Train Accuracy: 0.660406\n",
            "Epoch 1295 Test Loss: 1.025399, Test Accuracy: 0.634585, time: 7.4s\n",
            "  Epoch 1296 @ step 2026000: Train Loss: 0.983974, Train Accuracy: 0.655094\n",
            "  Epoch 1296 @ step 2027000: Train Loss: 0.981864, Train Accuracy: 0.655938\n",
            "Epoch 1296 Test Loss: 0.993216, Test Accuracy: 0.660543, time: 7.5s\n",
            "  Epoch 1297 @ step 2028000: Train Loss: 0.980016, Train Accuracy: 0.659125\n",
            "Epoch 1297 Test Loss: 1.087784, Test Accuracy: 0.615715, time: 7.5s\n",
            "  Epoch 1298 @ step 2029000: Train Loss: 0.973090, Train Accuracy: 0.660188\n",
            "  Epoch 1298 @ step 2030000: Train Loss: 0.979635, Train Accuracy: 0.658250\n",
            "Epoch 1298 Test Loss: 0.996623, Test Accuracy: 0.651258, time: 8.9s\n",
            "  Epoch 1299 @ step 2031000: Train Loss: 0.980931, Train Accuracy: 0.657594\n",
            "Epoch 1299 Test Loss: 1.038099, Test Accuracy: 0.631789, time: 7.4s\n",
            "  Epoch 1300 @ step 2032000: Train Loss: 0.980559, Train Accuracy: 0.657063\n",
            "  Epoch 1300 @ step 2033000: Train Loss: 0.976836, Train Accuracy: 0.656000\n",
            "Epoch 1300 Test Loss: 0.994318, Test Accuracy: 0.649161, time: 7.5s\n",
            "  Epoch 1301 @ step 2034000: Train Loss: 0.977646, Train Accuracy: 0.659781\n",
            "  Epoch 1301 @ step 2035000: Train Loss: 0.979466, Train Accuracy: 0.654688\n",
            "Epoch 1301 Test Loss: 0.996727, Test Accuracy: 0.652356, time: 7.5s\n",
            "  Epoch 1302 @ step 2036000: Train Loss: 0.978661, Train Accuracy: 0.656500\n",
            "Epoch 1302 Test Loss: 1.000451, Test Accuracy: 0.646965, time: 7.5s\n",
            "  Epoch 1303 @ step 2037000: Train Loss: 0.976239, Train Accuracy: 0.660063\n",
            "  Epoch 1303 @ step 2038000: Train Loss: 0.976045, Train Accuracy: 0.659313\n",
            "Epoch 1303 Test Loss: 0.994651, Test Accuracy: 0.655851, time: 8.9s\n",
            "  Epoch 1304 @ step 2039000: Train Loss: 0.974245, Train Accuracy: 0.660125\n",
            "Epoch 1304 Test Loss: 0.993097, Test Accuracy: 0.651258, time: 8.0s\n",
            "  Epoch 1305 @ step 2040000: Train Loss: 0.980211, Train Accuracy: 0.656781\n",
            "  Epoch 1305 @ step 2041000: Train Loss: 0.971272, Train Accuracy: 0.661969\n",
            "Epoch 1305 Test Loss: 1.012990, Test Accuracy: 0.646366, time: 7.6s\n",
            "  Epoch 1306 @ step 2042000: Train Loss: 0.980337, Train Accuracy: 0.655094\n",
            "Epoch 1306 Test Loss: 1.014151, Test Accuracy: 0.648163, time: 7.5s\n",
            "  Epoch 1307 @ step 2043000: Train Loss: 0.980958, Train Accuracy: 0.656406\n",
            "  Epoch 1307 @ step 2044000: Train Loss: 0.977315, Train Accuracy: 0.658688\n",
            "Epoch 1307 Test Loss: 1.077614, Test Accuracy: 0.621206, time: 7.5s\n",
            "  Epoch 1308 @ step 2045000: Train Loss: 0.977277, Train Accuracy: 0.655906\n",
            "Epoch 1308 Test Loss: 1.002837, Test Accuracy: 0.647664, time: 7.4s\n",
            "  Epoch 1309 @ step 2046000: Train Loss: 0.981012, Train Accuracy: 0.656719\n",
            "  Epoch 1309 @ step 2047000: Train Loss: 0.972734, Train Accuracy: 0.659500\n",
            "Epoch 1309 Test Loss: 1.200986, Test Accuracy: 0.570887, time: 8.1s\n",
            "  Epoch 1310 @ step 2048000: Train Loss: 0.980626, Train Accuracy: 0.655625\n",
            "  Epoch 1310 @ step 2049000: Train Loss: 0.979525, Train Accuracy: 0.657219\n",
            "Epoch 1310 Test Loss: 1.053902, Test Accuracy: 0.619609, time: 7.7s\n",
            "  Epoch 1311 @ step 2050000: Train Loss: 0.974970, Train Accuracy: 0.660219\n",
            "Epoch 1311 Test Loss: 1.016397, Test Accuracy: 0.647065, time: 7.5s\n",
            "  Epoch 1312 @ step 2051000: Train Loss: 0.975021, Train Accuracy: 0.656000\n",
            "  Epoch 1312 @ step 2052000: Train Loss: 0.981932, Train Accuracy: 0.654438\n",
            "Epoch 1312 Test Loss: 1.067266, Test Accuracy: 0.624800, time: 7.4s\n",
            "  Epoch 1313 @ step 2053000: Train Loss: 0.976768, Train Accuracy: 0.656563\n",
            "Epoch 1313 Test Loss: 1.066121, Test Accuracy: 0.630192, time: 7.4s\n",
            "  Epoch 1314 @ step 2054000: Train Loss: 0.975337, Train Accuracy: 0.657781\n",
            "  Epoch 1314 @ step 2055000: Train Loss: 0.981215, Train Accuracy: 0.657094\n",
            "Epoch 1314 Test Loss: 1.048497, Test Accuracy: 0.624501, time: 7.5s\n",
            "  Epoch 1315 @ step 2056000: Train Loss: 0.970431, Train Accuracy: 0.661000\n",
            "Epoch 1315 Test Loss: 1.074372, Test Accuracy: 0.627796, time: 8.2s\n",
            "  Epoch 1316 @ step 2057000: Train Loss: 0.988043, Train Accuracy: 0.655031\n",
            "  Epoch 1316 @ step 2058000: Train Loss: 0.982811, Train Accuracy: 0.654125\n",
            "Epoch 1316 Test Loss: 1.001140, Test Accuracy: 0.651158, time: 7.7s\n",
            "  Epoch 1317 @ step 2059000: Train Loss: 0.974075, Train Accuracy: 0.658469\n",
            "  Epoch 1317 @ step 2060000: Train Loss: 0.981065, Train Accuracy: 0.656281\n",
            "Epoch 1317 Test Loss: 1.024043, Test Accuracy: 0.638379, time: 7.4s\n",
            "  Epoch 1318 @ step 2061000: Train Loss: 0.980341, Train Accuracy: 0.657438\n",
            "Epoch 1318 Test Loss: 1.003741, Test Accuracy: 0.650759, time: 7.5s\n",
            "  Epoch 1319 @ step 2062000: Train Loss: 0.975269, Train Accuracy: 0.657000\n",
            "  Epoch 1319 @ step 2063000: Train Loss: 0.982079, Train Accuracy: 0.656656\n",
            "Epoch 1319 Test Loss: 1.025218, Test Accuracy: 0.632788, time: 7.5s\n",
            "  Epoch 1320 @ step 2064000: Train Loss: 0.975959, Train Accuracy: 0.658656\n",
            "Epoch 1320 Test Loss: 0.988422, Test Accuracy: 0.654253, time: 7.6s\n",
            "  Epoch 1321 @ step 2065000: Train Loss: 0.978408, Train Accuracy: 0.655656\n",
            "  Epoch 1321 @ step 2066000: Train Loss: 0.984605, Train Accuracy: 0.659188\n",
            "Epoch 1321 Test Loss: 1.071932, Test Accuracy: 0.625998, time: 8.9s\n",
            "  Epoch 1322 @ step 2067000: Train Loss: 0.973793, Train Accuracy: 0.659844\n",
            "Epoch 1322 Test Loss: 0.994884, Test Accuracy: 0.646166, time: 7.5s\n",
            "  Epoch 1323 @ step 2068000: Train Loss: 0.976155, Train Accuracy: 0.659000\n",
            "  Epoch 1323 @ step 2069000: Train Loss: 0.981431, Train Accuracy: 0.658531\n",
            "Epoch 1323 Test Loss: 0.993564, Test Accuracy: 0.652456, time: 7.5s\n",
            "  Epoch 1324 @ step 2070000: Train Loss: 0.976459, Train Accuracy: 0.659969\n",
            "Epoch 1324 Test Loss: 1.053143, Test Accuracy: 0.633187, time: 7.5s\n",
            "  Epoch 1325 @ step 2071000: Train Loss: 0.983888, Train Accuracy: 0.656781\n",
            "  Epoch 1325 @ step 2072000: Train Loss: 0.979553, Train Accuracy: 0.656281\n",
            "Epoch 1325 Test Loss: 1.019581, Test Accuracy: 0.640775, time: 8.0s\n",
            "  Epoch 1326 @ step 2073000: Train Loss: 0.972416, Train Accuracy: 0.660938\n",
            "  Epoch 1326 @ step 2074000: Train Loss: 0.979431, Train Accuracy: 0.656375\n",
            "Epoch 1326 Test Loss: 0.972785, Test Accuracy: 0.665435, time: 8.1s\n",
            "  Epoch 1327 @ step 2075000: Train Loss: 0.974743, Train Accuracy: 0.655156\n",
            "Epoch 1327 Test Loss: 1.087588, Test Accuracy: 0.615116, time: 8.5s\n",
            "  Epoch 1328 @ step 2076000: Train Loss: 0.972137, Train Accuracy: 0.662156\n",
            "  Epoch 1328 @ step 2077000: Train Loss: 0.987047, Train Accuracy: 0.653000\n",
            "Epoch 1328 Test Loss: 1.028376, Test Accuracy: 0.644469, time: 7.5s\n",
            "  Epoch 1329 @ step 2078000: Train Loss: 0.976180, Train Accuracy: 0.657813\n",
            "Epoch 1329 Test Loss: 1.022550, Test Accuracy: 0.642772, time: 7.5s\n",
            "  Epoch 1330 @ step 2079000: Train Loss: 0.982126, Train Accuracy: 0.658750\n",
            "  Epoch 1330 @ step 2080000: Train Loss: 0.975906, Train Accuracy: 0.659063\n",
            "Epoch 1330 Test Loss: 1.009760, Test Accuracy: 0.648962, time: 7.4s\n",
            "  Epoch 1331 @ step 2081000: Train Loss: 0.983005, Train Accuracy: 0.655719\n",
            "Epoch 1331 Test Loss: 0.985435, Test Accuracy: 0.659545, time: 7.5s\n",
            "  Epoch 1332 @ step 2082000: Train Loss: 0.977167, Train Accuracy: 0.657625\n",
            "  Epoch 1332 @ step 2083000: Train Loss: 0.971154, Train Accuracy: 0.660781\n",
            "Epoch 1332 Test Loss: 1.109257, Test Accuracy: 0.599441, time: 7.5s\n",
            "  Epoch 1333 @ step 2084000: Train Loss: 0.986777, Train Accuracy: 0.654719\n",
            "  Epoch 1333 @ step 2085000: Train Loss: 0.972709, Train Accuracy: 0.660469\n",
            "Epoch 1333 Test Loss: 1.057074, Test Accuracy: 0.626198, time: 8.4s\n",
            "  Epoch 1334 @ step 2086000: Train Loss: 0.977662, Train Accuracy: 0.655750\n",
            "Epoch 1334 Test Loss: 1.045390, Test Accuracy: 0.628195, time: 7.5s\n",
            "  Epoch 1335 @ step 2087000: Train Loss: 0.977079, Train Accuracy: 0.658125\n",
            "  Epoch 1335 @ step 2088000: Train Loss: 0.980498, Train Accuracy: 0.655844\n",
            "Epoch 1335 Test Loss: 1.125608, Test Accuracy: 0.600439, time: 7.5s\n",
            "  Epoch 1336 @ step 2089000: Train Loss: 0.974863, Train Accuracy: 0.659313\n",
            "Epoch 1336 Test Loss: 0.951563, Test Accuracy: 0.672524, time: 7.5s\n",
            "  Epoch 1337 @ step 2090000: Train Loss: 0.979478, Train Accuracy: 0.655031\n",
            "  Epoch 1337 @ step 2091000: Train Loss: 0.985191, Train Accuracy: 0.654156\n",
            "Epoch 1337 Test Loss: 0.978965, Test Accuracy: 0.660343, time: 7.5s\n",
            "  Epoch 1338 @ step 2092000: Train Loss: 0.974036, Train Accuracy: 0.659156\n",
            "Epoch 1338 Test Loss: 0.987826, Test Accuracy: 0.654253, time: 7.8s\n",
            "  Epoch 1339 @ step 2093000: Train Loss: 0.977619, Train Accuracy: 0.657000\n",
            "  Epoch 1339 @ step 2094000: Train Loss: 0.982695, Train Accuracy: 0.657844\n",
            "Epoch 1339 Test Loss: 1.028659, Test Accuracy: 0.636681, time: 8.1s\n",
            "  Epoch 1340 @ step 2095000: Train Loss: 0.968418, Train Accuracy: 0.659500\n",
            "Epoch 1340 Test Loss: 1.007326, Test Accuracy: 0.651458, time: 7.6s\n",
            "  Epoch 1341 @ step 2096000: Train Loss: 0.989349, Train Accuracy: 0.656000\n",
            "  Epoch 1341 @ step 2097000: Train Loss: 0.967376, Train Accuracy: 0.662563\n",
            "Epoch 1341 Test Loss: 0.995975, Test Accuracy: 0.649661, time: 7.6s\n",
            "  Epoch 1342 @ step 2098000: Train Loss: 0.982615, Train Accuracy: 0.654438\n",
            "  Epoch 1342 @ step 2099000: Train Loss: 0.985781, Train Accuracy: 0.654063\n",
            "Epoch 1342 Test Loss: 1.108171, Test Accuracy: 0.606629, time: 7.6s\n",
            "  Epoch 1343 @ step 2100000: Train Loss: 0.974366, Train Accuracy: 0.657594\n",
            "Epoch 1343 Test Loss: 1.025500, Test Accuracy: 0.644369, time: 8.1s\n",
            "  Epoch 1344 @ step 2101000: Train Loss: 0.977676, Train Accuracy: 0.658844\n",
            "  Epoch 1344 @ step 2102000: Train Loss: 0.981067, Train Accuracy: 0.659969\n",
            "Epoch 1344 Test Loss: 1.160490, Test Accuracy: 0.586861, time: 8.3s\n",
            "  Epoch 1345 @ step 2103000: Train Loss: 0.973301, Train Accuracy: 0.659813\n",
            "Epoch 1345 Test Loss: 1.007944, Test Accuracy: 0.648562, time: 7.9s\n",
            "  Epoch 1346 @ step 2104000: Train Loss: 0.981940, Train Accuracy: 0.657063\n",
            "  Epoch 1346 @ step 2105000: Train Loss: 0.975645, Train Accuracy: 0.657125\n",
            "Epoch 1346 Test Loss: 0.992871, Test Accuracy: 0.649760, time: 7.7s\n",
            "  Epoch 1347 @ step 2106000: Train Loss: 0.973444, Train Accuracy: 0.658125\n",
            "Epoch 1347 Test Loss: 1.046350, Test Accuracy: 0.631689, time: 7.6s\n",
            "  Epoch 1348 @ step 2107000: Train Loss: 0.984643, Train Accuracy: 0.656438\n",
            "  Epoch 1348 @ step 2108000: Train Loss: 0.980939, Train Accuracy: 0.660156\n",
            "Epoch 1348 Test Loss: 1.061363, Test Accuracy: 0.630092, time: 8.7s\n",
            "  Epoch 1349 @ step 2109000: Train Loss: 0.967868, Train Accuracy: 0.658750\n",
            "  Epoch 1349 @ step 2110000: Train Loss: 0.986110, Train Accuracy: 0.651125\n",
            "Epoch 1349 Test Loss: 0.996452, Test Accuracy: 0.647364, time: 7.6s\n",
            "  Epoch 1350 @ step 2111000: Train Loss: 0.980305, Train Accuracy: 0.657344\n",
            "Epoch 1350 Test Loss: 1.010320, Test Accuracy: 0.644868, time: 8.6s\n",
            "  Epoch 1351 @ step 2112000: Train Loss: 0.975356, Train Accuracy: 0.658844\n",
            "  Epoch 1351 @ step 2113000: Train Loss: 0.977805, Train Accuracy: 0.659906\n",
            "Epoch 1351 Test Loss: 0.966497, Test Accuracy: 0.662340, time: 7.6s\n",
            "  Epoch 1352 @ step 2114000: Train Loss: 0.980527, Train Accuracy: 0.654875\n",
            "Epoch 1352 Test Loss: 1.014678, Test Accuracy: 0.638279, time: 7.6s\n",
            "  Epoch 1353 @ step 2115000: Train Loss: 0.979571, Train Accuracy: 0.657563\n",
            "  Epoch 1353 @ step 2116000: Train Loss: 0.984829, Train Accuracy: 0.653000\n",
            "Epoch 1353 Test Loss: 1.048678, Test Accuracy: 0.625699, time: 7.7s\n",
            "  Epoch 1354 @ step 2117000: Train Loss: 0.965311, Train Accuracy: 0.662594\n",
            "Epoch 1354 Test Loss: 1.019786, Test Accuracy: 0.647963, time: 7.6s\n",
            "  Epoch 1355 @ step 2118000: Train Loss: 0.986755, Train Accuracy: 0.657875\n",
            "  Epoch 1355 @ step 2119000: Train Loss: 0.974920, Train Accuracy: 0.657188\n",
            "Epoch 1355 Test Loss: 0.995695, Test Accuracy: 0.650260, time: 7.8s\n",
            "  Epoch 1356 @ step 2120000: Train Loss: 0.982135, Train Accuracy: 0.654313\n",
            "Epoch 1356 Test Loss: 1.037400, Test Accuracy: 0.635383, time: 8.4s\n",
            "  Epoch 1357 @ step 2121000: Train Loss: 0.982660, Train Accuracy: 0.658469\n",
            "  Epoch 1357 @ step 2122000: Train Loss: 0.979730, Train Accuracy: 0.657156\n",
            "Epoch 1357 Test Loss: 1.102092, Test Accuracy: 0.601438, time: 7.6s\n",
            "  Epoch 1358 @ step 2123000: Train Loss: 0.968246, Train Accuracy: 0.661344\n",
            "  Epoch 1358 @ step 2124000: Train Loss: 0.982375, Train Accuracy: 0.652844\n",
            "Epoch 1358 Test Loss: 1.023927, Test Accuracy: 0.636781, time: 7.7s\n",
            "  Epoch 1359 @ step 2125000: Train Loss: 0.973308, Train Accuracy: 0.660813\n",
            "Epoch 1359 Test Loss: 0.982540, Test Accuracy: 0.652057, time: 7.6s\n",
            "  Epoch 1360 @ step 2126000: Train Loss: 0.982434, Train Accuracy: 0.655188\n",
            "  Epoch 1360 @ step 2127000: Train Loss: 0.977643, Train Accuracy: 0.657281\n",
            "Epoch 1360 Test Loss: 1.058815, Test Accuracy: 0.623602, time: 7.5s\n",
            "  Epoch 1361 @ step 2128000: Train Loss: 0.977472, Train Accuracy: 0.655500\n",
            "Epoch 1361 Test Loss: 1.065906, Test Accuracy: 0.614617, time: 7.9s\n",
            "  Epoch 1362 @ step 2129000: Train Loss: 0.980812, Train Accuracy: 0.658188\n",
            "  Epoch 1362 @ step 2130000: Train Loss: 0.982467, Train Accuracy: 0.656719\n",
            "Epoch 1362 Test Loss: 0.997626, Test Accuracy: 0.653954, time: 8.1s\n",
            "  Epoch 1363 @ step 2131000: Train Loss: 0.970241, Train Accuracy: 0.660063\n",
            "Epoch 1363 Test Loss: 0.993527, Test Accuracy: 0.647065, time: 7.5s\n",
            "  Epoch 1364 @ step 2132000: Train Loss: 0.987278, Train Accuracy: 0.655938\n",
            "  Epoch 1364 @ step 2133000: Train Loss: 0.972009, Train Accuracy: 0.657531\n",
            "Epoch 1364 Test Loss: 1.089726, Test Accuracy: 0.613718, time: 7.9s\n",
            "  Epoch 1365 @ step 2134000: Train Loss: 0.978948, Train Accuracy: 0.655219\n",
            "  Epoch 1365 @ step 2135000: Train Loss: 0.981633, Train Accuracy: 0.655563\n",
            "Epoch 1365 Test Loss: 1.153600, Test Accuracy: 0.593051, time: 7.6s\n",
            "  Epoch 1366 @ step 2136000: Train Loss: 0.980783, Train Accuracy: 0.655594\n",
            "Epoch 1366 Test Loss: 1.071502, Test Accuracy: 0.616813, time: 7.6s\n",
            "  Epoch 1367 @ step 2137000: Train Loss: 0.973603, Train Accuracy: 0.660594\n",
            "  Epoch 1367 @ step 2138000: Train Loss: 0.981296, Train Accuracy: 0.657313\n",
            "Epoch 1367 Test Loss: 1.002373, Test Accuracy: 0.649760, time: 8.2s\n",
            "  Epoch 1368 @ step 2139000: Train Loss: 0.973404, Train Accuracy: 0.662344\n",
            "Epoch 1368 Test Loss: 1.080491, Test Accuracy: 0.623802, time: 7.7s\n",
            "  Epoch 1369 @ step 2140000: Train Loss: 0.981413, Train Accuracy: 0.655875\n",
            "  Epoch 1369 @ step 2141000: Train Loss: 0.984292, Train Accuracy: 0.657125\n",
            "Epoch 1369 Test Loss: 1.055183, Test Accuracy: 0.626098, time: 7.6s\n",
            "  Epoch 1370 @ step 2142000: Train Loss: 0.977472, Train Accuracy: 0.659313\n",
            "Epoch 1370 Test Loss: 1.041314, Test Accuracy: 0.625499, time: 8.7s\n",
            "  Epoch 1371 @ step 2143000: Train Loss: 0.977999, Train Accuracy: 0.658938\n",
            "  Epoch 1371 @ step 2144000: Train Loss: 0.973611, Train Accuracy: 0.660750\n",
            "Epoch 1371 Test Loss: 0.991902, Test Accuracy: 0.648263, time: 7.6s\n",
            "  Epoch 1372 @ step 2145000: Train Loss: 0.973845, Train Accuracy: 0.658438\n",
            "Epoch 1372 Test Loss: 0.996028, Test Accuracy: 0.652656, time: 7.6s\n",
            "  Epoch 1373 @ step 2146000: Train Loss: 0.990042, Train Accuracy: 0.651750\n",
            "  Epoch 1373 @ step 2147000: Train Loss: 0.978056, Train Accuracy: 0.658281\n",
            "Epoch 1373 Test Loss: 1.023997, Test Accuracy: 0.644669, time: 8.6s\n",
            "  Epoch 1374 @ step 2148000: Train Loss: 0.971824, Train Accuracy: 0.657063\n",
            "  Epoch 1374 @ step 2149000: Train Loss: 0.987970, Train Accuracy: 0.654875\n",
            "Epoch 1374 Test Loss: 0.984031, Test Accuracy: 0.657149, time: 7.7s\n",
            "  Epoch 1375 @ step 2150000: Train Loss: 0.971344, Train Accuracy: 0.658125\n",
            "Epoch 1375 Test Loss: 0.970712, Test Accuracy: 0.664337, time: 7.6s\n",
            "  Epoch 1376 @ step 2151000: Train Loss: 0.984327, Train Accuracy: 0.656750\n",
            "  Epoch 1376 @ step 2152000: Train Loss: 0.983451, Train Accuracy: 0.655344\n",
            "Epoch 1376 Test Loss: 0.994517, Test Accuracy: 0.652456, time: 7.6s\n",
            "  Epoch 1377 @ step 2153000: Train Loss: 0.968723, Train Accuracy: 0.659219\n",
            "Epoch 1377 Test Loss: 1.113367, Test Accuracy: 0.624700, time: 7.6s\n",
            "  Epoch 1378 @ step 2154000: Train Loss: 0.983030, Train Accuracy: 0.652563\n",
            "  Epoch 1378 @ step 2155000: Train Loss: 0.980682, Train Accuracy: 0.658406\n",
            "Epoch 1378 Test Loss: 0.974315, Test Accuracy: 0.662240, time: 7.7s\n",
            "  Epoch 1379 @ step 2156000: Train Loss: 0.982762, Train Accuracy: 0.655375\n",
            "Epoch 1379 Test Loss: 1.104592, Test Accuracy: 0.603634, time: 8.5s\n",
            "  Epoch 1380 @ step 2157000: Train Loss: 0.980746, Train Accuracy: 0.658188\n",
            "  Epoch 1380 @ step 2158000: Train Loss: 0.972402, Train Accuracy: 0.659000\n",
            "Epoch 1380 Test Loss: 1.095402, Test Accuracy: 0.619309, time: 7.7s\n",
            "  Epoch 1381 @ step 2159000: Train Loss: 0.980647, Train Accuracy: 0.656656\n",
            "  Epoch 1381 @ step 2160000: Train Loss: 0.981152, Train Accuracy: 0.658188\n",
            "Epoch 1381 Test Loss: 0.989327, Test Accuracy: 0.661741, time: 7.6s\n",
            "  Epoch 1382 @ step 2161000: Train Loss: 0.976425, Train Accuracy: 0.660969\n",
            "Epoch 1382 Test Loss: 1.007163, Test Accuracy: 0.646366, time: 7.6s\n",
            "  Epoch 1383 @ step 2162000: Train Loss: 0.971409, Train Accuracy: 0.658031\n",
            "  Epoch 1383 @ step 2163000: Train Loss: 0.981028, Train Accuracy: 0.660000\n",
            "Epoch 1383 Test Loss: 1.056983, Test Accuracy: 0.626198, time: 7.7s\n",
            "  Epoch 1384 @ step 2164000: Train Loss: 0.978064, Train Accuracy: 0.655500\n",
            "Epoch 1384 Test Loss: 1.100184, Test Accuracy: 0.607927, time: 8.0s\n",
            "  Epoch 1385 @ step 2165000: Train Loss: 0.982799, Train Accuracy: 0.655938\n",
            "  Epoch 1385 @ step 2166000: Train Loss: 0.984624, Train Accuracy: 0.655313\n",
            "Epoch 1385 Test Loss: 1.057080, Test Accuracy: 0.631090, time: 8.1s\n",
            "  Epoch 1386 @ step 2167000: Train Loss: 0.974888, Train Accuracy: 0.657156\n",
            "Epoch 1386 Test Loss: 1.035488, Test Accuracy: 0.638678, time: 8.2s\n",
            "  Epoch 1387 @ step 2168000: Train Loss: 0.983952, Train Accuracy: 0.655813\n",
            "  Epoch 1387 @ step 2169000: Train Loss: 0.976106, Train Accuracy: 0.656219\n",
            "Epoch 1387 Test Loss: 1.063847, Test Accuracy: 0.619010, time: 7.6s\n",
            "  Epoch 1388 @ step 2170000: Train Loss: 0.973815, Train Accuracy: 0.660844\n",
            "  Epoch 1388 @ step 2171000: Train Loss: 0.982447, Train Accuracy: 0.655531\n",
            "Epoch 1388 Test Loss: 1.065854, Test Accuracy: 0.624401, time: 7.6s\n",
            "  Epoch 1389 @ step 2172000: Train Loss: 0.979746, Train Accuracy: 0.657219\n",
            "Epoch 1389 Test Loss: 1.065323, Test Accuracy: 0.621805, time: 7.6s\n",
            "  Epoch 1390 @ step 2173000: Train Loss: 0.968536, Train Accuracy: 0.661438\n",
            "  Epoch 1390 @ step 2174000: Train Loss: 0.986170, Train Accuracy: 0.651250\n",
            "Epoch 1390 Test Loss: 0.994292, Test Accuracy: 0.646965, time: 8.4s\n",
            "  Epoch 1391 @ step 2175000: Train Loss: 0.973192, Train Accuracy: 0.658625\n",
            "Epoch 1391 Test Loss: 1.000432, Test Accuracy: 0.648063, time: 7.8s\n",
            "  Epoch 1392 @ step 2176000: Train Loss: 0.983852, Train Accuracy: 0.656344\n",
            "  Epoch 1392 @ step 2177000: Train Loss: 0.984630, Train Accuracy: 0.655813\n",
            "Epoch 1392 Test Loss: 0.991499, Test Accuracy: 0.650260, time: 8.6s\n",
            "  Epoch 1393 @ step 2178000: Train Loss: 0.966044, Train Accuracy: 0.660906\n",
            "Epoch 1393 Test Loss: 1.011645, Test Accuracy: 0.649860, time: 7.6s\n",
            "  Epoch 1394 @ step 2179000: Train Loss: 0.985541, Train Accuracy: 0.655563\n",
            "  Epoch 1394 @ step 2180000: Train Loss: 0.973415, Train Accuracy: 0.659875\n",
            "Epoch 1394 Test Loss: 1.167382, Test Accuracy: 0.591953, time: 7.6s\n",
            "  Epoch 1395 @ step 2181000: Train Loss: 0.977670, Train Accuracy: 0.657000\n",
            "Epoch 1395 Test Loss: 1.061342, Test Accuracy: 0.628694, time: 7.6s\n",
            "  Epoch 1396 @ step 2182000: Train Loss: 0.977843, Train Accuracy: 0.659031\n",
            "  Epoch 1396 @ step 2183000: Train Loss: 0.977000, Train Accuracy: 0.656594\n",
            "Epoch 1396 Test Loss: 0.991973, Test Accuracy: 0.659245, time: 8.6s\n",
            "  Epoch 1397 @ step 2184000: Train Loss: 0.978038, Train Accuracy: 0.656500\n",
            "  Epoch 1397 @ step 2185000: Train Loss: 0.981581, Train Accuracy: 0.655125\n",
            "Epoch 1397 Test Loss: 1.099499, Test Accuracy: 0.612021, time: 7.6s\n",
            "  Epoch 1398 @ step 2186000: Train Loss: 0.977911, Train Accuracy: 0.656813\n",
            "Epoch 1398 Test Loss: 1.219773, Test Accuracy: 0.568391, time: 7.6s\n",
            "  Epoch 1399 @ step 2187000: Train Loss: 0.976684, Train Accuracy: 0.656813\n",
            "  Epoch 1399 @ step 2188000: Train Loss: 0.981261, Train Accuracy: 0.655500\n",
            "Epoch 1399 Test Loss: 1.074723, Test Accuracy: 0.621106, time: 7.6s\n",
            "  Epoch 1400 @ step 2189000: Train Loss: 0.977915, Train Accuracy: 0.656375\n",
            "Epoch 1400 Test Loss: 1.021369, Test Accuracy: 0.637280, time: 7.6s\n",
            "  Epoch 1401 @ step 2190000: Train Loss: 0.976661, Train Accuracy: 0.655969\n",
            "  Epoch 1401 @ step 2191000: Train Loss: 0.983302, Train Accuracy: 0.656969\n",
            "Epoch 1401 Test Loss: 0.982329, Test Accuracy: 0.658047, time: 7.8s\n",
            "  Epoch 1402 @ step 2192000: Train Loss: 0.969492, Train Accuracy: 0.662344\n",
            "Epoch 1402 Test Loss: 1.057726, Test Accuracy: 0.617113, time: 8.4s\n",
            "  Epoch 1403 @ step 2193000: Train Loss: 0.982514, Train Accuracy: 0.655250\n",
            "  Epoch 1403 @ step 2194000: Train Loss: 0.973426, Train Accuracy: 0.660625\n",
            "Epoch 1403 Test Loss: 1.003750, Test Accuracy: 0.647165, time: 7.5s\n",
            "  Epoch 1404 @ step 2195000: Train Loss: 0.983226, Train Accuracy: 0.654969\n",
            "  Epoch 1404 @ step 2196000: Train Loss: 0.984852, Train Accuracy: 0.655688\n",
            "Epoch 1404 Test Loss: 0.989945, Test Accuracy: 0.656050, time: 7.8s\n",
            "  Epoch 1405 @ step 2197000: Train Loss: 0.974099, Train Accuracy: 0.660188\n",
            "Epoch 1405 Test Loss: 1.048620, Test Accuracy: 0.626398, time: 7.5s\n",
            "  Epoch 1406 @ step 2198000: Train Loss: 0.979148, Train Accuracy: 0.653688\n",
            "  Epoch 1406 @ step 2199000: Train Loss: 0.980542, Train Accuracy: 0.657250\n",
            "Epoch 1406 Test Loss: 1.129382, Test Accuracy: 0.598143, time: 7.7s\n",
            "  Epoch 1407 @ step 2200000: Train Loss: 0.966770, Train Accuracy: 0.660313\n",
            "Epoch 1407 Test Loss: 1.184170, Test Accuracy: 0.580272, time: 8.0s\n",
            "  Epoch 1408 @ step 2201000: Train Loss: 0.986170, Train Accuracy: 0.655656\n",
            "  Epoch 1408 @ step 2202000: Train Loss: 0.982675, Train Accuracy: 0.654969\n",
            "Epoch 1408 Test Loss: 1.038053, Test Accuracy: 0.635483, time: 8.4s\n",
            "  Epoch 1409 @ step 2203000: Train Loss: 0.980620, Train Accuracy: 0.656375\n",
            "Epoch 1409 Test Loss: 1.139633, Test Accuracy: 0.594449, time: 7.6s\n",
            "  Epoch 1410 @ step 2204000: Train Loss: 0.981629, Train Accuracy: 0.658031\n",
            "  Epoch 1410 @ step 2205000: Train Loss: 0.973301, Train Accuracy: 0.656938\n",
            "Epoch 1410 Test Loss: 0.994916, Test Accuracy: 0.659046, time: 7.6s\n",
            "  Epoch 1411 @ step 2206000: Train Loss: 0.985237, Train Accuracy: 0.655438\n",
            "Epoch 1411 Test Loss: 1.006075, Test Accuracy: 0.646366, time: 7.5s\n",
            "  Epoch 1412 @ step 2207000: Train Loss: 0.970272, Train Accuracy: 0.660969\n",
            "  Epoch 1412 @ step 2208000: Train Loss: 0.961424, Train Accuracy: 0.663594\n",
            "Epoch 1412 Test Loss: 0.996142, Test Accuracy: 0.651258, time: 7.5s\n",
            "  Epoch 1413 @ step 2209000: Train Loss: 0.986089, Train Accuracy: 0.653969\n",
            "  Epoch 1413 @ step 2210000: Train Loss: 0.983816, Train Accuracy: 0.656500\n",
            "Epoch 1413 Test Loss: 1.098826, Test Accuracy: 0.610923, time: 8.5s\n",
            "  Epoch 1414 @ step 2211000: Train Loss: 0.971403, Train Accuracy: 0.660688\n",
            "Epoch 1414 Test Loss: 0.999885, Test Accuracy: 0.652556, time: 8.6s\n",
            "  Epoch 1415 @ step 2212000: Train Loss: 0.981602, Train Accuracy: 0.655688\n",
            "  Epoch 1415 @ step 2213000: Train Loss: 0.982021, Train Accuracy: 0.656531\n",
            "Epoch 1415 Test Loss: 0.976205, Test Accuracy: 0.659944, time: 7.6s\n",
            "  Epoch 1416 @ step 2214000: Train Loss: 0.981684, Train Accuracy: 0.659000\n",
            "Epoch 1416 Test Loss: 1.035039, Test Accuracy: 0.646865, time: 7.5s\n",
            "  Epoch 1417 @ step 2215000: Train Loss: 0.973152, Train Accuracy: 0.660625\n",
            "  Epoch 1417 @ step 2216000: Train Loss: 0.981486, Train Accuracy: 0.656156\n",
            "Epoch 1417 Test Loss: 0.991932, Test Accuracy: 0.654054, time: 7.7s\n",
            "  Epoch 1418 @ step 2217000: Train Loss: 0.969805, Train Accuracy: 0.660656\n",
            "Epoch 1418 Test Loss: 1.024765, Test Accuracy: 0.633486, time: 7.6s\n",
            "  Epoch 1419 @ step 2218000: Train Loss: 0.986769, Train Accuracy: 0.654844\n",
            "  Epoch 1419 @ step 2219000: Train Loss: 0.975696, Train Accuracy: 0.657000\n",
            "Epoch 1419 Test Loss: 1.000824, Test Accuracy: 0.649561, time: 8.5s\n",
            "  Epoch 1420 @ step 2220000: Train Loss: 0.978779, Train Accuracy: 0.659219\n",
            "  Epoch 1420 @ step 2221000: Train Loss: 0.984897, Train Accuracy: 0.654063\n",
            "Epoch 1420 Test Loss: 1.009168, Test Accuracy: 0.641973, time: 7.6s\n",
            "  Epoch 1421 @ step 2222000: Train Loss: 0.975330, Train Accuracy: 0.655844\n",
            "Epoch 1421 Test Loss: 1.016406, Test Accuracy: 0.649561, time: 7.7s\n",
            "  Epoch 1422 @ step 2223000: Train Loss: 0.974799, Train Accuracy: 0.657594\n",
            "  Epoch 1422 @ step 2224000: Train Loss: 0.982178, Train Accuracy: 0.654500\n",
            "Epoch 1422 Test Loss: 1.071694, Test Accuracy: 0.621106, time: 7.6s\n",
            "  Epoch 1423 @ step 2225000: Train Loss: 0.971268, Train Accuracy: 0.659688\n",
            "Epoch 1423 Test Loss: 0.965881, Test Accuracy: 0.662540, time: 7.6s\n",
            "  Epoch 1424 @ step 2226000: Train Loss: 0.985158, Train Accuracy: 0.655625\n",
            "  Epoch 1424 @ step 2227000: Train Loss: 0.974949, Train Accuracy: 0.658500\n",
            "Epoch 1424 Test Loss: 1.026686, Test Accuracy: 0.638179, time: 7.8s\n",
            "  Epoch 1425 @ step 2228000: Train Loss: 0.986084, Train Accuracy: 0.656094\n",
            "Epoch 1425 Test Loss: 1.185371, Test Accuracy: 0.575280, time: 8.2s\n",
            "  Epoch 1426 @ step 2229000: Train Loss: 0.971936, Train Accuracy: 0.659969\n",
            "  Epoch 1426 @ step 2230000: Train Loss: 0.977582, Train Accuracy: 0.658000\n",
            "Epoch 1426 Test Loss: 1.051597, Test Accuracy: 0.628195, time: 7.5s\n",
            "  Epoch 1427 @ step 2231000: Train Loss: 0.981930, Train Accuracy: 0.654813\n",
            "Epoch 1427 Test Loss: 1.123517, Test Accuracy: 0.590056, time: 7.5s\n",
            "  Epoch 1428 @ step 2232000: Train Loss: 0.981046, Train Accuracy: 0.657031\n",
            "  Epoch 1428 @ step 2233000: Train Loss: 0.979035, Train Accuracy: 0.657375\n",
            "Epoch 1428 Test Loss: 1.078003, Test Accuracy: 0.615715, time: 7.5s\n",
            "  Epoch 1429 @ step 2234000: Train Loss: 0.972331, Train Accuracy: 0.660469\n",
            "  Epoch 1429 @ step 2235000: Train Loss: 0.987054, Train Accuracy: 0.654000\n",
            "Epoch 1429 Test Loss: 1.110867, Test Accuracy: 0.610623, time: 7.5s\n",
            "  Epoch 1430 @ step 2236000: Train Loss: 0.975641, Train Accuracy: 0.657906\n",
            "Epoch 1430 Test Loss: 1.070497, Test Accuracy: 0.610323, time: 8.8s\n",
            "  Epoch 1431 @ step 2237000: Train Loss: 0.979525, Train Accuracy: 0.656688\n",
            "  Epoch 1431 @ step 2238000: Train Loss: 0.975431, Train Accuracy: 0.657313\n",
            "Epoch 1431 Test Loss: 1.065012, Test Accuracy: 0.617412, time: 7.7s\n",
            "  Epoch 1432 @ step 2239000: Train Loss: 0.979511, Train Accuracy: 0.660500\n",
            "Epoch 1432 Test Loss: 0.992181, Test Accuracy: 0.646865, time: 7.5s\n",
            "  Epoch 1433 @ step 2240000: Train Loss: 0.980187, Train Accuracy: 0.658031\n",
            "  Epoch 1433 @ step 2241000: Train Loss: 0.980768, Train Accuracy: 0.658094\n",
            "Epoch 1433 Test Loss: 0.991641, Test Accuracy: 0.650359, time: 7.4s\n",
            "  Epoch 1434 @ step 2242000: Train Loss: 0.972068, Train Accuracy: 0.658406\n",
            "Epoch 1434 Test Loss: 1.056727, Test Accuracy: 0.627396, time: 7.6s\n",
            "  Epoch 1435 @ step 2243000: Train Loss: 0.983250, Train Accuracy: 0.653063\n",
            "  Epoch 1435 @ step 2244000: Train Loss: 0.974707, Train Accuracy: 0.659281\n",
            "Epoch 1435 Test Loss: 0.974781, Test Accuracy: 0.650659, time: 7.5s\n",
            "  Epoch 1436 @ step 2245000: Train Loss: 0.977568, Train Accuracy: 0.654688\n",
            "  Epoch 1436 @ step 2246000: Train Loss: 0.985380, Train Accuracy: 0.654688\n",
            "Epoch 1436 Test Loss: 1.048506, Test Accuracy: 0.619409, time: 8.5s\n",
            "  Epoch 1437 @ step 2247000: Train Loss: 0.971971, Train Accuracy: 0.662375\n",
            "Epoch 1437 Test Loss: 1.073939, Test Accuracy: 0.621905, time: 7.8s\n",
            "  Epoch 1438 @ step 2248000: Train Loss: 0.982785, Train Accuracy: 0.654531\n",
            "  Epoch 1438 @ step 2249000: Train Loss: 0.978295, Train Accuracy: 0.658469\n",
            "Epoch 1438 Test Loss: 1.020013, Test Accuracy: 0.639577, time: 7.5s\n",
            "  Epoch 1439 @ step 2250000: Train Loss: 0.978889, Train Accuracy: 0.657781\n",
            "Epoch 1439 Test Loss: 1.100682, Test Accuracy: 0.621805, time: 7.5s\n",
            "  Epoch 1440 @ step 2251000: Train Loss: 0.969421, Train Accuracy: 0.659375\n",
            "  Epoch 1440 @ step 2252000: Train Loss: 0.987790, Train Accuracy: 0.651688\n",
            "Epoch 1440 Test Loss: 0.966522, Test Accuracy: 0.657947, time: 7.6s\n",
            "  Epoch 1441 @ step 2253000: Train Loss: 0.973844, Train Accuracy: 0.658531\n",
            "Epoch 1441 Test Loss: 1.016259, Test Accuracy: 0.643271, time: 7.6s\n",
            "  Epoch 1442 @ step 2254000: Train Loss: 0.979126, Train Accuracy: 0.656250\n",
            "  Epoch 1442 @ step 2255000: Train Loss: 0.985448, Train Accuracy: 0.656563\n",
            "Epoch 1442 Test Loss: 0.966279, Test Accuracy: 0.668730, time: 8.5s\n",
            "  Epoch 1443 @ step 2256000: Train Loss: 0.967630, Train Accuracy: 0.659250\n",
            "Epoch 1443 Test Loss: 0.991827, Test Accuracy: 0.655751, time: 7.6s\n",
            "  Epoch 1444 @ step 2257000: Train Loss: 0.984745, Train Accuracy: 0.652844\n",
            "  Epoch 1444 @ step 2258000: Train Loss: 0.975248, Train Accuracy: 0.659781\n",
            "Epoch 1444 Test Loss: 1.038329, Test Accuracy: 0.639677, time: 7.5s\n",
            "  Epoch 1445 @ step 2259000: Train Loss: 0.974877, Train Accuracy: 0.658625\n",
            "  Epoch 1445 @ step 2260000: Train Loss: 0.981294, Train Accuracy: 0.658438\n",
            "Epoch 1445 Test Loss: 1.003281, Test Accuracy: 0.645168, time: 7.5s\n",
            "  Epoch 1446 @ step 2261000: Train Loss: 0.966443, Train Accuracy: 0.661906\n",
            "Epoch 1446 Test Loss: 1.004548, Test Accuracy: 0.650859, time: 7.5s\n",
            "  Epoch 1447 @ step 2262000: Train Loss: 0.988309, Train Accuracy: 0.652938\n",
            "  Epoch 1447 @ step 2263000: Train Loss: 0.983867, Train Accuracy: 0.655406\n",
            "Epoch 1447 Test Loss: 0.983355, Test Accuracy: 0.660443, time: 7.6s\n",
            "  Epoch 1448 @ step 2264000: Train Loss: 0.971741, Train Accuracy: 0.660625\n",
            "Epoch 1448 Test Loss: 0.994032, Test Accuracy: 0.658047, time: 8.4s\n",
            "  Epoch 1449 @ step 2265000: Train Loss: 0.978632, Train Accuracy: 0.655906\n",
            "  Epoch 1449 @ step 2266000: Train Loss: 0.974481, Train Accuracy: 0.659750\n",
            "Epoch 1449 Test Loss: 0.976641, Test Accuracy: 0.657548, time: 7.6s\n",
            "  Epoch 1450 @ step 2267000: Train Loss: 0.978140, Train Accuracy: 0.660000\n",
            "Epoch 1450 Test Loss: 0.976131, Test Accuracy: 0.660443, time: 7.5s\n",
            "  Epoch 1451 @ step 2268000: Train Loss: 0.981788, Train Accuracy: 0.655156\n",
            "  Epoch 1451 @ step 2269000: Train Loss: 0.971030, Train Accuracy: 0.663219\n",
            "Epoch 1451 Test Loss: 0.994927, Test Accuracy: 0.651757, time: 7.5s\n",
            "  Epoch 1452 @ step 2270000: Train Loss: 0.982242, Train Accuracy: 0.656188\n",
            "  Epoch 1452 @ step 2271000: Train Loss: 0.984017, Train Accuracy: 0.655375\n",
            "Epoch 1452 Test Loss: 1.073829, Test Accuracy: 0.620407, time: 8.1s\n",
            "  Epoch 1453 @ step 2272000: Train Loss: 0.973396, Train Accuracy: 0.660344\n",
            "Epoch 1453 Test Loss: 1.099260, Test Accuracy: 0.608726, time: 7.9s\n",
            "  Epoch 1454 @ step 2273000: Train Loss: 0.974787, Train Accuracy: 0.660406\n",
            "  Epoch 1454 @ step 2274000: Train Loss: 0.980406, Train Accuracy: 0.657406\n",
            "Epoch 1454 Test Loss: 1.007939, Test Accuracy: 0.647364, time: 8.1s\n",
            "  Epoch 1455 @ step 2275000: Train Loss: 0.971436, Train Accuracy: 0.659688\n",
            "Epoch 1455 Test Loss: 1.017438, Test Accuracy: 0.639177, time: 7.5s\n",
            "  Epoch 1456 @ step 2276000: Train Loss: 0.979080, Train Accuracy: 0.655750\n",
            "  Epoch 1456 @ step 2277000: Train Loss: 0.990647, Train Accuracy: 0.653656\n",
            "Epoch 1456 Test Loss: 1.388656, Test Accuracy: 0.525859, time: 7.6s\n",
            "  Epoch 1457 @ step 2278000: Train Loss: 0.978325, Train Accuracy: 0.655250\n",
            "Epoch 1457 Test Loss: 1.054472, Test Accuracy: 0.621006, time: 7.5s\n",
            "  Epoch 1458 @ step 2279000: Train Loss: 0.974731, Train Accuracy: 0.661156\n",
            "  Epoch 1458 @ step 2280000: Train Loss: 0.980395, Train Accuracy: 0.656875\n",
            "Epoch 1458 Test Loss: 1.017454, Test Accuracy: 0.642173, time: 7.5s\n",
            "  Epoch 1459 @ step 2281000: Train Loss: 0.973676, Train Accuracy: 0.654750\n",
            "Epoch 1459 Test Loss: 1.057214, Test Accuracy: 0.623303, time: 9.2s\n",
            "  Epoch 1460 @ step 2282000: Train Loss: 0.986156, Train Accuracy: 0.654594\n",
            "  Epoch 1460 @ step 2283000: Train Loss: 0.976347, Train Accuracy: 0.658063\n",
            "Epoch 1460 Test Loss: 1.017153, Test Accuracy: 0.637181, time: 7.6s\n",
            "  Epoch 1461 @ step 2284000: Train Loss: 0.972137, Train Accuracy: 0.659719\n",
            "  Epoch 1461 @ step 2285000: Train Loss: 0.985709, Train Accuracy: 0.656656\n",
            "Epoch 1461 Test Loss: 1.066440, Test Accuracy: 0.615216, time: 7.5s\n",
            "  Epoch 1462 @ step 2286000: Train Loss: 0.974763, Train Accuracy: 0.659125\n",
            "Epoch 1462 Test Loss: 1.063067, Test Accuracy: 0.622105, time: 7.5s\n",
            "  Epoch 1463 @ step 2287000: Train Loss: 0.974490, Train Accuracy: 0.660688\n",
            "  Epoch 1463 @ step 2288000: Train Loss: 0.984105, Train Accuracy: 0.656875\n",
            "Epoch 1463 Test Loss: 1.010015, Test Accuracy: 0.647264, time: 7.4s\n",
            "  Epoch 1464 @ step 2289000: Train Loss: 0.975097, Train Accuracy: 0.659000\n",
            "Epoch 1464 Test Loss: 1.052937, Test Accuracy: 0.616613, time: 7.5s\n",
            "  Epoch 1465 @ step 2290000: Train Loss: 0.975748, Train Accuracy: 0.654969\n",
            "  Epoch 1465 @ step 2291000: Train Loss: 0.980610, Train Accuracy: 0.657875\n",
            "Epoch 1465 Test Loss: 1.117293, Test Accuracy: 0.610723, time: 8.5s\n",
            "  Epoch 1466 @ step 2292000: Train Loss: 0.984894, Train Accuracy: 0.653906\n",
            "Epoch 1466 Test Loss: 1.044915, Test Accuracy: 0.624800, time: 7.5s\n",
            "  Epoch 1467 @ step 2293000: Train Loss: 0.980658, Train Accuracy: 0.659625\n",
            "  Epoch 1467 @ step 2294000: Train Loss: 0.974690, Train Accuracy: 0.656313\n",
            "Epoch 1467 Test Loss: 0.992906, Test Accuracy: 0.659046, time: 7.5s\n",
            "  Epoch 1468 @ step 2295000: Train Loss: 0.981954, Train Accuracy: 0.655063\n",
            "  Epoch 1468 @ step 2296000: Train Loss: 0.980528, Train Accuracy: 0.654125\n",
            "Epoch 1468 Test Loss: 1.014548, Test Accuracy: 0.641973, time: 7.4s\n",
            "  Epoch 1469 @ step 2297000: Train Loss: 0.971235, Train Accuracy: 0.659906\n",
            "Epoch 1469 Test Loss: 1.117160, Test Accuracy: 0.595547, time: 7.5s\n",
            "  Epoch 1470 @ step 2298000: Train Loss: 0.981445, Train Accuracy: 0.656469\n",
            "  Epoch 1470 @ step 2299000: Train Loss: 0.980123, Train Accuracy: 0.654250\n",
            "Epoch 1470 Test Loss: 0.978325, Test Accuracy: 0.655950, time: 7.5s\n",
            "  Epoch 1471 @ step 2300000: Train Loss: 0.968609, Train Accuracy: 0.662844\n",
            "Epoch 1471 Test Loss: 1.086089, Test Accuracy: 0.607228, time: 8.5s\n",
            "  Epoch 1472 @ step 2301000: Train Loss: 0.985289, Train Accuracy: 0.655344\n",
            "  Epoch 1472 @ step 2302000: Train Loss: 0.980018, Train Accuracy: 0.657844\n",
            "Epoch 1472 Test Loss: 1.004958, Test Accuracy: 0.648562, time: 7.5s\n",
            "  Epoch 1473 @ step 2303000: Train Loss: 0.970488, Train Accuracy: 0.661063\n",
            "Epoch 1473 Test Loss: 0.981908, Test Accuracy: 0.656649, time: 7.5s\n",
            "  Epoch 1474 @ step 2304000: Train Loss: 0.983027, Train Accuracy: 0.655781\n",
            "  Epoch 1474 @ step 2305000: Train Loss: 0.976349, Train Accuracy: 0.657625\n",
            "Epoch 1474 Test Loss: 0.981398, Test Accuracy: 0.659545, time: 8.1s\n",
            "  Epoch 1475 @ step 2306000: Train Loss: 0.977476, Train Accuracy: 0.659031\n",
            "Epoch 1475 Test Loss: 1.016103, Test Accuracy: 0.644669, time: 7.5s\n",
            "  Epoch 1476 @ step 2307000: Train Loss: 0.984544, Train Accuracy: 0.654000\n",
            "  Epoch 1476 @ step 2308000: Train Loss: 0.982955, Train Accuracy: 0.654844\n",
            "Epoch 1476 Test Loss: 1.159832, Test Accuracy: 0.586861, time: 7.5s\n",
            "  Epoch 1477 @ step 2309000: Train Loss: 0.975773, Train Accuracy: 0.657969\n",
            "  Epoch 1477 @ step 2310000: Train Loss: 0.977661, Train Accuracy: 0.657063\n",
            "Epoch 1477 Test Loss: 1.027156, Test Accuracy: 0.637081, time: 8.5s\n",
            "  Epoch 1478 @ step 2311000: Train Loss: 0.980028, Train Accuracy: 0.657125\n",
            "Epoch 1478 Test Loss: 0.998903, Test Accuracy: 0.655052, time: 7.5s\n",
            "  Epoch 1479 @ step 2312000: Train Loss: 0.967691, Train Accuracy: 0.660938\n",
            "  Epoch 1479 @ step 2313000: Train Loss: 0.984366, Train Accuracy: 0.656531\n",
            "Epoch 1479 Test Loss: 1.069672, Test Accuracy: 0.620208, time: 7.5s\n",
            "  Epoch 1480 @ step 2314000: Train Loss: 0.980521, Train Accuracy: 0.659844\n",
            "Epoch 1480 Test Loss: 1.022571, Test Accuracy: 0.635084, time: 7.5s\n",
            "  Epoch 1481 @ step 2315000: Train Loss: 0.973894, Train Accuracy: 0.660531\n",
            "  Epoch 1481 @ step 2316000: Train Loss: 0.982698, Train Accuracy: 0.657188\n",
            "Epoch 1481 Test Loss: 0.988386, Test Accuracy: 0.652955, time: 8.3s\n",
            "  Epoch 1482 @ step 2317000: Train Loss: 0.975775, Train Accuracy: 0.658031\n",
            "Epoch 1482 Test Loss: 1.045003, Test Accuracy: 0.638678, time: 8.1s\n",
            "  Epoch 1483 @ step 2318000: Train Loss: 0.976582, Train Accuracy: 0.659406\n",
            "  Epoch 1483 @ step 2319000: Train Loss: 0.973225, Train Accuracy: 0.660438\n",
            "Epoch 1483 Test Loss: 1.029981, Test Accuracy: 0.636182, time: 8.1s\n",
            "  Epoch 1484 @ step 2320000: Train Loss: 0.979665, Train Accuracy: 0.653219\n",
            "  Epoch 1484 @ step 2321000: Train Loss: 0.987278, Train Accuracy: 0.655750\n",
            "Epoch 1484 Test Loss: 0.981961, Test Accuracy: 0.656250, time: 7.4s\n",
            "  Epoch 1485 @ step 2322000: Train Loss: 0.972458, Train Accuracy: 0.658906\n",
            "Epoch 1485 Test Loss: 0.994070, Test Accuracy: 0.651657, time: 7.4s\n",
            "  Epoch 1486 @ step 2323000: Train Loss: 0.985694, Train Accuracy: 0.653594\n",
            "  Epoch 1486 @ step 2324000: Train Loss: 0.978774, Train Accuracy: 0.658281\n",
            "Epoch 1486 Test Loss: 1.014978, Test Accuracy: 0.638778, time: 7.4s\n",
            "  Epoch 1487 @ step 2325000: Train Loss: 0.972893, Train Accuracy: 0.658250\n",
            "Epoch 1487 Test Loss: 1.133122, Test Accuracy: 0.595048, time: 7.4s\n",
            "  Epoch 1488 @ step 2326000: Train Loss: 0.978435, Train Accuracy: 0.659469\n",
            "  Epoch 1488 @ step 2327000: Train Loss: 0.975009, Train Accuracy: 0.658688\n",
            "Epoch 1488 Test Loss: 1.193970, Test Accuracy: 0.578075, time: 8.0s\n",
            "  Epoch 1489 @ step 2328000: Train Loss: 0.981435, Train Accuracy: 0.657719\n",
            "Epoch 1489 Test Loss: 1.038273, Test Accuracy: 0.627396, time: 7.8s\n",
            "  Epoch 1490 @ step 2329000: Train Loss: 0.982280, Train Accuracy: 0.654469\n",
            "  Epoch 1490 @ step 2330000: Train Loss: 0.978360, Train Accuracy: 0.659000\n",
            "Epoch 1490 Test Loss: 0.990448, Test Accuracy: 0.658347, time: 7.4s\n",
            "  Epoch 1491 @ step 2331000: Train Loss: 0.977785, Train Accuracy: 0.659188\n",
            "Epoch 1491 Test Loss: 1.073721, Test Accuracy: 0.615715, time: 7.4s\n",
            "  Epoch 1492 @ step 2332000: Train Loss: 0.981802, Train Accuracy: 0.653719\n",
            "  Epoch 1492 @ step 2333000: Train Loss: 0.978183, Train Accuracy: 0.656969\n",
            "Epoch 1492 Test Loss: 1.024049, Test Accuracy: 0.632987, time: 7.4s\n",
            "  Epoch 1493 @ step 2334000: Train Loss: 0.973674, Train Accuracy: 0.659563\n",
            "  Epoch 1493 @ step 2335000: Train Loss: 0.980420, Train Accuracy: 0.656750\n",
            "Epoch 1493 Test Loss: 1.002837, Test Accuracy: 0.647464, time: 7.4s\n",
            "  Epoch 1494 @ step 2336000: Train Loss: 0.978556, Train Accuracy: 0.657563\n",
            "Epoch 1494 Test Loss: 1.056538, Test Accuracy: 0.624201, time: 8.1s\n",
            "  Epoch 1495 @ step 2337000: Train Loss: 0.978597, Train Accuracy: 0.659906\n",
            "  Epoch 1495 @ step 2338000: Train Loss: 0.971379, Train Accuracy: 0.659281\n",
            "Epoch 1495 Test Loss: 1.011945, Test Accuracy: 0.648562, time: 7.7s\n",
            "  Epoch 1496 @ step 2339000: Train Loss: 0.976558, Train Accuracy: 0.657906\n",
            "Epoch 1496 Test Loss: 0.988159, Test Accuracy: 0.652756, time: 8.0s\n",
            "  Epoch 1497 @ step 2340000: Train Loss: 0.979847, Train Accuracy: 0.655844\n",
            "  Epoch 1497 @ step 2341000: Train Loss: 0.971829, Train Accuracy: 0.656906\n",
            "Epoch 1497 Test Loss: 0.983771, Test Accuracy: 0.652157, time: 7.4s\n",
            "  Epoch 1498 @ step 2342000: Train Loss: 0.974987, Train Accuracy: 0.658438\n",
            "Epoch 1498 Test Loss: 1.037500, Test Accuracy: 0.640375, time: 7.4s\n",
            "  Epoch 1499 @ step 2343000: Train Loss: 0.984465, Train Accuracy: 0.657375\n",
            "  Epoch 1499 @ step 2344000: Train Loss: 0.979786, Train Accuracy: 0.655719\n",
            "Epoch 1499 Test Loss: 1.066789, Test Accuracy: 0.622504, time: 7.5s\n",
            "  Epoch 1500 @ step 2345000: Train Loss: 0.968043, Train Accuracy: 0.660063\n",
            "  Epoch 1500 @ step 2346000: Train Loss: 0.987827, Train Accuracy: 0.652156\n",
            "Epoch 1500 Test Loss: 1.045232, Test Accuracy: 0.625998, time: 8.5s\n",
            "  Epoch 1501 @ step 2347000: Train Loss: 0.969081, Train Accuracy: 0.658156\n",
            "Epoch 1501 Test Loss: 0.980408, Test Accuracy: 0.656550, time: 7.5s\n",
            "  Epoch 1502 @ step 2348000: Train Loss: 0.979801, Train Accuracy: 0.659344\n",
            "  Epoch 1502 @ step 2349000: Train Loss: 0.982043, Train Accuracy: 0.655563\n",
            "Epoch 1502 Test Loss: 1.050682, Test Accuracy: 0.625599, time: 7.5s\n",
            "  Epoch 1503 @ step 2350000: Train Loss: 0.968736, Train Accuracy: 0.661156\n",
            "Epoch 1503 Test Loss: 1.023373, Test Accuracy: 0.646066, time: 7.5s\n",
            "  Epoch 1504 @ step 2351000: Train Loss: 0.981664, Train Accuracy: 0.661000\n",
            "  Epoch 1504 @ step 2352000: Train Loss: 0.983554, Train Accuracy: 0.653875\n",
            "Epoch 1504 Test Loss: 1.050663, Test Accuracy: 0.629293, time: 8.5s\n",
            "  Epoch 1505 @ step 2353000: Train Loss: 0.971596, Train Accuracy: 0.659563\n",
            "Epoch 1505 Test Loss: 1.097534, Test Accuracy: 0.601937, time: 7.6s\n",
            "  Epoch 1506 @ step 2354000: Train Loss: 0.986625, Train Accuracy: 0.654719\n",
            "  Epoch 1506 @ step 2355000: Train Loss: 0.980168, Train Accuracy: 0.656188\n",
            "Epoch 1506 Test Loss: 0.995455, Test Accuracy: 0.647764, time: 8.5s\n",
            "  Epoch 1507 @ step 2356000: Train Loss: 0.970974, Train Accuracy: 0.661156\n",
            "  Epoch 1507 @ step 2357000: Train Loss: 0.989325, Train Accuracy: 0.652844\n",
            "Epoch 1507 Test Loss: 1.083830, Test Accuracy: 0.620407, time: 7.6s\n",
            "  Epoch 1508 @ step 2358000: Train Loss: 0.973713, Train Accuracy: 0.659406\n",
            "Epoch 1508 Test Loss: 1.033162, Test Accuracy: 0.632887, time: 7.7s\n",
            "  Epoch 1509 @ step 2359000: Train Loss: 0.978234, Train Accuracy: 0.656531\n",
            "  Epoch 1509 @ step 2360000: Train Loss: 0.981166, Train Accuracy: 0.653719\n",
            "Epoch 1509 Test Loss: 0.965812, Test Accuracy: 0.660144, time: 7.7s\n",
            "  Epoch 1510 @ step 2361000: Train Loss: 0.969835, Train Accuracy: 0.656969\n",
            "Epoch 1510 Test Loss: 1.023854, Test Accuracy: 0.637680, time: 7.7s\n",
            "  Epoch 1511 @ step 2362000: Train Loss: 0.977513, Train Accuracy: 0.657719\n",
            "  Epoch 1511 @ step 2363000: Train Loss: 0.983486, Train Accuracy: 0.658063\n",
            "Epoch 1511 Test Loss: 1.021093, Test Accuracy: 0.634385, time: 8.0s\n",
            "  Epoch 1512 @ step 2364000: Train Loss: 0.972729, Train Accuracy: 0.658063\n",
            "Epoch 1512 Test Loss: 0.996390, Test Accuracy: 0.656250, time: 8.1s\n",
            "  Epoch 1513 @ step 2365000: Train Loss: 0.978104, Train Accuracy: 0.655344\n",
            "  Epoch 1513 @ step 2366000: Train Loss: 0.981832, Train Accuracy: 0.655938\n",
            "Epoch 1513 Test Loss: 1.142249, Test Accuracy: 0.587061, time: 7.7s\n",
            "  Epoch 1514 @ step 2367000: Train Loss: 0.970456, Train Accuracy: 0.662813\n",
            "Epoch 1514 Test Loss: 1.089256, Test Accuracy: 0.610423, time: 7.6s\n",
            "  Epoch 1515 @ step 2368000: Train Loss: 0.982750, Train Accuracy: 0.656469\n",
            "  Epoch 1515 @ step 2369000: Train Loss: 0.978401, Train Accuracy: 0.653031\n",
            "Epoch 1515 Test Loss: 1.058245, Test Accuracy: 0.629792, time: 7.6s\n",
            "  Epoch 1516 @ step 2370000: Train Loss: 0.979088, Train Accuracy: 0.658344\n",
            "  Epoch 1516 @ step 2371000: Train Loss: 0.979500, Train Accuracy: 0.654719\n",
            "Epoch 1516 Test Loss: 1.006934, Test Accuracy: 0.648662, time: 7.6s\n",
            "  Epoch 1517 @ step 2372000: Train Loss: 0.977800, Train Accuracy: 0.659250\n",
            "Epoch 1517 Test Loss: 0.992254, Test Accuracy: 0.651258, time: 8.3s\n",
            "  Epoch 1518 @ step 2373000: Train Loss: 0.975057, Train Accuracy: 0.657313\n",
            "  Epoch 1518 @ step 2374000: Train Loss: 0.985147, Train Accuracy: 0.656031\n",
            "Epoch 1518 Test Loss: 1.023365, Test Accuracy: 0.632288, time: 8.0s\n",
            "  Epoch 1519 @ step 2375000: Train Loss: 0.974881, Train Accuracy: 0.660219\n",
            "Epoch 1519 Test Loss: 0.968219, Test Accuracy: 0.665136, time: 7.6s\n",
            "  Epoch 1520 @ step 2376000: Train Loss: 0.983445, Train Accuracy: 0.656281\n",
            "  Epoch 1520 @ step 2377000: Train Loss: 0.975742, Train Accuracy: 0.658125\n",
            "Epoch 1520 Test Loss: 0.977096, Test Accuracy: 0.661442, time: 7.7s\n",
            "  Epoch 1521 @ step 2378000: Train Loss: 0.978934, Train Accuracy: 0.658688\n",
            "Epoch 1521 Test Loss: 1.108915, Test Accuracy: 0.614117, time: 7.6s\n",
            "  Epoch 1522 @ step 2379000: Train Loss: 0.983475, Train Accuracy: 0.654281\n",
            "  Epoch 1522 @ step 2380000: Train Loss: 0.972065, Train Accuracy: 0.658594\n",
            "Epoch 1522 Test Loss: 1.139820, Test Accuracy: 0.579872, time: 7.7s\n",
            "  Epoch 1523 @ step 2381000: Train Loss: 0.979591, Train Accuracy: 0.655375\n",
            "  Epoch 1523 @ step 2382000: Train Loss: 0.985243, Train Accuracy: 0.657719\n",
            "Epoch 1523 Test Loss: 0.990611, Test Accuracy: 0.652955, time: 8.6s\n",
            "  Epoch 1524 @ step 2383000: Train Loss: 0.970154, Train Accuracy: 0.658344\n",
            "Epoch 1524 Test Loss: 1.023963, Test Accuracy: 0.637280, time: 7.6s\n",
            "  Epoch 1525 @ step 2384000: Train Loss: 0.982542, Train Accuracy: 0.655625\n",
            "  Epoch 1525 @ step 2385000: Train Loss: 0.979030, Train Accuracy: 0.655813\n",
            "Epoch 1525 Test Loss: 1.084543, Test Accuracy: 0.610923, time: 7.7s\n",
            "  Epoch 1526 @ step 2386000: Train Loss: 0.976305, Train Accuracy: 0.656781\n",
            "Epoch 1526 Test Loss: 1.315321, Test Accuracy: 0.544329, time: 8.6s\n",
            "  Epoch 1527 @ step 2387000: Train Loss: 0.977565, Train Accuracy: 0.658094\n",
            "  Epoch 1527 @ step 2388000: Train Loss: 0.984530, Train Accuracy: 0.654031\n",
            "Epoch 1527 Test Loss: 1.039308, Test Accuracy: 0.630292, time: 7.6s\n",
            "  Epoch 1528 @ step 2389000: Train Loss: 0.976307, Train Accuracy: 0.659656\n",
            "Epoch 1528 Test Loss: 1.045983, Test Accuracy: 0.632887, time: 7.8s\n",
            "  Epoch 1529 @ step 2390000: Train Loss: 0.984523, Train Accuracy: 0.655688\n",
            "  Epoch 1529 @ step 2391000: Train Loss: 0.974422, Train Accuracy: 0.659656\n",
            "Epoch 1529 Test Loss: 1.027984, Test Accuracy: 0.637081, time: 8.3s\n",
            "  Epoch 1530 @ step 2392000: Train Loss: 0.980354, Train Accuracy: 0.654438\n",
            "Epoch 1530 Test Loss: 1.048839, Test Accuracy: 0.634285, time: 7.6s\n",
            "  Epoch 1531 @ step 2393000: Train Loss: 0.978639, Train Accuracy: 0.656750\n",
            "  Epoch 1531 @ step 2394000: Train Loss: 0.986807, Train Accuracy: 0.652344\n",
            "Epoch 1531 Test Loss: 0.991044, Test Accuracy: 0.652556, time: 7.6s\n",
            "  Epoch 1532 @ step 2395000: Train Loss: 0.973650, Train Accuracy: 0.659250\n",
            "  Epoch 1532 @ step 2396000: Train Loss: 0.980867, Train Accuracy: 0.658250\n",
            "Epoch 1532 Test Loss: 0.972162, Test Accuracy: 0.661442, time: 7.5s\n",
            "  Epoch 1533 @ step 2397000: Train Loss: 0.975882, Train Accuracy: 0.661750\n",
            "Epoch 1533 Test Loss: 1.041518, Test Accuracy: 0.624101, time: 7.6s\n",
            "  Epoch 1534 @ step 2398000: Train Loss: 0.977762, Train Accuracy: 0.656844\n",
            "  Epoch 1534 @ step 2399000: Train Loss: 0.981781, Train Accuracy: 0.658469\n",
            "Epoch 1534 Test Loss: 1.053783, Test Accuracy: 0.618710, time: 8.1s\n",
            "  Epoch 1535 @ step 2400000: Train Loss: 0.971984, Train Accuracy: 0.660875\n",
            "Epoch 1535 Test Loss: 0.995506, Test Accuracy: 0.657248, time: 7.9s\n",
            "  Epoch 1536 @ step 2401000: Train Loss: 0.986091, Train Accuracy: 0.651031\n",
            "  Epoch 1536 @ step 2402000: Train Loss: 0.975910, Train Accuracy: 0.659000\n",
            "Epoch 1536 Test Loss: 1.069131, Test Accuracy: 0.628594, time: 7.5s\n",
            "  Epoch 1537 @ step 2403000: Train Loss: 0.973436, Train Accuracy: 0.658094\n",
            "Epoch 1537 Test Loss: 1.009585, Test Accuracy: 0.651258, time: 7.6s\n",
            "  Epoch 1538 @ step 2404000: Train Loss: 0.988928, Train Accuracy: 0.653844\n",
            "  Epoch 1538 @ step 2405000: Train Loss: 0.972250, Train Accuracy: 0.661250\n",
            "Epoch 1538 Test Loss: 1.008333, Test Accuracy: 0.652756, time: 7.6s\n",
            "  Epoch 1539 @ step 2406000: Train Loss: 0.982500, Train Accuracy: 0.657594\n",
            "  Epoch 1539 @ step 2407000: Train Loss: 0.984239, Train Accuracy: 0.655656\n",
            "Epoch 1539 Test Loss: 1.047722, Test Accuracy: 0.628295, time: 8.1s\n",
            "  Epoch 1540 @ step 2408000: Train Loss: 0.979610, Train Accuracy: 0.655406\n",
            "Epoch 1540 Test Loss: 1.070756, Test Accuracy: 0.621106, time: 8.6s\n",
            "  Epoch 1541 @ step 2409000: Train Loss: 0.979232, Train Accuracy: 0.657188\n",
            "  Epoch 1541 @ step 2410000: Train Loss: 0.978119, Train Accuracy: 0.656219\n",
            "Epoch 1541 Test Loss: 1.048895, Test Accuracy: 0.634085, time: 7.6s\n",
            "  Epoch 1542 @ step 2411000: Train Loss: 0.974593, Train Accuracy: 0.658344\n",
            "Epoch 1542 Test Loss: 1.039693, Test Accuracy: 0.634884, time: 7.7s\n",
            "  Epoch 1543 @ step 2412000: Train Loss: 0.981252, Train Accuracy: 0.656813\n",
            "  Epoch 1543 @ step 2413000: Train Loss: 0.980460, Train Accuracy: 0.655969\n",
            "Epoch 1543 Test Loss: 1.039783, Test Accuracy: 0.635383, time: 7.6s\n",
            "  Epoch 1544 @ step 2414000: Train Loss: 0.976193, Train Accuracy: 0.656063\n",
            "Epoch 1544 Test Loss: 1.013048, Test Accuracy: 0.643770, time: 7.6s\n",
            "  Epoch 1545 @ step 2415000: Train Loss: 0.977522, Train Accuracy: 0.656594\n",
            "  Epoch 1545 @ step 2416000: Train Loss: 0.981880, Train Accuracy: 0.654656\n",
            "Epoch 1545 Test Loss: 0.995687, Test Accuracy: 0.656450, time: 7.5s\n",
            "  Epoch 1546 @ step 2417000: Train Loss: 0.983186, Train Accuracy: 0.655375\n",
            "Epoch 1546 Test Loss: 0.981903, Test Accuracy: 0.657847, time: 8.6s\n",
            "  Epoch 1547 @ step 2418000: Train Loss: 0.969860, Train Accuracy: 0.659844\n",
            "  Epoch 1547 @ step 2419000: Train Loss: 0.978023, Train Accuracy: 0.658250\n",
            "Epoch 1547 Test Loss: 1.078726, Test Accuracy: 0.606530, time: 7.7s\n",
            "  Epoch 1548 @ step 2420000: Train Loss: 0.979773, Train Accuracy: 0.656063\n",
            "  Epoch 1548 @ step 2421000: Train Loss: 0.988017, Train Accuracy: 0.655031\n",
            "Epoch 1548 Test Loss: 1.029670, Test Accuracy: 0.637181, time: 8.6s\n",
            "  Epoch 1549 @ step 2422000: Train Loss: 0.969444, Train Accuracy: 0.660375\n",
            "Epoch 1549 Test Loss: 1.029447, Test Accuracy: 0.642971, time: 7.6s\n",
            "  Epoch 1550 @ step 2423000: Train Loss: 0.975788, Train Accuracy: 0.656938\n",
            "  Epoch 1550 @ step 2424000: Train Loss: 0.983550, Train Accuracy: 0.653188\n",
            "Epoch 1550 Test Loss: 1.004861, Test Accuracy: 0.650459, time: 7.6s\n",
            "  Epoch 1551 @ step 2425000: Train Loss: 0.980846, Train Accuracy: 0.655625\n",
            "Epoch 1551 Test Loss: 1.094760, Test Accuracy: 0.615216, time: 8.1s\n",
            "  Epoch 1552 @ step 2426000: Train Loss: 0.976482, Train Accuracy: 0.658406\n",
            "  Epoch 1552 @ step 2427000: Train Loss: 0.977788, Train Accuracy: 0.655625\n",
            "Epoch 1552 Test Loss: 0.988528, Test Accuracy: 0.650958, time: 8.1s\n",
            "  Epoch 1553 @ step 2428000: Train Loss: 0.980198, Train Accuracy: 0.658750\n",
            "Epoch 1553 Test Loss: 1.027943, Test Accuracy: 0.633187, time: 7.6s\n",
            "  Epoch 1554 @ step 2429000: Train Loss: 0.977358, Train Accuracy: 0.655250\n",
            "  Epoch 1554 @ step 2430000: Train Loss: 0.971993, Train Accuracy: 0.660563\n",
            "Epoch 1554 Test Loss: 1.051137, Test Accuracy: 0.632488, time: 7.7s\n",
            "  Epoch 1555 @ step 2431000: Train Loss: 0.983357, Train Accuracy: 0.656938\n",
            "  Epoch 1555 @ step 2432000: Train Loss: 0.982729, Train Accuracy: 0.652531\n",
            "Epoch 1555 Test Loss: 1.115906, Test Accuracy: 0.618510, time: 7.7s\n",
            "  Epoch 1556 @ step 2433000: Train Loss: 0.969362, Train Accuracy: 0.660563\n",
            "Epoch 1556 Test Loss: 0.988156, Test Accuracy: 0.653754, time: 7.7s\n",
            "  Epoch 1557 @ step 2434000: Train Loss: 0.982522, Train Accuracy: 0.656781\n",
            "  Epoch 1557 @ step 2435000: Train Loss: 0.977101, Train Accuracy: 0.659063\n",
            "Epoch 1557 Test Loss: 1.020412, Test Accuracy: 0.642173, time: 8.5s\n",
            "  Epoch 1558 @ step 2436000: Train Loss: 0.981557, Train Accuracy: 0.655750\n",
            "Epoch 1558 Test Loss: 1.096810, Test Accuracy: 0.618910, time: 7.6s\n",
            "  Epoch 1559 @ step 2437000: Train Loss: 0.972232, Train Accuracy: 0.657813\n",
            "  Epoch 1559 @ step 2438000: Train Loss: 0.980651, Train Accuracy: 0.656063\n",
            "Epoch 1559 Test Loss: 1.000691, Test Accuracy: 0.648363, time: 7.6s\n",
            "  Epoch 1560 @ step 2439000: Train Loss: 0.978797, Train Accuracy: 0.657125\n",
            "Epoch 1560 Test Loss: 0.976070, Test Accuracy: 0.655651, time: 7.6s\n",
            "  Epoch 1561 @ step 2440000: Train Loss: 0.982860, Train Accuracy: 0.656281\n",
            "  Epoch 1561 @ step 2441000: Train Loss: 0.973437, Train Accuracy: 0.658750\n",
            "Epoch 1561 Test Loss: 1.092773, Test Accuracy: 0.609125, time: 8.2s\n",
            "  Epoch 1562 @ step 2442000: Train Loss: 0.976692, Train Accuracy: 0.659594\n",
            "Epoch 1562 Test Loss: 1.002100, Test Accuracy: 0.656350, time: 7.6s\n",
            "  Epoch 1563 @ step 2443000: Train Loss: 0.985321, Train Accuracy: 0.656375\n",
            "  Epoch 1563 @ step 2444000: Train Loss: 0.976047, Train Accuracy: 0.658719\n",
            "Epoch 1563 Test Loss: 0.995309, Test Accuracy: 0.647764, time: 8.6s\n",
            "  Epoch 1564 @ step 2445000: Train Loss: 0.980860, Train Accuracy: 0.657750\n",
            "  Epoch 1564 @ step 2446000: Train Loss: 0.979947, Train Accuracy: 0.657438\n",
            "Epoch 1564 Test Loss: 0.951432, Test Accuracy: 0.670527, time: 7.7s\n",
            "  Epoch 1565 @ step 2447000: Train Loss: 0.969017, Train Accuracy: 0.660281\n",
            "Epoch 1565 Test Loss: 0.982457, Test Accuracy: 0.657548, time: 7.7s\n",
            "  Epoch 1566 @ step 2448000: Train Loss: 0.978540, Train Accuracy: 0.658125\n",
            "  Epoch 1566 @ step 2449000: Train Loss: 0.985398, Train Accuracy: 0.656219\n",
            "Epoch 1566 Test Loss: 1.003231, Test Accuracy: 0.647264, time: 7.7s\n",
            "  Epoch 1567 @ step 2450000: Train Loss: 0.974862, Train Accuracy: 0.658406\n",
            "Epoch 1567 Test Loss: 1.115258, Test Accuracy: 0.599042, time: 7.6s\n",
            "  Epoch 1568 @ step 2451000: Train Loss: 0.976124, Train Accuracy: 0.657250\n",
            "  Epoch 1568 @ step 2452000: Train Loss: 0.983025, Train Accuracy: 0.656656\n",
            "Epoch 1568 Test Loss: 1.145031, Test Accuracy: 0.593151, time: 7.8s\n",
            "  Epoch 1569 @ step 2453000: Train Loss: 0.976656, Train Accuracy: 0.658531\n",
            "Epoch 1569 Test Loss: 1.001378, Test Accuracy: 0.646765, time: 8.4s\n",
            "  Epoch 1570 @ step 2454000: Train Loss: 0.980008, Train Accuracy: 0.657688\n",
            "  Epoch 1570 @ step 2455000: Train Loss: 0.977179, Train Accuracy: 0.657563\n",
            "Epoch 1570 Test Loss: 1.170569, Test Accuracy: 0.564397, time: 8.6s\n",
            "  Epoch 1571 @ step 2456000: Train Loss: 0.978407, Train Accuracy: 0.655813\n",
            "  Epoch 1571 @ step 2457000: Train Loss: 0.981994, Train Accuracy: 0.655438\n",
            "Epoch 1571 Test Loss: 0.982331, Test Accuracy: 0.655851, time: 7.7s\n",
            "  Epoch 1572 @ step 2458000: Train Loss: 0.972136, Train Accuracy: 0.658031\n",
            "Epoch 1572 Test Loss: 1.051144, Test Accuracy: 0.625000, time: 7.7s\n",
            "  Epoch 1573 @ step 2459000: Train Loss: 0.979746, Train Accuracy: 0.656719\n",
            "  Epoch 1573 @ step 2460000: Train Loss: 0.979245, Train Accuracy: 0.657375\n",
            "Epoch 1573 Test Loss: 1.094767, Test Accuracy: 0.614117, time: 7.7s\n",
            "  Epoch 1574 @ step 2461000: Train Loss: 0.970498, Train Accuracy: 0.659500\n",
            "Epoch 1574 Test Loss: 1.013219, Test Accuracy: 0.646565, time: 8.4s\n",
            "  Epoch 1575 @ step 2462000: Train Loss: 0.978316, Train Accuracy: 0.658906\n",
            "  Epoch 1575 @ step 2463000: Train Loss: 0.980543, Train Accuracy: 0.656594\n",
            "Epoch 1575 Test Loss: 1.079676, Test Accuracy: 0.617911, time: 7.8s\n",
            "  Epoch 1576 @ step 2464000: Train Loss: 0.976699, Train Accuracy: 0.660000\n",
            "Epoch 1576 Test Loss: 1.041092, Test Accuracy: 0.634285, time: 7.7s\n",
            "  Epoch 1577 @ step 2465000: Train Loss: 0.974116, Train Accuracy: 0.660125\n",
            "  Epoch 1577 @ step 2466000: Train Loss: 0.978581, Train Accuracy: 0.658844\n",
            "Epoch 1577 Test Loss: 0.980532, Test Accuracy: 0.660942, time: 7.7s\n",
            "  Epoch 1578 @ step 2467000: Train Loss: 0.981489, Train Accuracy: 0.655469\n",
            "Epoch 1578 Test Loss: 1.012230, Test Accuracy: 0.644469, time: 7.6s\n",
            "  Epoch 1579 @ step 2468000: Train Loss: 0.980861, Train Accuracy: 0.657594\n",
            "  Epoch 1579 @ step 2469000: Train Loss: 0.979428, Train Accuracy: 0.659594\n",
            "Epoch 1579 Test Loss: 1.029566, Test Accuracy: 0.633786, time: 7.7s\n",
            "  Epoch 1580 @ step 2470000: Train Loss: 0.971861, Train Accuracy: 0.656906\n",
            "  Epoch 1580 @ step 2471000: Train Loss: 0.981307, Train Accuracy: 0.657156\n",
            "Epoch 1580 Test Loss: 0.983711, Test Accuracy: 0.655351, time: 8.6s\n",
            "  Epoch 1581 @ step 2472000: Train Loss: 0.967616, Train Accuracy: 0.657969\n",
            "Epoch 1581 Test Loss: 0.977298, Test Accuracy: 0.662540, time: 7.7s\n",
            "  Epoch 1582 @ step 2473000: Train Loss: 0.980091, Train Accuracy: 0.656188\n",
            "  Epoch 1582 @ step 2474000: Train Loss: 0.985496, Train Accuracy: 0.654031\n",
            "Epoch 1582 Test Loss: 1.048699, Test Accuracy: 0.632288, time: 7.9s\n",
            "  Epoch 1583 @ step 2475000: Train Loss: 0.975013, Train Accuracy: 0.658000\n",
            "Epoch 1583 Test Loss: 0.996780, Test Accuracy: 0.647863, time: 7.8s\n",
            "  Epoch 1584 @ step 2476000: Train Loss: 0.982765, Train Accuracy: 0.654813\n",
            "  Epoch 1584 @ step 2477000: Train Loss: 0.970281, Train Accuracy: 0.660281\n",
            "Epoch 1584 Test Loss: 1.011834, Test Accuracy: 0.647464, time: 7.7s\n",
            "  Epoch 1585 @ step 2478000: Train Loss: 0.986465, Train Accuracy: 0.652750\n",
            "Epoch 1585 Test Loss: 1.144150, Test Accuracy: 0.589058, time: 7.8s\n",
            "  Epoch 1586 @ step 2479000: Train Loss: 0.979150, Train Accuracy: 0.655406\n",
            "  Epoch 1586 @ step 2480000: Train Loss: 0.975990, Train Accuracy: 0.656781\n",
            "Epoch 1586 Test Loss: 0.996639, Test Accuracy: 0.657348, time: 8.4s\n",
            "  Epoch 1587 @ step 2481000: Train Loss: 0.972894, Train Accuracy: 0.657281\n",
            "  Epoch 1587 @ step 2482000: Train Loss: 0.989030, Train Accuracy: 0.653219\n",
            "Epoch 1587 Test Loss: 1.040668, Test Accuracy: 0.635982, time: 7.7s\n",
            "  Epoch 1588 @ step 2483000: Train Loss: 0.978004, Train Accuracy: 0.656250\n",
            "Epoch 1588 Test Loss: 1.063125, Test Accuracy: 0.622604, time: 7.7s\n",
            "  Epoch 1589 @ step 2484000: Train Loss: 0.973114, Train Accuracy: 0.658344\n",
            "  Epoch 1589 @ step 2485000: Train Loss: 0.982266, Train Accuracy: 0.657406\n",
            "Epoch 1589 Test Loss: 1.045663, Test Accuracy: 0.623502, time: 7.7s\n",
            "  Epoch 1590 @ step 2486000: Train Loss: 0.978372, Train Accuracy: 0.657281\n",
            "Epoch 1590 Test Loss: 0.987961, Test Accuracy: 0.654952, time: 7.7s\n",
            "  Epoch 1591 @ step 2487000: Train Loss: 0.984129, Train Accuracy: 0.654844\n",
            "  Epoch 1591 @ step 2488000: Train Loss: 0.979136, Train Accuracy: 0.657000\n",
            "Epoch 1591 Test Loss: 0.995987, Test Accuracy: 0.648662, time: 8.2s\n",
            "  Epoch 1592 @ step 2489000: Train Loss: 0.973123, Train Accuracy: 0.661875\n",
            "Epoch 1592 Test Loss: 1.014587, Test Accuracy: 0.647863, time: 8.6s\n",
            "  Epoch 1593 @ step 2490000: Train Loss: 0.980661, Train Accuracy: 0.655688\n",
            "  Epoch 1593 @ step 2491000: Train Loss: 0.978987, Train Accuracy: 0.657844\n",
            "Epoch 1593 Test Loss: 1.012259, Test Accuracy: 0.644968, time: 7.7s\n",
            "  Epoch 1594 @ step 2492000: Train Loss: 0.985219, Train Accuracy: 0.656063\n",
            "Epoch 1594 Test Loss: 1.035353, Test Accuracy: 0.629093, time: 7.7s\n",
            "  Epoch 1595 @ step 2493000: Train Loss: 0.974920, Train Accuracy: 0.656281\n",
            "  Epoch 1595 @ step 2494000: Train Loss: 0.970690, Train Accuracy: 0.661656\n",
            "Epoch 1595 Test Loss: 1.018015, Test Accuracy: 0.640475, time: 7.7s\n",
            "  Epoch 1596 @ step 2495000: Train Loss: 0.976475, Train Accuracy: 0.658531\n",
            "  Epoch 1596 @ step 2496000: Train Loss: 0.988168, Train Accuracy: 0.652375\n",
            "Epoch 1596 Test Loss: 0.996906, Test Accuracy: 0.653554, time: 7.7s\n",
            "  Epoch 1597 @ step 2497000: Train Loss: 0.971015, Train Accuracy: 0.660438\n",
            "Epoch 1597 Test Loss: 1.041713, Test Accuracy: 0.633686, time: 8.6s\n",
            "  Epoch 1598 @ step 2498000: Train Loss: 0.979718, Train Accuracy: 0.656406\n",
            "  Epoch 1598 @ step 2499000: Train Loss: 0.975877, Train Accuracy: 0.654719\n",
            "Epoch 1598 Test Loss: 1.051421, Test Accuracy: 0.621905, time: 7.7s\n",
            "  Epoch 1599 @ step 2500000: Train Loss: 0.983556, Train Accuracy: 0.657375\n",
            "Epoch 1599 Test Loss: 1.099573, Test Accuracy: 0.610124, time: 7.7s\n",
            "  Epoch 1600 @ step 2501000: Train Loss: 0.977354, Train Accuracy: 0.660375\n",
            "  Epoch 1600 @ step 2502000: Train Loss: 0.982204, Train Accuracy: 0.656000\n",
            "Epoch 1600 Test Loss: 1.001047, Test Accuracy: 0.646466, time: 7.7s\n",
            "  Epoch 1601 @ step 2503000: Train Loss: 0.967687, Train Accuracy: 0.660781\n",
            "Epoch 1601 Test Loss: 0.998089, Test Accuracy: 0.651258, time: 7.7s\n",
            "  Epoch 1602 @ step 2504000: Train Loss: 0.983524, Train Accuracy: 0.656656\n",
            "  Epoch 1602 @ step 2505000: Train Loss: 0.968775, Train Accuracy: 0.663156\n",
            "Epoch 1602 Test Loss: 1.033374, Test Accuracy: 0.641374, time: 7.7s\n",
            "  Epoch 1603 @ step 2506000: Train Loss: 0.982201, Train Accuracy: 0.653719\n",
            "  Epoch 1603 @ step 2507000: Train Loss: 0.985937, Train Accuracy: 0.653313\n",
            "Epoch 1603 Test Loss: 0.991125, Test Accuracy: 0.649161, time: 8.5s\n",
            "  Epoch 1604 @ step 2508000: Train Loss: 0.967142, Train Accuracy: 0.662281\n",
            "Epoch 1604 Test Loss: 1.044402, Test Accuracy: 0.632987, time: 8.2s\n",
            "  Epoch 1605 @ step 2509000: Train Loss: 0.988404, Train Accuracy: 0.657375\n",
            "  Epoch 1605 @ step 2510000: Train Loss: 0.974408, Train Accuracy: 0.658563\n",
            "Epoch 1605 Test Loss: 1.104269, Test Accuracy: 0.603235, time: 7.7s\n",
            "  Epoch 1606 @ step 2511000: Train Loss: 0.970879, Train Accuracy: 0.661719\n",
            "Epoch 1606 Test Loss: 1.035932, Test Accuracy: 0.634784, time: 7.7s\n",
            "  Epoch 1607 @ step 2512000: Train Loss: 0.979298, Train Accuracy: 0.657063\n",
            "  Epoch 1607 @ step 2513000: Train Loss: 0.979723, Train Accuracy: 0.657813\n",
            "Epoch 1607 Test Loss: 0.995577, Test Accuracy: 0.649461, time: 7.7s\n",
            "  Epoch 1608 @ step 2514000: Train Loss: 0.973126, Train Accuracy: 0.656313\n",
            "Epoch 1608 Test Loss: 0.969223, Test Accuracy: 0.665535, time: 8.2s\n",
            "  Epoch 1609 @ step 2515000: Train Loss: 0.984874, Train Accuracy: 0.656594\n",
            "  Epoch 1609 @ step 2516000: Train Loss: 0.976684, Train Accuracy: 0.658438\n",
            "Epoch 1609 Test Loss: 1.030455, Test Accuracy: 0.640875, time: 8.0s\n",
            "  Epoch 1610 @ step 2517000: Train Loss: 0.968410, Train Accuracy: 0.661781\n",
            "Epoch 1610 Test Loss: 1.086667, Test Accuracy: 0.615315, time: 7.6s\n",
            "  Epoch 1611 @ step 2518000: Train Loss: 0.986778, Train Accuracy: 0.656469\n",
            "  Epoch 1611 @ step 2519000: Train Loss: 0.973200, Train Accuracy: 0.660063\n",
            "Epoch 1611 Test Loss: 0.978523, Test Accuracy: 0.663339, time: 7.7s\n",
            "  Epoch 1612 @ step 2520000: Train Loss: 0.978531, Train Accuracy: 0.657344\n",
            "  Epoch 1612 @ step 2521000: Train Loss: 0.978463, Train Accuracy: 0.655563\n",
            "Epoch 1612 Test Loss: 1.032302, Test Accuracy: 0.636681, time: 7.7s\n",
            "  Epoch 1613 @ step 2522000: Train Loss: 0.977145, Train Accuracy: 0.658094\n",
            "Epoch 1613 Test Loss: 0.996360, Test Accuracy: 0.645068, time: 7.8s\n",
            "  Epoch 1614 @ step 2523000: Train Loss: 0.977656, Train Accuracy: 0.656063\n",
            "  Epoch 1614 @ step 2524000: Train Loss: 0.973802, Train Accuracy: 0.660000\n",
            "Epoch 1614 Test Loss: 1.035429, Test Accuracy: 0.636681, time: 9.0s\n",
            "  Epoch 1615 @ step 2525000: Train Loss: 0.979863, Train Accuracy: 0.658156\n",
            "Epoch 1615 Test Loss: 0.973097, Test Accuracy: 0.659245, time: 7.8s\n",
            "  Epoch 1616 @ step 2526000: Train Loss: 0.986435, Train Accuracy: 0.654438\n",
            "  Epoch 1616 @ step 2527000: Train Loss: 0.977825, Train Accuracy: 0.658813\n",
            "Epoch 1616 Test Loss: 1.089043, Test Accuracy: 0.610623, time: 7.6s\n",
            "  Epoch 1617 @ step 2528000: Train Loss: 0.970066, Train Accuracy: 0.657906\n",
            "Epoch 1617 Test Loss: 1.023950, Test Accuracy: 0.646765, time: 7.6s\n",
            "  Epoch 1618 @ step 2529000: Train Loss: 0.986093, Train Accuracy: 0.656000\n",
            "  Epoch 1618 @ step 2530000: Train Loss: 0.976014, Train Accuracy: 0.662313\n",
            "Epoch 1618 Test Loss: 1.020611, Test Accuracy: 0.638678, time: 7.6s\n",
            "  Epoch 1619 @ step 2531000: Train Loss: 0.976920, Train Accuracy: 0.659563\n",
            "  Epoch 1619 @ step 2532000: Train Loss: 0.980286, Train Accuracy: 0.655125\n",
            "Epoch 1619 Test Loss: 1.105586, Test Accuracy: 0.615016, time: 7.7s\n",
            "  Epoch 1620 @ step 2533000: Train Loss: 0.975412, Train Accuracy: 0.658656\n",
            "Epoch 1620 Test Loss: 1.026507, Test Accuracy: 0.640375, time: 8.7s\n",
            "  Epoch 1621 @ step 2534000: Train Loss: 0.971383, Train Accuracy: 0.661344\n",
            "  Epoch 1621 @ step 2535000: Train Loss: 0.985594, Train Accuracy: 0.654563\n",
            "Epoch 1621 Test Loss: 1.011631, Test Accuracy: 0.646066, time: 7.6s\n",
            "  Epoch 1622 @ step 2536000: Train Loss: 0.977454, Train Accuracy: 0.658813\n",
            "Epoch 1622 Test Loss: 1.006569, Test Accuracy: 0.647963, time: 7.7s\n",
            "  Epoch 1623 @ step 2537000: Train Loss: 0.975422, Train Accuracy: 0.655469\n",
            "  Epoch 1623 @ step 2538000: Train Loss: 0.978982, Train Accuracy: 0.655906\n",
            "Epoch 1623 Test Loss: 0.975494, Test Accuracy: 0.652456, time: 7.6s\n",
            "  Epoch 1624 @ step 2539000: Train Loss: 0.981562, Train Accuracy: 0.655563\n",
            "Epoch 1624 Test Loss: 1.007257, Test Accuracy: 0.643470, time: 7.7s\n",
            "  Epoch 1625 @ step 2540000: Train Loss: 0.981260, Train Accuracy: 0.654000\n",
            "  Epoch 1625 @ step 2541000: Train Loss: 0.978722, Train Accuracy: 0.659375\n",
            "Epoch 1625 Test Loss: 1.056685, Test Accuracy: 0.626198, time: 8.0s\n",
            "  Epoch 1626 @ step 2542000: Train Loss: 0.972650, Train Accuracy: 0.659375\n",
            "  Epoch 1626 @ step 2543000: Train Loss: 0.983556, Train Accuracy: 0.654406\n",
            "Epoch 1626 Test Loss: 1.076554, Test Accuracy: 0.612620, time: 8.3s\n",
            "  Epoch 1627 @ step 2544000: Train Loss: 0.967774, Train Accuracy: 0.659750\n",
            "Epoch 1627 Test Loss: 0.957616, Test Accuracy: 0.669828, time: 7.7s\n",
            "  Epoch 1628 @ step 2545000: Train Loss: 0.988434, Train Accuracy: 0.656156\n",
            "  Epoch 1628 @ step 2546000: Train Loss: 0.976877, Train Accuracy: 0.659438\n",
            "Epoch 1628 Test Loss: 1.007305, Test Accuracy: 0.642871, time: 7.8s\n",
            "  Epoch 1629 @ step 2547000: Train Loss: 0.973727, Train Accuracy: 0.660344\n",
            "Epoch 1629 Test Loss: 1.037910, Test Accuracy: 0.632688, time: 7.6s\n",
            "  Epoch 1630 @ step 2548000: Train Loss: 0.976524, Train Accuracy: 0.656625\n",
            "  Epoch 1630 @ step 2549000: Train Loss: 0.977261, Train Accuracy: 0.654875\n",
            "Epoch 1630 Test Loss: 1.087101, Test Accuracy: 0.610923, time: 7.6s\n",
            "  Epoch 1631 @ step 2550000: Train Loss: 0.980617, Train Accuracy: 0.656750\n",
            "Epoch 1631 Test Loss: 0.970368, Test Accuracy: 0.665435, time: 8.4s\n",
            "  Epoch 1632 @ step 2551000: Train Loss: 0.980790, Train Accuracy: 0.656219\n",
            "  Epoch 1632 @ step 2552000: Train Loss: 0.984279, Train Accuracy: 0.653469\n",
            "Epoch 1632 Test Loss: 1.031304, Test Accuracy: 0.627296, time: 7.8s\n",
            "  Epoch 1633 @ step 2553000: Train Loss: 0.969799, Train Accuracy: 0.661688\n",
            "Epoch 1633 Test Loss: 0.997699, Test Accuracy: 0.656849, time: 7.7s\n",
            "  Epoch 1634 @ step 2554000: Train Loss: 0.987669, Train Accuracy: 0.653438\n",
            "  Epoch 1634 @ step 2555000: Train Loss: 0.970677, Train Accuracy: 0.660281\n",
            "Epoch 1634 Test Loss: 1.000688, Test Accuracy: 0.649161, time: 7.6s\n",
            "  Epoch 1635 @ step 2556000: Train Loss: 0.976732, Train Accuracy: 0.658094\n",
            "  Epoch 1635 @ step 2557000: Train Loss: 0.986754, Train Accuracy: 0.652625\n",
            "Epoch 1635 Test Loss: 1.060217, Test Accuracy: 0.629393, time: 7.7s\n",
            "  Epoch 1636 @ step 2558000: Train Loss: 0.976553, Train Accuracy: 0.657719\n",
            "Epoch 1636 Test Loss: 1.048289, Test Accuracy: 0.633886, time: 8.5s\n",
            "  Epoch 1637 @ step 2559000: Train Loss: 0.976732, Train Accuracy: 0.659344\n",
            "  Epoch 1637 @ step 2560000: Train Loss: 0.979598, Train Accuracy: 0.658844\n",
            "Epoch 1637 Test Loss: 1.102429, Test Accuracy: 0.612220, time: 8.6s\n",
            "  Epoch 1638 @ step 2561000: Train Loss: 0.983951, Train Accuracy: 0.655656\n",
            "Epoch 1638 Test Loss: 1.017460, Test Accuracy: 0.643271, time: 7.6s\n",
            "  Epoch 1639 @ step 2562000: Train Loss: 0.974521, Train Accuracy: 0.658188\n",
            "  Epoch 1639 @ step 2563000: Train Loss: 0.979763, Train Accuracy: 0.656781\n",
            "Epoch 1639 Test Loss: 0.961314, Test Accuracy: 0.667532, time: 7.6s\n",
            "  Epoch 1640 @ step 2564000: Train Loss: 0.976950, Train Accuracy: 0.656875\n",
            "Epoch 1640 Test Loss: 1.002718, Test Accuracy: 0.640475, time: 7.6s\n",
            "  Epoch 1641 @ step 2565000: Train Loss: 0.977726, Train Accuracy: 0.660094\n",
            "  Epoch 1641 @ step 2566000: Train Loss: 0.977036, Train Accuracy: 0.658438\n",
            "Epoch 1641 Test Loss: 1.112750, Test Accuracy: 0.604034, time: 7.7s\n",
            "  Epoch 1642 @ step 2567000: Train Loss: 0.973889, Train Accuracy: 0.655750\n",
            "  Epoch 1642 @ step 2568000: Train Loss: 0.991865, Train Accuracy: 0.654375\n",
            "Epoch 1642 Test Loss: 1.041331, Test Accuracy: 0.634884, time: 7.8s\n",
            "  Epoch 1643 @ step 2569000: Train Loss: 0.969592, Train Accuracy: 0.661406\n",
            "Epoch 1643 Test Loss: 1.002608, Test Accuracy: 0.657049, time: 8.3s\n",
            "  Epoch 1644 @ step 2570000: Train Loss: 0.974155, Train Accuracy: 0.659344\n",
            "  Epoch 1644 @ step 2571000: Train Loss: 0.986424, Train Accuracy: 0.654438\n",
            "Epoch 1644 Test Loss: 0.965179, Test Accuracy: 0.667831, time: 7.6s\n",
            "  Epoch 1645 @ step 2572000: Train Loss: 0.973040, Train Accuracy: 0.658125\n",
            "Epoch 1645 Test Loss: 1.093160, Test Accuracy: 0.620108, time: 7.7s\n",
            "  Epoch 1646 @ step 2573000: Train Loss: 0.978869, Train Accuracy: 0.656344\n",
            "  Epoch 1646 @ step 2574000: Train Loss: 0.984988, Train Accuracy: 0.655563\n",
            "Epoch 1646 Test Loss: 0.960493, Test Accuracy: 0.666434, time: 7.7s\n",
            "  Epoch 1647 @ step 2575000: Train Loss: 0.976764, Train Accuracy: 0.658531\n",
            "Epoch 1647 Test Loss: 1.012477, Test Accuracy: 0.641573, time: 8.1s\n",
            "  Epoch 1648 @ step 2576000: Train Loss: 0.976404, Train Accuracy: 0.658531\n",
            "  Epoch 1648 @ step 2577000: Train Loss: 0.973604, Train Accuracy: 0.658719\n",
            "Epoch 1648 Test Loss: 1.067648, Test Accuracy: 0.626398, time: 8.4s\n",
            "  Epoch 1649 @ step 2578000: Train Loss: 0.982899, Train Accuracy: 0.655594\n",
            "Epoch 1649 Test Loss: 0.968584, Test Accuracy: 0.661342, time: 7.9s\n",
            "  Epoch 1650 @ step 2579000: Train Loss: 0.977448, Train Accuracy: 0.655063\n",
            "  Epoch 1650 @ step 2580000: Train Loss: 0.975841, Train Accuracy: 0.657188\n",
            "Epoch 1650 Test Loss: 0.984132, Test Accuracy: 0.658546, time: 7.7s\n",
            "  Epoch 1651 @ step 2581000: Train Loss: 0.983362, Train Accuracy: 0.656750\n",
            "  Epoch 1651 @ step 2582000: Train Loss: 0.981768, Train Accuracy: 0.657063\n",
            "Epoch 1651 Test Loss: 0.991663, Test Accuracy: 0.657049, time: 7.6s\n",
            "  Epoch 1652 @ step 2583000: Train Loss: 0.970766, Train Accuracy: 0.664031\n",
            "Epoch 1652 Test Loss: 1.042806, Test Accuracy: 0.630292, time: 7.7s\n",
            "  Epoch 1653 @ step 2584000: Train Loss: 0.980074, Train Accuracy: 0.655969\n",
            "  Epoch 1653 @ step 2585000: Train Loss: 0.978558, Train Accuracy: 0.658563\n",
            "Epoch 1653 Test Loss: 0.984827, Test Accuracy: 0.656050, time: 7.7s\n",
            "  Epoch 1654 @ step 2586000: Train Loss: 0.977602, Train Accuracy: 0.656375\n",
            "Epoch 1654 Test Loss: 0.996004, Test Accuracy: 0.649361, time: 8.6s\n",
            "  Epoch 1655 @ step 2587000: Train Loss: 0.973634, Train Accuracy: 0.659406\n",
            "  Epoch 1655 @ step 2588000: Train Loss: 0.976727, Train Accuracy: 0.657344\n",
            "Epoch 1655 Test Loss: 1.002905, Test Accuracy: 0.650459, time: 7.6s\n",
            "  Epoch 1656 @ step 2589000: Train Loss: 0.980719, Train Accuracy: 0.655844\n",
            "Epoch 1656 Test Loss: 0.980750, Test Accuracy: 0.656749, time: 7.7s\n",
            "  Epoch 1657 @ step 2590000: Train Loss: 0.979692, Train Accuracy: 0.656031\n",
            "  Epoch 1657 @ step 2591000: Train Loss: 0.979190, Train Accuracy: 0.654500\n",
            "Epoch 1657 Test Loss: 1.124493, Test Accuracy: 0.611522, time: 7.9s\n",
            "  Epoch 1658 @ step 2592000: Train Loss: 0.979522, Train Accuracy: 0.656719\n",
            "  Epoch 1658 @ step 2593000: Train Loss: 0.980897, Train Accuracy: 0.655875\n",
            "Epoch 1658 Test Loss: 1.078719, Test Accuracy: 0.613618, time: 8.6s\n",
            "  Epoch 1659 @ step 2594000: Train Loss: 0.982451, Train Accuracy: 0.656094\n",
            "Epoch 1659 Test Loss: 1.123800, Test Accuracy: 0.606729, time: 8.0s\n",
            "  Epoch 1660 @ step 2595000: Train Loss: 0.975831, Train Accuracy: 0.659906\n",
            "  Epoch 1660 @ step 2596000: Train Loss: 0.982769, Train Accuracy: 0.655500\n",
            "Epoch 1660 Test Loss: 1.026152, Test Accuracy: 0.641174, time: 8.4s\n",
            "  Epoch 1661 @ step 2597000: Train Loss: 0.974953, Train Accuracy: 0.659219\n",
            "Epoch 1661 Test Loss: 1.037493, Test Accuracy: 0.629193, time: 7.7s\n",
            "  Epoch 1662 @ step 2598000: Train Loss: 0.972215, Train Accuracy: 0.662469\n",
            "  Epoch 1662 @ step 2599000: Train Loss: 0.981912, Train Accuracy: 0.656313\n",
            "Epoch 1662 Test Loss: 1.030674, Test Accuracy: 0.636781, time: 7.7s\n",
            "  Epoch 1663 @ step 2600000: Train Loss: 0.979069, Train Accuracy: 0.655781\n",
            "Epoch 1663 Test Loss: 0.984634, Test Accuracy: 0.649860, time: 7.7s\n",
            "  Epoch 1664 @ step 2601000: Train Loss: 0.985176, Train Accuracy: 0.655188\n",
            "  Epoch 1664 @ step 2602000: Train Loss: 0.977226, Train Accuracy: 0.657406\n",
            "Epoch 1664 Test Loss: 1.000025, Test Accuracy: 0.650559, time: 7.6s\n",
            "  Epoch 1665 @ step 2603000: Train Loss: 0.983767, Train Accuracy: 0.657219\n",
            "Epoch 1665 Test Loss: 1.123317, Test Accuracy: 0.585563, time: 8.2s\n",
            "  Epoch 1666 @ step 2604000: Train Loss: 0.974922, Train Accuracy: 0.658906\n",
            "  Epoch 1666 @ step 2605000: Train Loss: 0.980390, Train Accuracy: 0.657375\n",
            "Epoch 1666 Test Loss: 1.026975, Test Accuracy: 0.636182, time: 8.0s\n",
            "  Epoch 1667 @ step 2606000: Train Loss: 0.973423, Train Accuracy: 0.657594\n",
            "  Epoch 1667 @ step 2607000: Train Loss: 0.983393, Train Accuracy: 0.654906\n",
            "Epoch 1667 Test Loss: 1.039035, Test Accuracy: 0.634085, time: 7.7s\n",
            "  Epoch 1668 @ step 2608000: Train Loss: 0.973743, Train Accuracy: 0.658125\n",
            "Epoch 1668 Test Loss: 1.046143, Test Accuracy: 0.639377, time: 7.6s\n",
            "  Epoch 1669 @ step 2609000: Train Loss: 0.976898, Train Accuracy: 0.661906\n",
            "  Epoch 1669 @ step 2610000: Train Loss: 0.979484, Train Accuracy: 0.655625\n",
            "Epoch 1669 Test Loss: 1.032048, Test Accuracy: 0.635284, time: 8.3s\n",
            "  Epoch 1670 @ step 2611000: Train Loss: 0.983685, Train Accuracy: 0.654156\n",
            "Epoch 1670 Test Loss: 0.995007, Test Accuracy: 0.647664, time: 7.7s\n",
            "  Epoch 1671 @ step 2612000: Train Loss: 0.972497, Train Accuracy: 0.659531\n",
            "  Epoch 1671 @ step 2613000: Train Loss: 0.978597, Train Accuracy: 0.655313\n",
            "Epoch 1671 Test Loss: 0.996884, Test Accuracy: 0.648762, time: 8.6s\n",
            "  Epoch 1672 @ step 2614000: Train Loss: 0.977736, Train Accuracy: 0.658281\n",
            "Epoch 1672 Test Loss: 0.994703, Test Accuracy: 0.646965, time: 7.6s\n",
            "  Epoch 1673 @ step 2615000: Train Loss: 0.979178, Train Accuracy: 0.657188\n",
            "  Epoch 1673 @ step 2616000: Train Loss: 0.974374, Train Accuracy: 0.657563\n",
            "Epoch 1673 Test Loss: 1.034551, Test Accuracy: 0.633686, time: 7.6s\n",
            "  Epoch 1674 @ step 2617000: Train Loss: 0.976487, Train Accuracy: 0.658125\n",
            "  Epoch 1674 @ step 2618000: Train Loss: 0.986675, Train Accuracy: 0.657063\n",
            "Epoch 1674 Test Loss: 1.053942, Test Accuracy: 0.622604, time: 7.6s\n",
            "  Epoch 1675 @ step 2619000: Train Loss: 0.969349, Train Accuracy: 0.662813\n",
            "Epoch 1675 Test Loss: 1.004763, Test Accuracy: 0.651058, time: 7.7s\n",
            "  Epoch 1676 @ step 2620000: Train Loss: 0.980406, Train Accuracy: 0.654969\n",
            "  Epoch 1676 @ step 2621000: Train Loss: 0.974688, Train Accuracy: 0.659250\n",
            "Epoch 1676 Test Loss: 1.115762, Test Accuracy: 0.599541, time: 7.7s\n",
            "  Epoch 1677 @ step 2622000: Train Loss: 0.977141, Train Accuracy: 0.659031\n",
            "Epoch 1677 Test Loss: 1.056211, Test Accuracy: 0.618211, time: 8.6s\n",
            "  Epoch 1678 @ step 2623000: Train Loss: 0.979931, Train Accuracy: 0.658000\n",
            "  Epoch 1678 @ step 2624000: Train Loss: 0.979065, Train Accuracy: 0.657594\n",
            "Epoch 1678 Test Loss: 1.005199, Test Accuracy: 0.652256, time: 7.7s\n",
            "  Epoch 1679 @ step 2625000: Train Loss: 0.976199, Train Accuracy: 0.658281\n",
            "Epoch 1679 Test Loss: 1.011262, Test Accuracy: 0.645467, time: 7.8s\n",
            "  Epoch 1680 @ step 2626000: Train Loss: 0.981921, Train Accuracy: 0.655844\n",
            "  Epoch 1680 @ step 2627000: Train Loss: 0.987278, Train Accuracy: 0.653469\n",
            "Epoch 1680 Test Loss: 1.032954, Test Accuracy: 0.636981, time: 8.5s\n",
            "  Epoch 1681 @ step 2628000: Train Loss: 0.966653, Train Accuracy: 0.661844\n",
            "Epoch 1681 Test Loss: 1.034663, Test Accuracy: 0.636082, time: 7.6s\n",
            "  Epoch 1682 @ step 2629000: Train Loss: 0.984876, Train Accuracy: 0.655531\n",
            "  Epoch 1682 @ step 2630000: Train Loss: 0.981022, Train Accuracy: 0.657781\n",
            "Epoch 1682 Test Loss: 0.967478, Test Accuracy: 0.665136, time: 8.0s\n",
            "  Epoch 1683 @ step 2631000: Train Loss: 0.976620, Train Accuracy: 0.657156\n",
            "  Epoch 1683 @ step 2632000: Train Loss: 0.981935, Train Accuracy: 0.657219\n",
            "Epoch 1683 Test Loss: 1.032119, Test Accuracy: 0.634185, time: 8.1s\n",
            "  Epoch 1684 @ step 2633000: Train Loss: 0.971418, Train Accuracy: 0.657938\n",
            "Epoch 1684 Test Loss: 1.041127, Test Accuracy: 0.624301, time: 7.6s\n",
            "  Epoch 1685 @ step 2634000: Train Loss: 0.986871, Train Accuracy: 0.654250\n",
            "  Epoch 1685 @ step 2635000: Train Loss: 0.976843, Train Accuracy: 0.659469\n",
            "Epoch 1685 Test Loss: 0.998177, Test Accuracy: 0.652556, time: 7.6s\n",
            "  Epoch 1686 @ step 2636000: Train Loss: 0.976272, Train Accuracy: 0.658781\n",
            "Epoch 1686 Test Loss: 1.023914, Test Accuracy: 0.637580, time: 7.6s\n",
            "  Epoch 1687 @ step 2637000: Train Loss: 0.977175, Train Accuracy: 0.655875\n",
            "  Epoch 1687 @ step 2638000: Train Loss: 0.976379, Train Accuracy: 0.658313\n",
            "Epoch 1687 Test Loss: 1.006297, Test Accuracy: 0.649161, time: 7.6s\n",
            "  Epoch 1688 @ step 2639000: Train Loss: 0.974196, Train Accuracy: 0.657438\n",
            "Epoch 1688 Test Loss: 1.111310, Test Accuracy: 0.597145, time: 8.3s\n",
            "  Epoch 1689 @ step 2640000: Train Loss: 0.982335, Train Accuracy: 0.655344\n",
            "  Epoch 1689 @ step 2641000: Train Loss: 0.971940, Train Accuracy: 0.660688\n",
            "Epoch 1689 Test Loss: 1.004428, Test Accuracy: 0.646466, time: 7.8s\n",
            "  Epoch 1690 @ step 2642000: Train Loss: 0.986244, Train Accuracy: 0.654688\n",
            "  Epoch 1690 @ step 2643000: Train Loss: 0.978940, Train Accuracy: 0.655938\n",
            "Epoch 1690 Test Loss: 1.124579, Test Accuracy: 0.602236, time: 7.7s\n",
            "  Epoch 1691 @ step 2644000: Train Loss: 0.976858, Train Accuracy: 0.656969\n",
            "Epoch 1691 Test Loss: 1.101708, Test Accuracy: 0.611921, time: 8.0s\n",
            "  Epoch 1692 @ step 2645000: Train Loss: 0.974834, Train Accuracy: 0.657625\n",
            "  Epoch 1692 @ step 2646000: Train Loss: 0.982471, Train Accuracy: 0.656938\n",
            "Epoch 1692 Test Loss: 1.026085, Test Accuracy: 0.644169, time: 7.6s\n",
            "  Epoch 1693 @ step 2647000: Train Loss: 0.977967, Train Accuracy: 0.656531\n",
            "Epoch 1693 Test Loss: 1.109995, Test Accuracy: 0.596246, time: 7.6s\n",
            "  Epoch 1694 @ step 2648000: Train Loss: 0.974345, Train Accuracy: 0.659344\n",
            "  Epoch 1694 @ step 2649000: Train Loss: 0.979186, Train Accuracy: 0.654281\n",
            "Epoch 1694 Test Loss: 1.216720, Test Accuracy: 0.567292, time: 8.5s\n",
            "  Epoch 1695 @ step 2650000: Train Loss: 0.978859, Train Accuracy: 0.658094\n",
            "Epoch 1695 Test Loss: 0.997642, Test Accuracy: 0.649461, time: 7.5s\n",
            "  Epoch 1696 @ step 2651000: Train Loss: 0.977266, Train Accuracy: 0.655438\n",
            "  Epoch 1696 @ step 2652000: Train Loss: 0.978972, Train Accuracy: 0.657813\n",
            "Epoch 1696 Test Loss: 1.137543, Test Accuracy: 0.580471, time: 7.6s\n",
            "  Epoch 1697 @ step 2653000: Train Loss: 0.976862, Train Accuracy: 0.656844\n",
            "Epoch 1697 Test Loss: 1.052147, Test Accuracy: 0.630990, time: 7.6s\n",
            "  Epoch 1698 @ step 2654000: Train Loss: 0.984588, Train Accuracy: 0.654969\n",
            "  Epoch 1698 @ step 2655000: Train Loss: 0.971385, Train Accuracy: 0.658781\n",
            "Epoch 1698 Test Loss: 0.975796, Test Accuracy: 0.659744, time: 7.6s\n",
            "  Epoch 1699 @ step 2656000: Train Loss: 0.973799, Train Accuracy: 0.655531\n",
            "  Epoch 1699 @ step 2657000: Train Loss: 0.985342, Train Accuracy: 0.656781\n",
            "Epoch 1699 Test Loss: 1.073326, Test Accuracy: 0.611222, time: 7.5s\n",
            "  Epoch 1700 @ step 2658000: Train Loss: 0.969075, Train Accuracy: 0.659281\n",
            "Epoch 1700 Test Loss: 1.062012, Test Accuracy: 0.623902, time: 8.5s\n",
            "  Epoch 1701 @ step 2659000: Train Loss: 0.980879, Train Accuracy: 0.660031\n",
            "  Epoch 1701 @ step 2660000: Train Loss: 0.980519, Train Accuracy: 0.654031\n",
            "Epoch 1701 Test Loss: 0.979867, Test Accuracy: 0.660942, time: 7.6s\n",
            "  Epoch 1702 @ step 2661000: Train Loss: 0.968772, Train Accuracy: 0.661000\n",
            "Epoch 1702 Test Loss: 1.068383, Test Accuracy: 0.615915, time: 8.6s\n",
            "  Epoch 1703 @ step 2662000: Train Loss: 0.993308, Train Accuracy: 0.651156\n",
            "  Epoch 1703 @ step 2663000: Train Loss: 0.971649, Train Accuracy: 0.658938\n",
            "Epoch 1703 Test Loss: 0.998495, Test Accuracy: 0.647264, time: 7.6s\n",
            "  Epoch 1704 @ step 2664000: Train Loss: 0.978282, Train Accuracy: 0.658438\n",
            "Epoch 1704 Test Loss: 1.041204, Test Accuracy: 0.626098, time: 7.5s\n",
            "  Epoch 1705 @ step 2665000: Train Loss: 0.985084, Train Accuracy: 0.654406\n",
            "  Epoch 1705 @ step 2666000: Train Loss: 0.976254, Train Accuracy: 0.659438\n",
            "Epoch 1705 Test Loss: 1.045449, Test Accuracy: 0.628794, time: 8.0s\n",
            "  Epoch 1706 @ step 2667000: Train Loss: 0.978807, Train Accuracy: 0.658625\n",
            "  Epoch 1706 @ step 2668000: Train Loss: 0.984539, Train Accuracy: 0.653969\n",
            "Epoch 1706 Test Loss: 1.050212, Test Accuracy: 0.628395, time: 7.9s\n",
            "  Epoch 1707 @ step 2669000: Train Loss: 0.976766, Train Accuracy: 0.658313\n",
            "Epoch 1707 Test Loss: 1.004678, Test Accuracy: 0.649261, time: 7.5s\n",
            "  Epoch 1708 @ step 2670000: Train Loss: 0.971759, Train Accuracy: 0.660313\n",
            "  Epoch 1708 @ step 2671000: Train Loss: 0.985246, Train Accuracy: 0.653906\n",
            "Epoch 1708 Test Loss: 1.074078, Test Accuracy: 0.615315, time: 7.5s\n",
            "  Epoch 1709 @ step 2672000: Train Loss: 0.978948, Train Accuracy: 0.658063\n",
            "Epoch 1709 Test Loss: 1.035233, Test Accuracy: 0.638478, time: 7.6s\n",
            "  Epoch 1710 @ step 2673000: Train Loss: 0.971765, Train Accuracy: 0.659500\n",
            "  Epoch 1710 @ step 2674000: Train Loss: 0.981238, Train Accuracy: 0.657719\n",
            "Epoch 1710 Test Loss: 1.084490, Test Accuracy: 0.617512, time: 7.6s\n",
            "  Epoch 1711 @ step 2675000: Train Loss: 0.980848, Train Accuracy: 0.655625\n",
            "Epoch 1711 Test Loss: 1.016456, Test Accuracy: 0.647564, time: 8.4s\n",
            "  Epoch 1712 @ step 2676000: Train Loss: 0.968527, Train Accuracy: 0.658844\n",
            "  Epoch 1712 @ step 2677000: Train Loss: 0.978846, Train Accuracy: 0.657844\n",
            "Epoch 1712 Test Loss: 1.073429, Test Accuracy: 0.619509, time: 8.2s\n",
            "  Epoch 1713 @ step 2678000: Train Loss: 0.977613, Train Accuracy: 0.656313\n",
            "Epoch 1713 Test Loss: 1.143759, Test Accuracy: 0.598442, time: 7.6s\n",
            "  Epoch 1714 @ step 2679000: Train Loss: 0.981929, Train Accuracy: 0.656375\n",
            "  Epoch 1714 @ step 2680000: Train Loss: 0.979282, Train Accuracy: 0.657813\n",
            "Epoch 1714 Test Loss: 0.980202, Test Accuracy: 0.659645, time: 7.6s\n",
            "  Epoch 1715 @ step 2681000: Train Loss: 0.972930, Train Accuracy: 0.661375\n",
            "  Epoch 1715 @ step 2682000: Train Loss: 0.980646, Train Accuracy: 0.657719\n",
            "Epoch 1715 Test Loss: 1.040820, Test Accuracy: 0.634585, time: 7.6s\n",
            "  Epoch 1716 @ step 2683000: Train Loss: 0.975245, Train Accuracy: 0.659094\n",
            "Epoch 1716 Test Loss: 0.996225, Test Accuracy: 0.648462, time: 7.5s\n",
            "  Epoch 1717 @ step 2684000: Train Loss: 0.980641, Train Accuracy: 0.658406\n",
            "  Epoch 1717 @ step 2685000: Train Loss: 0.979578, Train Accuracy: 0.657125\n",
            "Epoch 1717 Test Loss: 1.042475, Test Accuracy: 0.630791, time: 8.6s\n",
            "  Epoch 1718 @ step 2686000: Train Loss: 0.971493, Train Accuracy: 0.660594\n",
            "Epoch 1718 Test Loss: 0.996053, Test Accuracy: 0.651258, time: 7.6s\n",
            "  Epoch 1719 @ step 2687000: Train Loss: 0.986246, Train Accuracy: 0.653594\n",
            "  Epoch 1719 @ step 2688000: Train Loss: 0.979206, Train Accuracy: 0.657188\n",
            "Epoch 1719 Test Loss: 1.039761, Test Accuracy: 0.630591, time: 7.5s\n",
            "  Epoch 1720 @ step 2689000: Train Loss: 0.968804, Train Accuracy: 0.661781\n",
            "Epoch 1720 Test Loss: 1.016111, Test Accuracy: 0.636881, time: 7.5s\n",
            "  Epoch 1721 @ step 2690000: Train Loss: 0.984352, Train Accuracy: 0.654281\n",
            "  Epoch 1721 @ step 2691000: Train Loss: 0.974327, Train Accuracy: 0.658875\n",
            "Epoch 1721 Test Loss: 1.014811, Test Accuracy: 0.644469, time: 7.5s\n",
            "  Epoch 1722 @ step 2692000: Train Loss: 0.974211, Train Accuracy: 0.659781\n",
            "  Epoch 1722 @ step 2693000: Train Loss: 0.984376, Train Accuracy: 0.655344\n",
            "Epoch 1722 Test Loss: 1.053275, Test Accuracy: 0.634685, time: 7.6s\n",
            "  Epoch 1723 @ step 2694000: Train Loss: 0.977661, Train Accuracy: 0.659125\n",
            "Epoch 1723 Test Loss: 0.980403, Test Accuracy: 0.661841, time: 8.5s\n",
            "  Epoch 1724 @ step 2695000: Train Loss: 0.976975, Train Accuracy: 0.658594\n",
            "  Epoch 1724 @ step 2696000: Train Loss: 0.979123, Train Accuracy: 0.658188\n",
            "Epoch 1724 Test Loss: 0.957738, Test Accuracy: 0.667732, time: 8.7s\n",
            "  Epoch 1725 @ step 2697000: Train Loss: 0.980285, Train Accuracy: 0.657219\n",
            "Epoch 1725 Test Loss: 1.126316, Test Accuracy: 0.609924, time: 7.9s\n",
            "  Epoch 1726 @ step 2698000: Train Loss: 0.973983, Train Accuracy: 0.658938\n",
            "  Epoch 1726 @ step 2699000: Train Loss: 0.979860, Train Accuracy: 0.657281\n",
            "Epoch 1726 Test Loss: 1.017599, Test Accuracy: 0.637979, time: 8.1s\n",
            "  Epoch 1727 @ step 2700000: Train Loss: 0.973573, Train Accuracy: 0.659313\n",
            "Epoch 1727 Test Loss: 1.050229, Test Accuracy: 0.633387, time: 8.2s\n",
            "  Epoch 1728 @ step 2701000: Train Loss: 0.985505, Train Accuracy: 0.654094\n",
            "  Epoch 1728 @ step 2702000: Train Loss: 0.978149, Train Accuracy: 0.655719\n",
            "Epoch 1728 Test Loss: 1.019775, Test Accuracy: 0.636681, time: 8.9s\n",
            "  Epoch 1729 @ step 2703000: Train Loss: 0.975859, Train Accuracy: 0.660844\n",
            "Epoch 1729 Test Loss: 1.004703, Test Accuracy: 0.649361, time: 8.3s\n",
            "  Epoch 1730 @ step 2704000: Train Loss: 0.981292, Train Accuracy: 0.656719\n",
            "  Epoch 1730 @ step 2705000: Train Loss: 0.975239, Train Accuracy: 0.658875\n",
            "Epoch 1730 Test Loss: 0.974700, Test Accuracy: 0.657748, time: 7.8s\n",
            "  Epoch 1731 @ step 2706000: Train Loss: 0.974653, Train Accuracy: 0.659563\n",
            "  Epoch 1731 @ step 2707000: Train Loss: 0.983813, Train Accuracy: 0.654188\n",
            "Epoch 1731 Test Loss: 0.997223, Test Accuracy: 0.650160, time: 8.1s\n",
            "  Epoch 1732 @ step 2708000: Train Loss: 0.978468, Train Accuracy: 0.657125\n",
            "Epoch 1732 Test Loss: 1.147830, Test Accuracy: 0.594848, time: 8.5s\n",
            "  Epoch 1733 @ step 2709000: Train Loss: 0.975594, Train Accuracy: 0.657875\n",
            "  Epoch 1733 @ step 2710000: Train Loss: 0.982032, Train Accuracy: 0.655281\n",
            "Epoch 1733 Test Loss: 1.260039, Test Accuracy: 0.551518, time: 8.3s\n",
            "  Epoch 1734 @ step 2711000: Train Loss: 0.973948, Train Accuracy: 0.657531\n",
            "Epoch 1734 Test Loss: 1.032264, Test Accuracy: 0.636082, time: 8.7s\n",
            "  Epoch 1735 @ step 2712000: Train Loss: 0.982312, Train Accuracy: 0.658000\n",
            "  Epoch 1735 @ step 2713000: Train Loss: 0.976700, Train Accuracy: 0.655813\n",
            "Epoch 1735 Test Loss: 0.982555, Test Accuracy: 0.656949, time: 7.8s\n",
            "  Epoch 1736 @ step 2714000: Train Loss: 0.966758, Train Accuracy: 0.659938\n",
            "Epoch 1736 Test Loss: 1.022585, Test Accuracy: 0.647464, time: 7.7s\n",
            "  Epoch 1737 @ step 2715000: Train Loss: 0.981367, Train Accuracy: 0.657813\n",
            "  Epoch 1737 @ step 2716000: Train Loss: 0.978712, Train Accuracy: 0.656250\n",
            "Epoch 1737 Test Loss: 1.038093, Test Accuracy: 0.631989, time: 7.7s\n",
            "  Epoch 1738 @ step 2717000: Train Loss: 0.971261, Train Accuracy: 0.660063\n",
            "  Epoch 1738 @ step 2718000: Train Loss: 0.992919, Train Accuracy: 0.653750\n",
            "Epoch 1738 Test Loss: 1.131791, Test Accuracy: 0.595946, time: 7.7s\n",
            "  Epoch 1739 @ step 2719000: Train Loss: 0.975706, Train Accuracy: 0.658156\n",
            "Epoch 1739 Test Loss: 0.991355, Test Accuracy: 0.653454, time: 8.3s\n",
            "  Epoch 1740 @ step 2720000: Train Loss: 0.969945, Train Accuracy: 0.658656\n",
            "  Epoch 1740 @ step 2721000: Train Loss: 0.980883, Train Accuracy: 0.657406\n",
            "Epoch 1740 Test Loss: 1.077733, Test Accuracy: 0.613818, time: 7.9s\n",
            "  Epoch 1741 @ step 2722000: Train Loss: 0.975285, Train Accuracy: 0.661031\n",
            "Epoch 1741 Test Loss: 1.104218, Test Accuracy: 0.609525, time: 7.7s\n",
            "  Epoch 1742 @ step 2723000: Train Loss: 0.972830, Train Accuracy: 0.656281\n",
            "  Epoch 1742 @ step 2724000: Train Loss: 0.979358, Train Accuracy: 0.661125\n",
            "Epoch 1742 Test Loss: 0.996708, Test Accuracy: 0.651258, time: 7.6s\n",
            "  Epoch 1743 @ step 2725000: Train Loss: 0.970438, Train Accuracy: 0.661344\n",
            "Epoch 1743 Test Loss: 1.115952, Test Accuracy: 0.611322, time: 7.7s\n",
            "  Epoch 1744 @ step 2726000: Train Loss: 0.986269, Train Accuracy: 0.652313\n",
            "  Epoch 1744 @ step 2727000: Train Loss: 0.975242, Train Accuracy: 0.657938\n",
            "Epoch 1744 Test Loss: 1.025558, Test Accuracy: 0.641174, time: 7.7s\n",
            "  Epoch 1745 @ step 2728000: Train Loss: 0.977592, Train Accuracy: 0.658156\n",
            "Epoch 1745 Test Loss: 0.989894, Test Accuracy: 0.661142, time: 8.7s\n",
            "  Epoch 1746 @ step 2729000: Train Loss: 0.982244, Train Accuracy: 0.654719\n",
            "  Epoch 1746 @ step 2730000: Train Loss: 0.974765, Train Accuracy: 0.658500\n",
            "Epoch 1746 Test Loss: 1.019915, Test Accuracy: 0.634285, time: 8.0s\n",
            "  Epoch 1747 @ step 2731000: Train Loss: 0.972692, Train Accuracy: 0.661438\n",
            "  Epoch 1747 @ step 2732000: Train Loss: 0.985001, Train Accuracy: 0.656438\n",
            "Epoch 1747 Test Loss: 1.018283, Test Accuracy: 0.644768, time: 7.6s\n",
            "  Epoch 1748 @ step 2733000: Train Loss: 0.972294, Train Accuracy: 0.659688\n",
            "Epoch 1748 Test Loss: 1.028096, Test Accuracy: 0.646266, time: 7.6s\n",
            "  Epoch 1749 @ step 2734000: Train Loss: 0.984479, Train Accuracy: 0.656594\n",
            "  Epoch 1749 @ step 2735000: Train Loss: 0.982025, Train Accuracy: 0.657406\n",
            "Epoch 1749 Test Loss: 1.089681, Test Accuracy: 0.607927, time: 7.6s\n",
            "  Epoch 1750 @ step 2736000: Train Loss: 0.975710, Train Accuracy: 0.657938\n",
            "Epoch 1750 Test Loss: 1.035518, Test Accuracy: 0.631789, time: 7.7s\n",
            "  Epoch 1751 @ step 2737000: Train Loss: 0.978935, Train Accuracy: 0.659438\n",
            "  Epoch 1751 @ step 2738000: Train Loss: 0.979637, Train Accuracy: 0.659063\n",
            "Epoch 1751 Test Loss: 1.032762, Test Accuracy: 0.630691, time: 8.6s\n",
            "  Epoch 1752 @ step 2739000: Train Loss: 0.977384, Train Accuracy: 0.657281\n",
            "Epoch 1752 Test Loss: 1.121705, Test Accuracy: 0.604133, time: 7.6s\n",
            "  Epoch 1753 @ step 2740000: Train Loss: 0.976111, Train Accuracy: 0.657563\n",
            "  Epoch 1753 @ step 2741000: Train Loss: 0.978458, Train Accuracy: 0.656344\n",
            "Epoch 1753 Test Loss: 1.032582, Test Accuracy: 0.627396, time: 7.7s\n",
            "  Epoch 1754 @ step 2742000: Train Loss: 0.975512, Train Accuracy: 0.660063\n",
            "  Epoch 1754 @ step 2743000: Train Loss: 0.983007, Train Accuracy: 0.655094\n",
            "Epoch 1754 Test Loss: 1.051517, Test Accuracy: 0.628494, time: 7.8s\n",
            "  Epoch 1755 @ step 2744000: Train Loss: 0.973158, Train Accuracy: 0.658125\n",
            "Epoch 1755 Test Loss: 1.104635, Test Accuracy: 0.606430, time: 8.0s\n",
            "  Epoch 1756 @ step 2745000: Train Loss: 0.981236, Train Accuracy: 0.656875\n",
            "  Epoch 1756 @ step 2746000: Train Loss: 0.983677, Train Accuracy: 0.656438\n",
            "Epoch 1756 Test Loss: 1.024596, Test Accuracy: 0.637081, time: 8.1s\n",
            "  Epoch 1757 @ step 2747000: Train Loss: 0.968442, Train Accuracy: 0.660781\n",
            "Epoch 1757 Test Loss: 1.120175, Test Accuracy: 0.603335, time: 8.0s\n",
            "  Epoch 1758 @ step 2748000: Train Loss: 0.981410, Train Accuracy: 0.657156\n",
            "  Epoch 1758 @ step 2749000: Train Loss: 0.981354, Train Accuracy: 0.655563\n",
            "Epoch 1758 Test Loss: 1.342576, Test Accuracy: 0.529353, time: 7.7s\n",
            "  Epoch 1759 @ step 2750000: Train Loss: 0.976226, Train Accuracy: 0.657469\n",
            "Epoch 1759 Test Loss: 1.025924, Test Accuracy: 0.637780, time: 7.8s\n",
            "  Epoch 1760 @ step 2751000: Train Loss: 0.985790, Train Accuracy: 0.655375\n",
            "  Epoch 1760 @ step 2752000: Train Loss: 0.978648, Train Accuracy: 0.657406\n",
            "Epoch 1760 Test Loss: 1.065653, Test Accuracy: 0.619908, time: 7.7s\n",
            "  Epoch 1761 @ step 2753000: Train Loss: 0.978390, Train Accuracy: 0.656375\n",
            "  Epoch 1761 @ step 2754000: Train Loss: 0.981375, Train Accuracy: 0.657313\n",
            "Epoch 1761 Test Loss: 1.024786, Test Accuracy: 0.641374, time: 7.6s\n",
            "  Epoch 1762 @ step 2755000: Train Loss: 0.974700, Train Accuracy: 0.658188\n",
            "Epoch 1762 Test Loss: 1.067982, Test Accuracy: 0.636681, time: 8.4s\n",
            "  Epoch 1763 @ step 2756000: Train Loss: 0.973553, Train Accuracy: 0.659781\n",
            "  Epoch 1763 @ step 2757000: Train Loss: 0.988374, Train Accuracy: 0.653969\n",
            "Epoch 1763 Test Loss: 1.056899, Test Accuracy: 0.627895, time: 7.7s\n",
            "  Epoch 1764 @ step 2758000: Train Loss: 0.971646, Train Accuracy: 0.658094\n",
            "Epoch 1764 Test Loss: 1.038254, Test Accuracy: 0.640675, time: 7.8s\n",
            "  Epoch 1765 @ step 2759000: Train Loss: 0.977019, Train Accuracy: 0.659719\n",
            "  Epoch 1765 @ step 2760000: Train Loss: 0.979140, Train Accuracy: 0.654281\n",
            "Epoch 1765 Test Loss: 1.013840, Test Accuracy: 0.651058, time: 7.7s\n",
            "  Epoch 1766 @ step 2761000: Train Loss: 0.975334, Train Accuracy: 0.659438\n",
            "Epoch 1766 Test Loss: 1.007855, Test Accuracy: 0.637081, time: 7.7s\n",
            "  Epoch 1767 @ step 2762000: Train Loss: 0.984292, Train Accuracy: 0.654656\n",
            "  Epoch 1767 @ step 2763000: Train Loss: 0.971692, Train Accuracy: 0.658875\n",
            "Epoch 1767 Test Loss: 0.997029, Test Accuracy: 0.651158, time: 8.0s\n",
            "  Epoch 1768 @ step 2764000: Train Loss: 0.978313, Train Accuracy: 0.655656\n",
            "Epoch 1768 Test Loss: 1.064106, Test Accuracy: 0.632089, time: 8.8s\n",
            "  Epoch 1769 @ step 2765000: Train Loss: 0.984140, Train Accuracy: 0.653219\n",
            "  Epoch 1769 @ step 2766000: Train Loss: 0.976595, Train Accuracy: 0.657875\n",
            "Epoch 1769 Test Loss: 1.037410, Test Accuracy: 0.627895, time: 7.7s\n",
            "  Epoch 1770 @ step 2767000: Train Loss: 0.979999, Train Accuracy: 0.659750\n",
            "  Epoch 1770 @ step 2768000: Train Loss: 0.977914, Train Accuracy: 0.657438\n",
            "Epoch 1770 Test Loss: 1.047233, Test Accuracy: 0.631689, time: 7.7s\n",
            "  Epoch 1771 @ step 2769000: Train Loss: 0.978805, Train Accuracy: 0.659188\n",
            "Epoch 1771 Test Loss: 1.033769, Test Accuracy: 0.644569, time: 7.6s\n",
            "  Epoch 1772 @ step 2770000: Train Loss: 0.981804, Train Accuracy: 0.655438\n",
            "  Epoch 1772 @ step 2771000: Train Loss: 0.971348, Train Accuracy: 0.660875\n",
            "Epoch 1772 Test Loss: 1.059376, Test Accuracy: 0.621605, time: 7.5s\n",
            "  Epoch 1773 @ step 2772000: Train Loss: 0.981395, Train Accuracy: 0.656125\n",
            "Epoch 1773 Test Loss: 1.030520, Test Accuracy: 0.628494, time: 7.7s\n",
            "  Epoch 1774 @ step 2773000: Train Loss: 0.983993, Train Accuracy: 0.653594\n",
            "  Epoch 1774 @ step 2774000: Train Loss: 0.983396, Train Accuracy: 0.656906\n",
            "Epoch 1774 Test Loss: 1.051916, Test Accuracy: 0.627097, time: 8.4s\n",
            "  Epoch 1775 @ step 2775000: Train Loss: 0.965434, Train Accuracy: 0.660563\n",
            "Epoch 1775 Test Loss: 1.073534, Test Accuracy: 0.625599, time: 7.5s\n",
            "  Epoch 1776 @ step 2776000: Train Loss: 0.977857, Train Accuracy: 0.659656\n",
            "  Epoch 1776 @ step 2777000: Train Loss: 0.973539, Train Accuracy: 0.659344\n",
            "Epoch 1776 Test Loss: 0.978143, Test Accuracy: 0.654153, time: 7.5s\n",
            "  Epoch 1777 @ step 2778000: Train Loss: 0.986376, Train Accuracy: 0.654156\n",
            "  Epoch 1777 @ step 2779000: Train Loss: 0.980147, Train Accuracy: 0.658281\n",
            "Epoch 1777 Test Loss: 1.040529, Test Accuracy: 0.629792, time: 8.1s\n",
            "  Epoch 1778 @ step 2780000: Train Loss: 0.974769, Train Accuracy: 0.660938\n",
            "Epoch 1778 Test Loss: 1.075440, Test Accuracy: 0.617013, time: 7.5s\n",
            "  Epoch 1779 @ step 2781000: Train Loss: 0.977100, Train Accuracy: 0.660031\n",
            "  Epoch 1779 @ step 2782000: Train Loss: 0.982903, Train Accuracy: 0.656813\n",
            "Epoch 1779 Test Loss: 1.016735, Test Accuracy: 0.641973, time: 8.0s\n",
            "  Epoch 1780 @ step 2783000: Train Loss: 0.966194, Train Accuracy: 0.660438\n",
            "Epoch 1780 Test Loss: 1.004116, Test Accuracy: 0.645467, time: 8.3s\n",
            "  Epoch 1781 @ step 2784000: Train Loss: 0.985210, Train Accuracy: 0.654125\n",
            "  Epoch 1781 @ step 2785000: Train Loss: 0.981704, Train Accuracy: 0.656875\n",
            "Epoch 1781 Test Loss: 1.065652, Test Accuracy: 0.612420, time: 8.2s\n",
            "  Epoch 1782 @ step 2786000: Train Loss: 0.967011, Train Accuracy: 0.661031\n",
            "Epoch 1782 Test Loss: 1.145084, Test Accuracy: 0.597444, time: 8.2s\n",
            "  Epoch 1783 @ step 2787000: Train Loss: 0.978804, Train Accuracy: 0.657469\n",
            "  Epoch 1783 @ step 2788000: Train Loss: 0.979413, Train Accuracy: 0.656375\n",
            "Epoch 1783 Test Loss: 1.039017, Test Accuracy: 0.625499, time: 8.3s\n",
            "  Epoch 1784 @ step 2789000: Train Loss: 0.980664, Train Accuracy: 0.657063\n",
            "Epoch 1784 Test Loss: 1.027612, Test Accuracy: 0.641174, time: 8.1s\n",
            "  Epoch 1785 @ step 2790000: Train Loss: 0.979053, Train Accuracy: 0.656375\n",
            "  Epoch 1785 @ step 2791000: Train Loss: 0.978698, Train Accuracy: 0.657500\n",
            "Epoch 1785 Test Loss: 1.001723, Test Accuracy: 0.647664, time: 8.7s\n",
            "  Epoch 1786 @ step 2792000: Train Loss: 0.977716, Train Accuracy: 0.659656\n",
            "  Epoch 1786 @ step 2793000: Train Loss: 0.981882, Train Accuracy: 0.657000\n",
            "Epoch 1786 Test Loss: 0.999626, Test Accuracy: 0.646565, time: 8.5s\n",
            "  Epoch 1787 @ step 2794000: Train Loss: 0.984849, Train Accuracy: 0.655969\n",
            "Epoch 1787 Test Loss: 1.042791, Test Accuracy: 0.625100, time: 8.5s\n",
            "  Epoch 1788 @ step 2795000: Train Loss: 0.973631, Train Accuracy: 0.658156\n",
            "  Epoch 1788 @ step 2796000: Train Loss: 0.980396, Train Accuracy: 0.656063\n",
            "Epoch 1788 Test Loss: 1.109311, Test Accuracy: 0.605032, time: 8.5s\n",
            "  Epoch 1789 @ step 2797000: Train Loss: 0.980185, Train Accuracy: 0.657781\n",
            "Epoch 1789 Test Loss: 1.028870, Test Accuracy: 0.641673, time: 8.9s\n",
            "  Epoch 1790 @ step 2798000: Train Loss: 0.977635, Train Accuracy: 0.657625\n",
            "  Epoch 1790 @ step 2799000: Train Loss: 0.979379, Train Accuracy: 0.655000\n",
            "Epoch 1790 Test Loss: 1.007170, Test Accuracy: 0.641174, time: 8.6s\n",
            "  Epoch 1791 @ step 2800000: Train Loss: 0.971887, Train Accuracy: 0.657313\n",
            "Epoch 1791 Test Loss: 1.038744, Test Accuracy: 0.631190, time: 8.0s\n",
            "  Epoch 1792 @ step 2801000: Train Loss: 0.982130, Train Accuracy: 0.655719\n",
            "  Epoch 1792 @ step 2802000: Train Loss: 0.973319, Train Accuracy: 0.658531\n",
            "Epoch 1792 Test Loss: 1.076714, Test Accuracy: 0.620008, time: 8.0s\n",
            "  Epoch 1793 @ step 2803000: Train Loss: 0.979796, Train Accuracy: 0.655438\n",
            "  Epoch 1793 @ step 2804000: Train Loss: 0.981637, Train Accuracy: 0.654781\n",
            "Epoch 1793 Test Loss: 1.069302, Test Accuracy: 0.625998, time: 8.0s\n",
            "  Epoch 1794 @ step 2805000: Train Loss: 0.970857, Train Accuracy: 0.658969\n",
            "Epoch 1794 Test Loss: 1.038242, Test Accuracy: 0.628095, time: 7.9s\n",
            "  Epoch 1795 @ step 2806000: Train Loss: 0.980088, Train Accuracy: 0.657500\n",
            "  Epoch 1795 @ step 2807000: Train Loss: 0.980013, Train Accuracy: 0.658719\n",
            "Epoch 1795 Test Loss: 1.143125, Test Accuracy: 0.581669, time: 7.8s\n",
            "  Epoch 1796 @ step 2808000: Train Loss: 0.978910, Train Accuracy: 0.658938\n",
            "Epoch 1796 Test Loss: 1.080000, Test Accuracy: 0.612121, time: 8.8s\n",
            "  Epoch 1797 @ step 2809000: Train Loss: 0.975911, Train Accuracy: 0.657125\n",
            "  Epoch 1797 @ step 2810000: Train Loss: 0.973136, Train Accuracy: 0.661125\n",
            "Epoch 1797 Test Loss: 0.985428, Test Accuracy: 0.655551, time: 7.9s\n",
            "  Epoch 1798 @ step 2811000: Train Loss: 0.974363, Train Accuracy: 0.657938\n",
            "Epoch 1798 Test Loss: 1.011975, Test Accuracy: 0.640974, time: 8.5s\n",
            "  Epoch 1799 @ step 2812000: Train Loss: 0.981784, Train Accuracy: 0.658094\n",
            "  Epoch 1799 @ step 2813000: Train Loss: 0.982743, Train Accuracy: 0.653750\n",
            "Epoch 1799 Test Loss: 0.974261, Test Accuracy: 0.657847, time: 7.8s\n",
            "  Epoch 1800 @ step 2814000: Train Loss: 0.980840, Train Accuracy: 0.656375\n",
            "Epoch 1800 Test Loss: 1.124826, Test Accuracy: 0.600539, time: 7.7s\n",
            "  Epoch 1801 @ step 2815000: Train Loss: 0.976622, Train Accuracy: 0.656563\n",
            "  Epoch 1801 @ step 2816000: Train Loss: 0.969193, Train Accuracy: 0.659063\n",
            "Epoch 1801 Test Loss: 1.134674, Test Accuracy: 0.597444, time: 8.2s\n",
            "  Epoch 1802 @ step 2817000: Train Loss: 0.977443, Train Accuracy: 0.658469\n",
            "  Epoch 1802 @ step 2818000: Train Loss: 0.982234, Train Accuracy: 0.657250\n",
            "Epoch 1802 Test Loss: 1.051729, Test Accuracy: 0.626997, time: 8.0s\n",
            "  Epoch 1803 @ step 2819000: Train Loss: 0.980816, Train Accuracy: 0.657781\n",
            "Epoch 1803 Test Loss: 1.008558, Test Accuracy: 0.644169, time: 7.7s\n",
            "  Epoch 1804 @ step 2820000: Train Loss: 0.974757, Train Accuracy: 0.658219\n",
            "  Epoch 1804 @ step 2821000: Train Loss: 0.974575, Train Accuracy: 0.656969\n",
            "Epoch 1804 Test Loss: 1.038578, Test Accuracy: 0.632788, time: 7.8s\n",
            "  Epoch 1805 @ step 2822000: Train Loss: 0.977837, Train Accuracy: 0.658563\n",
            "Epoch 1805 Test Loss: 1.031479, Test Accuracy: 0.636781, time: 7.7s\n",
            "  Epoch 1806 @ step 2823000: Train Loss: 0.981432, Train Accuracy: 0.656031\n",
            "  Epoch 1806 @ step 2824000: Train Loss: 0.983190, Train Accuracy: 0.657250\n",
            "Epoch 1806 Test Loss: 1.011032, Test Accuracy: 0.652955, time: 7.7s\n",
            "  Epoch 1807 @ step 2825000: Train Loss: 0.963337, Train Accuracy: 0.663625\n",
            "Epoch 1807 Test Loss: 1.056483, Test Accuracy: 0.626098, time: 8.5s\n",
            "  Epoch 1808 @ step 2826000: Train Loss: 0.990653, Train Accuracy: 0.652688\n",
            "  Epoch 1808 @ step 2827000: Train Loss: 0.974514, Train Accuracy: 0.658906\n",
            "Epoch 1808 Test Loss: 1.190384, Test Accuracy: 0.573183, time: 7.7s\n",
            "  Epoch 1809 @ step 2828000: Train Loss: 0.976939, Train Accuracy: 0.661000\n",
            "  Epoch 1809 @ step 2829000: Train Loss: 0.985904, Train Accuracy: 0.654688\n",
            "Epoch 1809 Test Loss: 1.013860, Test Accuracy: 0.648962, time: 7.6s\n",
            "  Epoch 1810 @ step 2830000: Train Loss: 0.976697, Train Accuracy: 0.661156\n",
            "Epoch 1810 Test Loss: 1.019750, Test Accuracy: 0.635883, time: 7.6s\n",
            "  Epoch 1811 @ step 2831000: Train Loss: 0.976464, Train Accuracy: 0.658188\n",
            "  Epoch 1811 @ step 2832000: Train Loss: 0.984130, Train Accuracy: 0.656938\n",
            "Epoch 1811 Test Loss: 1.047490, Test Accuracy: 0.629792, time: 8.7s\n",
            "  Epoch 1812 @ step 2833000: Train Loss: 0.967539, Train Accuracy: 0.661313\n",
            "Epoch 1812 Test Loss: 1.045076, Test Accuracy: 0.631390, time: 7.8s\n",
            "  Epoch 1813 @ step 2834000: Train Loss: 0.985781, Train Accuracy: 0.654750\n",
            "  Epoch 1813 @ step 2835000: Train Loss: 0.982296, Train Accuracy: 0.656406\n",
            "Epoch 1813 Test Loss: 0.979698, Test Accuracy: 0.658746, time: 8.6s\n",
            "  Epoch 1814 @ step 2836000: Train Loss: 0.981907, Train Accuracy: 0.656844\n",
            "Epoch 1814 Test Loss: 1.038506, Test Accuracy: 0.625699, time: 7.7s\n",
            "  Epoch 1815 @ step 2837000: Train Loss: 0.975127, Train Accuracy: 0.658906\n",
            "  Epoch 1815 @ step 2838000: Train Loss: 0.979194, Train Accuracy: 0.657188\n",
            "Epoch 1815 Test Loss: 1.092031, Test Accuracy: 0.603634, time: 7.7s\n",
            "  Epoch 1816 @ step 2839000: Train Loss: 0.978018, Train Accuracy: 0.656406\n",
            "Epoch 1816 Test Loss: 0.987089, Test Accuracy: 0.647564, time: 7.6s\n",
            "  Epoch 1817 @ step 2840000: Train Loss: 0.976752, Train Accuracy: 0.659000\n",
            "  Epoch 1817 @ step 2841000: Train Loss: 0.973410, Train Accuracy: 0.658063\n",
            "Epoch 1817 Test Loss: 0.988022, Test Accuracy: 0.655152, time: 7.9s\n",
            "  Epoch 1818 @ step 2842000: Train Loss: 0.984664, Train Accuracy: 0.654656\n",
            "  Epoch 1818 @ step 2843000: Train Loss: 0.977362, Train Accuracy: 0.657219\n",
            "Epoch 1818 Test Loss: 0.991554, Test Accuracy: 0.651458, time: 8.2s\n",
            "  Epoch 1819 @ step 2844000: Train Loss: 0.975207, Train Accuracy: 0.657563\n",
            "Epoch 1819 Test Loss: 1.042850, Test Accuracy: 0.633586, time: 8.5s\n",
            "  Epoch 1820 @ step 2845000: Train Loss: 0.978551, Train Accuracy: 0.656875\n",
            "  Epoch 1820 @ step 2846000: Train Loss: 0.982542, Train Accuracy: 0.656063\n",
            "Epoch 1820 Test Loss: 0.982227, Test Accuracy: 0.661442, time: 7.8s\n",
            "  Epoch 1821 @ step 2847000: Train Loss: 0.976429, Train Accuracy: 0.660094\n",
            "Epoch 1821 Test Loss: 0.973939, Test Accuracy: 0.659545, time: 7.6s\n",
            "  Epoch 1822 @ step 2848000: Train Loss: 0.976415, Train Accuracy: 0.657125\n",
            "  Epoch 1822 @ step 2849000: Train Loss: 0.974782, Train Accuracy: 0.658031\n",
            "Epoch 1822 Test Loss: 0.995711, Test Accuracy: 0.653155, time: 7.7s\n",
            "  Epoch 1823 @ step 2850000: Train Loss: 0.976804, Train Accuracy: 0.660938\n",
            "Epoch 1823 Test Loss: 1.009118, Test Accuracy: 0.644369, time: 7.7s\n",
            "  Epoch 1824 @ step 2851000: Train Loss: 0.986618, Train Accuracy: 0.653469\n",
            "  Epoch 1824 @ step 2852000: Train Loss: 0.975008, Train Accuracy: 0.660125\n",
            "Epoch 1824 Test Loss: 0.983950, Test Accuracy: 0.661442, time: 8.5s\n",
            "  Epoch 1825 @ step 2853000: Train Loss: 0.980261, Train Accuracy: 0.656781\n",
            "  Epoch 1825 @ step 2854000: Train Loss: 0.980966, Train Accuracy: 0.657063\n",
            "Epoch 1825 Test Loss: 1.026273, Test Accuracy: 0.644569, time: 7.7s\n",
            "  Epoch 1826 @ step 2855000: Train Loss: 0.978468, Train Accuracy: 0.658063\n",
            "Epoch 1826 Test Loss: 1.044222, Test Accuracy: 0.616813, time: 7.7s\n",
            "  Epoch 1827 @ step 2856000: Train Loss: 0.976401, Train Accuracy: 0.657688\n",
            "  Epoch 1827 @ step 2857000: Train Loss: 0.983071, Train Accuracy: 0.653688\n",
            "Epoch 1827 Test Loss: 1.005006, Test Accuracy: 0.648263, time: 7.7s\n",
            "  Epoch 1828 @ step 2858000: Train Loss: 0.970329, Train Accuracy: 0.662219\n",
            "Epoch 1828 Test Loss: 1.005864, Test Accuracy: 0.651258, time: 7.6s\n",
            "  Epoch 1829 @ step 2859000: Train Loss: 0.981827, Train Accuracy: 0.659125\n",
            "  Epoch 1829 @ step 2860000: Train Loss: 0.986850, Train Accuracy: 0.652156\n",
            "Epoch 1829 Test Loss: 1.002675, Test Accuracy: 0.648962, time: 7.7s\n",
            "  Epoch 1830 @ step 2861000: Train Loss: 0.977651, Train Accuracy: 0.656719\n",
            "Epoch 1830 Test Loss: 1.025996, Test Accuracy: 0.636981, time: 8.6s\n",
            "  Epoch 1831 @ step 2862000: Train Loss: 0.972623, Train Accuracy: 0.658375\n",
            "  Epoch 1831 @ step 2863000: Train Loss: 0.980327, Train Accuracy: 0.658656\n",
            "Epoch 1831 Test Loss: 1.032256, Test Accuracy: 0.633187, time: 7.7s\n",
            "  Epoch 1832 @ step 2864000: Train Loss: 0.974793, Train Accuracy: 0.659563\n",
            "Epoch 1832 Test Loss: 1.079018, Test Accuracy: 0.618710, time: 7.6s\n",
            "  Epoch 1833 @ step 2865000: Train Loss: 0.988019, Train Accuracy: 0.655125\n",
            "  Epoch 1833 @ step 2866000: Train Loss: 0.974438, Train Accuracy: 0.659500\n",
            "Epoch 1833 Test Loss: 0.992861, Test Accuracy: 0.650359, time: 8.7s\n",
            "  Epoch 1834 @ step 2867000: Train Loss: 0.976231, Train Accuracy: 0.660281\n",
            "  Epoch 1834 @ step 2868000: Train Loss: 0.981982, Train Accuracy: 0.657469\n",
            "Epoch 1834 Test Loss: 1.137507, Test Accuracy: 0.595547, time: 7.7s\n",
            "  Epoch 1835 @ step 2869000: Train Loss: 0.973673, Train Accuracy: 0.661719\n",
            "Epoch 1835 Test Loss: 1.076773, Test Accuracy: 0.624301, time: 7.9s\n",
            "  Epoch 1836 @ step 2870000: Train Loss: 0.984691, Train Accuracy: 0.653125\n",
            "  Epoch 1836 @ step 2871000: Train Loss: 0.976781, Train Accuracy: 0.656406\n",
            "Epoch 1836 Test Loss: 0.990586, Test Accuracy: 0.655351, time: 8.2s\n",
            "  Epoch 1837 @ step 2872000: Train Loss: 0.973990, Train Accuracy: 0.657844\n",
            "Epoch 1837 Test Loss: 0.995238, Test Accuracy: 0.649461, time: 7.5s\n",
            "  Epoch 1838 @ step 2873000: Train Loss: 0.983897, Train Accuracy: 0.658875\n",
            "  Epoch 1838 @ step 2874000: Train Loss: 0.976245, Train Accuracy: 0.656563\n",
            "Epoch 1838 Test Loss: 1.017112, Test Accuracy: 0.645268, time: 7.6s\n",
            "  Epoch 1839 @ step 2875000: Train Loss: 0.979187, Train Accuracy: 0.657938\n",
            "Epoch 1839 Test Loss: 0.980363, Test Accuracy: 0.659645, time: 7.4s\n",
            "  Epoch 1840 @ step 2876000: Train Loss: 0.978498, Train Accuracy: 0.661625\n",
            "  Epoch 1840 @ step 2877000: Train Loss: 0.978576, Train Accuracy: 0.657406\n",
            "Epoch 1840 Test Loss: 1.043356, Test Accuracy: 0.643570, time: 7.5s\n",
            "  Epoch 1841 @ step 2878000: Train Loss: 0.978953, Train Accuracy: 0.657250\n",
            "  Epoch 1841 @ step 2879000: Train Loss: 0.985252, Train Accuracy: 0.652563\n",
            "Epoch 1841 Test Loss: 1.247266, Test Accuracy: 0.562001, time: 8.0s\n",
            "  Epoch 1842 @ step 2880000: Train Loss: 0.972658, Train Accuracy: 0.659000\n",
            "Epoch 1842 Test Loss: 1.030088, Test Accuracy: 0.634485, time: 8.0s\n",
            "  Epoch 1843 @ step 2881000: Train Loss: 0.975818, Train Accuracy: 0.660813\n",
            "  Epoch 1843 @ step 2882000: Train Loss: 0.990685, Train Accuracy: 0.653500\n",
            "Epoch 1843 Test Loss: 1.042332, Test Accuracy: 0.625599, time: 7.4s\n",
            "  Epoch 1844 @ step 2883000: Train Loss: 0.974511, Train Accuracy: 0.660219\n",
            "Epoch 1844 Test Loss: 1.273042, Test Accuracy: 0.550819, time: 7.4s\n",
            "  Epoch 1845 @ step 2884000: Train Loss: 0.979858, Train Accuracy: 0.656844\n",
            "  Epoch 1845 @ step 2885000: Train Loss: 0.984775, Train Accuracy: 0.653656\n",
            "Epoch 1845 Test Loss: 1.030573, Test Accuracy: 0.636981, time: 7.7s\n",
            "  Epoch 1846 @ step 2886000: Train Loss: 0.964233, Train Accuracy: 0.664344\n",
            "Epoch 1846 Test Loss: 1.075001, Test Accuracy: 0.615515, time: 7.7s\n",
            "  Epoch 1847 @ step 2887000: Train Loss: 0.982826, Train Accuracy: 0.654375\n",
            "  Epoch 1847 @ step 2888000: Train Loss: 0.983148, Train Accuracy: 0.654594\n",
            "Epoch 1847 Test Loss: 0.996310, Test Accuracy: 0.653754, time: 8.3s\n",
            "  Epoch 1848 @ step 2889000: Train Loss: 0.972373, Train Accuracy: 0.658594\n",
            "Epoch 1848 Test Loss: 0.993474, Test Accuracy: 0.646466, time: 7.9s\n",
            "  Epoch 1849 @ step 2890000: Train Loss: 0.987790, Train Accuracy: 0.652031\n",
            "  Epoch 1849 @ step 2891000: Train Loss: 0.978435, Train Accuracy: 0.657188\n",
            "Epoch 1849 Test Loss: 1.091739, Test Accuracy: 0.619010, time: 7.7s\n",
            "  Epoch 1850 @ step 2892000: Train Loss: 0.975409, Train Accuracy: 0.657969\n",
            "  Epoch 1850 @ step 2893000: Train Loss: 0.983348, Train Accuracy: 0.655563\n",
            "Epoch 1850 Test Loss: 1.126304, Test Accuracy: 0.605531, time: 7.4s\n",
            "  Epoch 1851 @ step 2894000: Train Loss: 0.978839, Train Accuracy: 0.656875\n",
            "Epoch 1851 Test Loss: 0.989816, Test Accuracy: 0.657149, time: 7.5s\n",
            "  Epoch 1852 @ step 2895000: Train Loss: 0.969972, Train Accuracy: 0.658031\n",
            "  Epoch 1852 @ step 2896000: Train Loss: 0.986079, Train Accuracy: 0.655969\n",
            "Epoch 1852 Test Loss: 1.031924, Test Accuracy: 0.641973, time: 7.4s\n",
            "  Epoch 1853 @ step 2897000: Train Loss: 0.974838, Train Accuracy: 0.658969\n",
            "Epoch 1853 Test Loss: 1.048297, Test Accuracy: 0.625699, time: 8.4s\n",
            "  Epoch 1854 @ step 2898000: Train Loss: 0.977885, Train Accuracy: 0.657656\n",
            "  Epoch 1854 @ step 2899000: Train Loss: 0.979833, Train Accuracy: 0.654594\n",
            "Epoch 1854 Test Loss: 0.995277, Test Accuracy: 0.651358, time: 7.4s\n",
            "  Epoch 1855 @ step 2900000: Train Loss: 0.979642, Train Accuracy: 0.659750\n",
            "Epoch 1855 Test Loss: 1.030240, Test Accuracy: 0.630591, time: 8.5s\n",
            "  Epoch 1856 @ step 2901000: Train Loss: 0.981310, Train Accuracy: 0.656656\n",
            "  Epoch 1856 @ step 2902000: Train Loss: 0.976582, Train Accuracy: 0.658031\n",
            "Epoch 1856 Test Loss: 1.236788, Test Accuracy: 0.569289, time: 7.4s\n",
            "  Epoch 1857 @ step 2903000: Train Loss: 0.976314, Train Accuracy: 0.658438\n",
            "  Epoch 1857 @ step 2904000: Train Loss: 0.982324, Train Accuracy: 0.657188\n",
            "Epoch 1857 Test Loss: 0.996055, Test Accuracy: 0.654852, time: 7.4s\n",
            "  Epoch 1858 @ step 2905000: Train Loss: 0.974354, Train Accuracy: 0.657719\n",
            "Epoch 1858 Test Loss: 1.086061, Test Accuracy: 0.621006, time: 7.5s\n",
            "  Epoch 1859 @ step 2906000: Train Loss: 0.977995, Train Accuracy: 0.654781\n",
            "  Epoch 1859 @ step 2907000: Train Loss: 0.977970, Train Accuracy: 0.656531\n",
            "Epoch 1859 Test Loss: 1.141112, Test Accuracy: 0.588658, time: 8.5s\n",
            "  Epoch 1860 @ step 2908000: Train Loss: 0.974905, Train Accuracy: 0.660344\n",
            "Epoch 1860 Test Loss: 1.122846, Test Accuracy: 0.587560, time: 7.5s\n",
            "  Epoch 1861 @ step 2909000: Train Loss: 0.975261, Train Accuracy: 0.661156\n",
            "  Epoch 1861 @ step 2910000: Train Loss: 0.979161, Train Accuracy: 0.655250\n",
            "Epoch 1861 Test Loss: 1.096076, Test Accuracy: 0.592752, time: 7.4s\n",
            "  Epoch 1862 @ step 2911000: Train Loss: 0.976701, Train Accuracy: 0.657625\n",
            "Epoch 1862 Test Loss: 1.007759, Test Accuracy: 0.641074, time: 7.5s\n",
            "  Epoch 1863 @ step 2912000: Train Loss: 0.984359, Train Accuracy: 0.655250\n",
            "  Epoch 1863 @ step 2913000: Train Loss: 0.967800, Train Accuracy: 0.663250\n",
            "Epoch 1863 Test Loss: 1.041005, Test Accuracy: 0.633486, time: 7.8s\n",
            "  Epoch 1864 @ step 2914000: Train Loss: 0.974874, Train Accuracy: 0.659500\n",
            "Epoch 1864 Test Loss: 0.986608, Test Accuracy: 0.657548, time: 7.7s\n",
            "  Epoch 1865 @ step 2915000: Train Loss: 0.989763, Train Accuracy: 0.651719\n",
            "  Epoch 1865 @ step 2916000: Train Loss: 0.971163, Train Accuracy: 0.658906\n",
            "Epoch 1865 Test Loss: 1.045415, Test Accuracy: 0.626797, time: 8.3s\n",
            "  Epoch 1866 @ step 2917000: Train Loss: 0.979387, Train Accuracy: 0.657500\n",
            "  Epoch 1866 @ step 2918000: Train Loss: 0.982868, Train Accuracy: 0.652656\n",
            "Epoch 1866 Test Loss: 0.988244, Test Accuracy: 0.654852, time: 7.4s\n",
            "  Epoch 1867 @ step 2919000: Train Loss: 0.982945, Train Accuracy: 0.654875\n",
            "Epoch 1867 Test Loss: 1.048794, Test Accuracy: 0.622804, time: 7.5s\n",
            "  Epoch 1868 @ step 2920000: Train Loss: 0.966972, Train Accuracy: 0.659469\n",
            "  Epoch 1868 @ step 2921000: Train Loss: 0.978960, Train Accuracy: 0.660063\n",
            "Epoch 1868 Test Loss: 1.076395, Test Accuracy: 0.621206, time: 7.5s\n",
            "  Epoch 1869 @ step 2922000: Train Loss: 0.976095, Train Accuracy: 0.656281\n",
            "Epoch 1869 Test Loss: 0.979451, Test Accuracy: 0.653355, time: 7.5s\n",
            "  Epoch 1870 @ step 2923000: Train Loss: 0.987148, Train Accuracy: 0.653500\n",
            "  Epoch 1870 @ step 2924000: Train Loss: 0.980736, Train Accuracy: 0.658000\n",
            "Epoch 1870 Test Loss: 1.052407, Test Accuracy: 0.628295, time: 7.8s\n",
            "  Epoch 1871 @ step 2925000: Train Loss: 0.968663, Train Accuracy: 0.659688\n",
            "Epoch 1871 Test Loss: 1.042650, Test Accuracy: 0.631989, time: 8.2s\n",
            "  Epoch 1872 @ step 2926000: Train Loss: 0.988232, Train Accuracy: 0.654219\n",
            "  Epoch 1872 @ step 2927000: Train Loss: 0.975035, Train Accuracy: 0.658063\n",
            "Epoch 1872 Test Loss: 0.989659, Test Accuracy: 0.649860, time: 7.5s\n",
            "  Epoch 1873 @ step 2928000: Train Loss: 0.982645, Train Accuracy: 0.656813\n",
            "  Epoch 1873 @ step 2929000: Train Loss: 0.976781, Train Accuracy: 0.657969\n",
            "Epoch 1873 Test Loss: 1.052937, Test Accuracy: 0.621206, time: 7.5s\n",
            "  Epoch 1874 @ step 2930000: Train Loss: 0.969021, Train Accuracy: 0.661781\n",
            "Epoch 1874 Test Loss: 1.065823, Test Accuracy: 0.620208, time: 7.5s\n",
            "  Epoch 1875 @ step 2931000: Train Loss: 0.980495, Train Accuracy: 0.656750\n",
            "  Epoch 1875 @ step 2932000: Train Loss: 0.982159, Train Accuracy: 0.654500\n",
            "Epoch 1875 Test Loss: 1.057617, Test Accuracy: 0.636581, time: 7.5s\n",
            "  Epoch 1876 @ step 2933000: Train Loss: 0.977540, Train Accuracy: 0.658156\n",
            "Epoch 1876 Test Loss: 1.103423, Test Accuracy: 0.607827, time: 7.9s\n",
            "  Epoch 1877 @ step 2934000: Train Loss: 0.979278, Train Accuracy: 0.656500\n",
            "  Epoch 1877 @ step 2935000: Train Loss: 0.977515, Train Accuracy: 0.657563\n",
            "Epoch 1877 Test Loss: 1.261309, Test Accuracy: 0.562300, time: 8.2s\n",
            "  Epoch 1878 @ step 2936000: Train Loss: 0.980222, Train Accuracy: 0.656719\n",
            "Epoch 1878 Test Loss: 1.050640, Test Accuracy: 0.630591, time: 8.3s\n",
            "  Epoch 1879 @ step 2937000: Train Loss: 0.979472, Train Accuracy: 0.655156\n",
            "  Epoch 1879 @ step 2938000: Train Loss: 0.982036, Train Accuracy: 0.653188\n",
            "Epoch 1879 Test Loss: 1.024889, Test Accuracy: 0.635583, time: 7.5s\n",
            "  Epoch 1880 @ step 2939000: Train Loss: 0.977644, Train Accuracy: 0.661188\n",
            "  Epoch 1880 @ step 2940000: Train Loss: 0.981848, Train Accuracy: 0.655750\n",
            "Epoch 1880 Test Loss: 0.980546, Test Accuracy: 0.660843, time: 7.5s\n",
            "  Epoch 1881 @ step 2941000: Train Loss: 0.978644, Train Accuracy: 0.661438\n",
            "Epoch 1881 Test Loss: 1.082646, Test Accuracy: 0.615915, time: 7.5s\n",
            "  Epoch 1882 @ step 2942000: Train Loss: 0.975855, Train Accuracy: 0.658969\n",
            "  Epoch 1882 @ step 2943000: Train Loss: 0.981002, Train Accuracy: 0.657750\n",
            "Epoch 1882 Test Loss: 0.979003, Test Accuracy: 0.659645, time: 8.5s\n",
            "  Epoch 1883 @ step 2944000: Train Loss: 0.969573, Train Accuracy: 0.658563\n",
            "Epoch 1883 Test Loss: 1.040049, Test Accuracy: 0.632987, time: 7.5s\n",
            "  Epoch 1884 @ step 2945000: Train Loss: 0.980120, Train Accuracy: 0.658469\n",
            "  Epoch 1884 @ step 2946000: Train Loss: 0.980373, Train Accuracy: 0.655438\n",
            "Epoch 1884 Test Loss: 1.047309, Test Accuracy: 0.631090, time: 7.5s\n",
            "  Epoch 1885 @ step 2947000: Train Loss: 0.979287, Train Accuracy: 0.659563\n",
            "Epoch 1885 Test Loss: 1.044288, Test Accuracy: 0.634085, time: 7.8s\n",
            "  Epoch 1886 @ step 2948000: Train Loss: 0.977326, Train Accuracy: 0.658188\n",
            "  Epoch 1886 @ step 2949000: Train Loss: 0.977294, Train Accuracy: 0.657781\n",
            "Epoch 1886 Test Loss: 1.036495, Test Accuracy: 0.625300, time: 7.7s\n",
            "  Epoch 1887 @ step 2950000: Train Loss: 0.984058, Train Accuracy: 0.655406\n",
            "Epoch 1887 Test Loss: 0.959578, Test Accuracy: 0.666134, time: 7.5s\n",
            "  Epoch 1888 @ step 2951000: Train Loss: 0.979988, Train Accuracy: 0.657344\n",
            "  Epoch 1888 @ step 2952000: Train Loss: 0.974835, Train Accuracy: 0.656938\n",
            "Epoch 1888 Test Loss: 1.052902, Test Accuracy: 0.624401, time: 8.6s\n",
            "  Epoch 1889 @ step 2953000: Train Loss: 0.980320, Train Accuracy: 0.654813\n",
            "  Epoch 1889 @ step 2954000: Train Loss: 0.983737, Train Accuracy: 0.653313\n",
            "Epoch 1889 Test Loss: 1.005460, Test Accuracy: 0.648562, time: 7.5s\n",
            "  Epoch 1890 @ step 2955000: Train Loss: 0.973601, Train Accuracy: 0.659094\n",
            "Epoch 1890 Test Loss: 1.011250, Test Accuracy: 0.639776, time: 7.5s\n",
            "  Epoch 1891 @ step 2956000: Train Loss: 0.976308, Train Accuracy: 0.658031\n",
            "  Epoch 1891 @ step 2957000: Train Loss: 0.986364, Train Accuracy: 0.654531\n",
            "Epoch 1891 Test Loss: 0.983145, Test Accuracy: 0.652855, time: 7.6s\n",
            "  Epoch 1892 @ step 2958000: Train Loss: 0.977130, Train Accuracy: 0.655219\n",
            "Epoch 1892 Test Loss: 1.004916, Test Accuracy: 0.651558, time: 7.5s\n",
            "  Epoch 1893 @ step 2959000: Train Loss: 0.976599, Train Accuracy: 0.657063\n",
            "  Epoch 1893 @ step 2960000: Train Loss: 0.979927, Train Accuracy: 0.660438\n",
            "Epoch 1893 Test Loss: 0.981176, Test Accuracy: 0.657548, time: 7.6s\n",
            "  Epoch 1894 @ step 2961000: Train Loss: 0.974754, Train Accuracy: 0.658063\n",
            "Epoch 1894 Test Loss: 1.065509, Test Accuracy: 0.625399, time: 8.7s\n",
            "  Epoch 1895 @ step 2962000: Train Loss: 0.986866, Train Accuracy: 0.655375\n",
            "  Epoch 1895 @ step 2963000: Train Loss: 0.975101, Train Accuracy: 0.655938\n",
            "Epoch 1895 Test Loss: 1.113376, Test Accuracy: 0.616713, time: 7.6s\n",
            "  Epoch 1896 @ step 2964000: Train Loss: 0.975343, Train Accuracy: 0.659563\n",
            "  Epoch 1896 @ step 2965000: Train Loss: 0.986193, Train Accuracy: 0.653625\n",
            "Epoch 1896 Test Loss: 1.007986, Test Accuracy: 0.643970, time: 7.4s\n",
            "  Epoch 1897 @ step 2966000: Train Loss: 0.976705, Train Accuracy: 0.656875\n",
            "Epoch 1897 Test Loss: 1.098934, Test Accuracy: 0.624601, time: 7.4s\n",
            "  Epoch 1898 @ step 2967000: Train Loss: 0.981879, Train Accuracy: 0.653250\n",
            "  Epoch 1898 @ step 2968000: Train Loss: 0.983024, Train Accuracy: 0.655594\n",
            "Epoch 1898 Test Loss: 1.036586, Test Accuracy: 0.629093, time: 7.5s\n",
            "  Epoch 1899 @ step 2969000: Train Loss: 0.974904, Train Accuracy: 0.661125\n",
            "Epoch 1899 Test Loss: 1.057808, Test Accuracy: 0.622903, time: 7.5s\n",
            "  Epoch 1900 @ step 2970000: Train Loss: 0.976118, Train Accuracy: 0.657219\n",
            "  Epoch 1900 @ step 2971000: Train Loss: 0.980660, Train Accuracy: 0.655156\n",
            "Epoch 1900 Test Loss: 0.959139, Test Accuracy: 0.673722, time: 9.1s\n",
            "  Epoch 1901 @ step 2972000: Train Loss: 0.973149, Train Accuracy: 0.661531\n",
            "Epoch 1901 Test Loss: 1.008475, Test Accuracy: 0.644768, time: 7.4s\n",
            "  Epoch 1902 @ step 2973000: Train Loss: 0.988194, Train Accuracy: 0.654125\n",
            "  Epoch 1902 @ step 2974000: Train Loss: 0.983364, Train Accuracy: 0.653563\n",
            "Epoch 1902 Test Loss: 0.968323, Test Accuracy: 0.662839, time: 7.4s\n",
            "  Epoch 1903 @ step 2975000: Train Loss: 0.975445, Train Accuracy: 0.656438\n",
            "Epoch 1903 Test Loss: 0.991401, Test Accuracy: 0.653554, time: 7.5s\n",
            "  Epoch 1904 @ step 2976000: Train Loss: 0.984281, Train Accuracy: 0.655625\n",
            "  Epoch 1904 @ step 2977000: Train Loss: 0.971939, Train Accuracy: 0.657344\n",
            "Epoch 1904 Test Loss: 1.069150, Test Accuracy: 0.617612, time: 7.5s\n",
            "  Epoch 1905 @ step 2978000: Train Loss: 0.985979, Train Accuracy: 0.652250\n",
            "  Epoch 1905 @ step 2979000: Train Loss: 0.976009, Train Accuracy: 0.658156\n",
            "Epoch 1905 Test Loss: 1.012641, Test Accuracy: 0.645068, time: 7.8s\n",
            "  Epoch 1906 @ step 2980000: Train Loss: 0.971369, Train Accuracy: 0.660094\n",
            "Epoch 1906 Test Loss: 1.160759, Test Accuracy: 0.588658, time: 8.1s\n",
            "  Epoch 1907 @ step 2981000: Train Loss: 0.984140, Train Accuracy: 0.656188\n",
            "  Epoch 1907 @ step 2982000: Train Loss: 0.980631, Train Accuracy: 0.655813\n",
            "Epoch 1907 Test Loss: 0.993926, Test Accuracy: 0.649062, time: 7.6s\n",
            "  Epoch 1908 @ step 2983000: Train Loss: 0.967024, Train Accuracy: 0.661813\n",
            "Epoch 1908 Test Loss: 0.992622, Test Accuracy: 0.654353, time: 7.9s\n",
            "  Epoch 1909 @ step 2984000: Train Loss: 0.984831, Train Accuracy: 0.658063\n",
            "  Epoch 1909 @ step 2985000: Train Loss: 0.983808, Train Accuracy: 0.655219\n",
            "Epoch 1909 Test Loss: 1.067591, Test Accuracy: 0.631190, time: 7.4s\n",
            "  Epoch 1910 @ step 2986000: Train Loss: 0.974058, Train Accuracy: 0.659594\n",
            "Epoch 1910 Test Loss: 1.038947, Test Accuracy: 0.630391, time: 7.5s\n",
            "  Epoch 1911 @ step 2987000: Train Loss: 0.983308, Train Accuracy: 0.654125\n",
            "  Epoch 1911 @ step 2988000: Train Loss: 0.977689, Train Accuracy: 0.657156\n",
            "Epoch 1911 Test Loss: 1.027299, Test Accuracy: 0.638279, time: 8.1s\n",
            "  Epoch 1912 @ step 2989000: Train Loss: 0.970894, Train Accuracy: 0.659219\n",
            "  Epoch 1912 @ step 2990000: Train Loss: 0.988158, Train Accuracy: 0.653094\n",
            "Epoch 1912 Test Loss: 1.025975, Test Accuracy: 0.646066, time: 7.8s\n",
            "  Epoch 1913 @ step 2991000: Train Loss: 0.977146, Train Accuracy: 0.657813\n",
            "Epoch 1913 Test Loss: 1.052292, Test Accuracy: 0.626498, time: 7.5s\n",
            "  Epoch 1914 @ step 2992000: Train Loss: 0.975190, Train Accuracy: 0.659750\n",
            "  Epoch 1914 @ step 2993000: Train Loss: 0.981605, Train Accuracy: 0.654063\n",
            "Epoch 1914 Test Loss: 1.018555, Test Accuracy: 0.646865, time: 7.5s\n",
            "  Epoch 1915 @ step 2994000: Train Loss: 0.978465, Train Accuracy: 0.657063\n",
            "Epoch 1915 Test Loss: 1.022424, Test Accuracy: 0.626997, time: 7.5s\n",
            "  Epoch 1916 @ step 2995000: Train Loss: 0.983818, Train Accuracy: 0.653719\n",
            "  Epoch 1916 @ step 2996000: Train Loss: 0.980539, Train Accuracy: 0.656156\n",
            "Epoch 1916 Test Loss: 1.002899, Test Accuracy: 0.646266, time: 7.5s\n",
            "  Epoch 1917 @ step 2997000: Train Loss: 0.970743, Train Accuracy: 0.660500\n",
            "Epoch 1917 Test Loss: 1.090926, Test Accuracy: 0.620607, time: 8.3s\n",
            "  Epoch 1918 @ step 2998000: Train Loss: 0.985485, Train Accuracy: 0.654188\n",
            "  Epoch 1918 @ step 2999000: Train Loss: 0.974411, Train Accuracy: 0.660000\n",
            "Epoch 1918 Test Loss: 1.006125, Test Accuracy: 0.643770, time: 7.6s\n",
            "  Epoch 1919 @ step 3000000: Train Loss: 0.978472, Train Accuracy: 0.658344\n",
            "Epoch 1919 Test Loss: 1.011456, Test Accuracy: 0.638978, time: 7.5s\n",
            "  Epoch 1920 @ step 3001000: Train Loss: 0.978033, Train Accuracy: 0.657500\n",
            "  Epoch 1920 @ step 3002000: Train Loss: 0.975946, Train Accuracy: 0.659750\n",
            "Epoch 1920 Test Loss: 1.054172, Test Accuracy: 0.621006, time: 7.5s\n",
            "  Epoch 1921 @ step 3003000: Train Loss: 0.968205, Train Accuracy: 0.660750\n",
            "  Epoch 1921 @ step 3004000: Train Loss: 0.991944, Train Accuracy: 0.652750\n",
            "Epoch 1921 Test Loss: 1.062225, Test Accuracy: 0.628794, time: 7.5s\n",
            "  Epoch 1922 @ step 3005000: Train Loss: 0.974974, Train Accuracy: 0.657719\n",
            "Epoch 1922 Test Loss: 1.042942, Test Accuracy: 0.624900, time: 7.5s\n",
            "  Epoch 1923 @ step 3006000: Train Loss: 0.973370, Train Accuracy: 0.658750\n",
            "  Epoch 1923 @ step 3007000: Train Loss: 0.984405, Train Accuracy: 0.658313\n",
            "Epoch 1923 Test Loss: 1.082932, Test Accuracy: 0.609625, time: 9.1s\n",
            "  Epoch 1924 @ step 3008000: Train Loss: 0.977168, Train Accuracy: 0.658594\n",
            "Epoch 1924 Test Loss: 1.048960, Test Accuracy: 0.634784, time: 7.5s\n",
            "  Epoch 1925 @ step 3009000: Train Loss: 0.968987, Train Accuracy: 0.661563\n",
            "  Epoch 1925 @ step 3010000: Train Loss: 0.981045, Train Accuracy: 0.655531\n",
            "Epoch 1925 Test Loss: 1.094068, Test Accuracy: 0.613918, time: 7.4s\n",
            "  Epoch 1926 @ step 3011000: Train Loss: 0.979760, Train Accuracy: 0.657063\n",
            "Epoch 1926 Test Loss: 1.027551, Test Accuracy: 0.633486, time: 7.4s\n",
            "  Epoch 1927 @ step 3012000: Train Loss: 0.974320, Train Accuracy: 0.660750\n",
            "  Epoch 1927 @ step 3013000: Train Loss: 0.980577, Train Accuracy: 0.658125\n",
            "Epoch 1927 Test Loss: 0.998021, Test Accuracy: 0.651258, time: 7.4s\n",
            "  Epoch 1928 @ step 3014000: Train Loss: 0.964938, Train Accuracy: 0.660938\n",
            "  Epoch 1928 @ step 3015000: Train Loss: 0.992097, Train Accuracy: 0.651906\n",
            "Epoch 1928 Test Loss: 1.068866, Test Accuracy: 0.617911, time: 7.4s\n",
            "  Epoch 1929 @ step 3016000: Train Loss: 0.977626, Train Accuracy: 0.657750\n",
            "Epoch 1929 Test Loss: 0.995022, Test Accuracy: 0.654453, time: 8.5s\n",
            "  Epoch 1930 @ step 3017000: Train Loss: 0.973918, Train Accuracy: 0.658188\n",
            "  Epoch 1930 @ step 3018000: Train Loss: 0.983513, Train Accuracy: 0.658156\n",
            "Epoch 1930 Test Loss: 1.009350, Test Accuracy: 0.645467, time: 8.0s\n",
            "  Epoch 1931 @ step 3019000: Train Loss: 0.979713, Train Accuracy: 0.656656\n",
            "Epoch 1931 Test Loss: 1.056123, Test Accuracy: 0.631490, time: 7.4s\n",
            "  Epoch 1932 @ step 3020000: Train Loss: 0.972521, Train Accuracy: 0.658781\n",
            "  Epoch 1932 @ step 3021000: Train Loss: 0.978644, Train Accuracy: 0.657250\n",
            "Epoch 1932 Test Loss: 1.013035, Test Accuracy: 0.650160, time: 7.4s\n",
            "  Epoch 1933 @ step 3022000: Train Loss: 0.978805, Train Accuracy: 0.655625\n",
            "Epoch 1933 Test Loss: 1.198992, Test Accuracy: 0.587959, time: 7.4s\n",
            "  Epoch 1934 @ step 3023000: Train Loss: 0.981651, Train Accuracy: 0.657219\n",
            "  Epoch 1934 @ step 3024000: Train Loss: 0.979034, Train Accuracy: 0.657688\n",
            "Epoch 1934 Test Loss: 0.997746, Test Accuracy: 0.648762, time: 7.4s\n",
            "  Epoch 1935 @ step 3025000: Train Loss: 0.975988, Train Accuracy: 0.656781\n",
            "Epoch 1935 Test Loss: 0.975971, Test Accuracy: 0.656150, time: 8.6s\n",
            "  Epoch 1936 @ step 3026000: Train Loss: 0.981968, Train Accuracy: 0.656594\n",
            "  Epoch 1936 @ step 3027000: Train Loss: 0.975258, Train Accuracy: 0.662500\n",
            "Epoch 1936 Test Loss: 1.098771, Test Accuracy: 0.608626, time: 7.5s\n",
            "  Epoch 1937 @ step 3028000: Train Loss: 0.980903, Train Accuracy: 0.658438\n",
            "  Epoch 1937 @ step 3029000: Train Loss: 0.985863, Train Accuracy: 0.652875\n",
            "Epoch 1937 Test Loss: 0.978568, Test Accuracy: 0.656450, time: 7.4s\n",
            "  Epoch 1938 @ step 3030000: Train Loss: 0.978278, Train Accuracy: 0.658531\n",
            "Epoch 1938 Test Loss: 1.000998, Test Accuracy: 0.649760, time: 7.4s\n",
            "  Epoch 1939 @ step 3031000: Train Loss: 0.973992, Train Accuracy: 0.655844\n",
            "  Epoch 1939 @ step 3032000: Train Loss: 0.977305, Train Accuracy: 0.659188\n",
            "Epoch 1939 Test Loss: 1.059529, Test Accuracy: 0.625599, time: 7.4s\n",
            "  Epoch 1940 @ step 3033000: Train Loss: 0.974929, Train Accuracy: 0.656094\n",
            "Epoch 1940 Test Loss: 0.999979, Test Accuracy: 0.647963, time: 7.6s\n",
            "  Epoch 1941 @ step 3034000: Train Loss: 0.986146, Train Accuracy: 0.654938\n",
            "  Epoch 1941 @ step 3035000: Train Loss: 0.977651, Train Accuracy: 0.658156\n",
            "Epoch 1941 Test Loss: 1.001421, Test Accuracy: 0.650359, time: 8.5s\n",
            "  Epoch 1942 @ step 3036000: Train Loss: 0.969994, Train Accuracy: 0.659813\n",
            "Epoch 1942 Test Loss: 0.991744, Test Accuracy: 0.652256, time: 7.4s\n",
            "  Epoch 1943 @ step 3037000: Train Loss: 0.986199, Train Accuracy: 0.655156\n",
            "  Epoch 1943 @ step 3038000: Train Loss: 0.981075, Train Accuracy: 0.654375\n",
            "Epoch 1943 Test Loss: 1.006853, Test Accuracy: 0.641074, time: 7.4s\n",
            "  Epoch 1944 @ step 3039000: Train Loss: 0.973228, Train Accuracy: 0.656906\n",
            "  Epoch 1944 @ step 3040000: Train Loss: 0.984533, Train Accuracy: 0.655063\n",
            "Epoch 1944 Test Loss: 0.984822, Test Accuracy: 0.656849, time: 7.5s\n",
            "  Epoch 1945 @ step 3041000: Train Loss: 0.974653, Train Accuracy: 0.658281\n",
            "Epoch 1945 Test Loss: 1.009216, Test Accuracy: 0.655252, time: 8.2s\n",
            "  Epoch 1946 @ step 3042000: Train Loss: 0.977043, Train Accuracy: 0.657000\n",
            "  Epoch 1946 @ step 3043000: Train Loss: 0.980868, Train Accuracy: 0.657375\n",
            "Epoch 1946 Test Loss: 0.988505, Test Accuracy: 0.655751, time: 8.2s\n",
            "  Epoch 1947 @ step 3044000: Train Loss: 0.973914, Train Accuracy: 0.661375\n",
            "Epoch 1947 Test Loss: 1.023030, Test Accuracy: 0.645767, time: 7.9s\n",
            "  Epoch 1948 @ step 3045000: Train Loss: 0.980964, Train Accuracy: 0.658750\n",
            "  Epoch 1948 @ step 3046000: Train Loss: 0.977889, Train Accuracy: 0.658281\n",
            "Epoch 1948 Test Loss: 1.009771, Test Accuracy: 0.637081, time: 7.3s\n",
            "  Epoch 1949 @ step 3047000: Train Loss: 0.983597, Train Accuracy: 0.657219\n",
            "Epoch 1949 Test Loss: 1.127565, Test Accuracy: 0.607827, time: 7.4s\n",
            "  Epoch 1950 @ step 3048000: Train Loss: 0.975077, Train Accuracy: 0.657906\n",
            "  Epoch 1950 @ step 3049000: Train Loss: 0.976058, Train Accuracy: 0.657125\n",
            "Epoch 1950 Test Loss: 1.020421, Test Accuracy: 0.642272, time: 7.3s\n",
            "  Epoch 1951 @ step 3050000: Train Loss: 0.981158, Train Accuracy: 0.657406\n",
            "Epoch 1951 Test Loss: 1.043810, Test Accuracy: 0.624401, time: 7.4s\n",
            "  Epoch 1952 @ step 3051000: Train Loss: 0.982423, Train Accuracy: 0.656469\n",
            "  Epoch 1952 @ step 3052000: Train Loss: 0.971400, Train Accuracy: 0.660438\n",
            "Epoch 1952 Test Loss: 1.000969, Test Accuracy: 0.647664, time: 8.5s\n",
            "  Epoch 1953 @ step 3053000: Train Loss: 0.975641, Train Accuracy: 0.656813\n",
            "  Epoch 1953 @ step 3054000: Train Loss: 0.982280, Train Accuracy: 0.656750\n",
            "Epoch 1953 Test Loss: 1.014722, Test Accuracy: 0.649661, time: 7.8s\n",
            "  Epoch 1954 @ step 3055000: Train Loss: 0.970257, Train Accuracy: 0.659156\n",
            "Epoch 1954 Test Loss: 1.113765, Test Accuracy: 0.609724, time: 7.3s\n",
            "  Epoch 1955 @ step 3056000: Train Loss: 0.978561, Train Accuracy: 0.657344\n",
            "  Epoch 1955 @ step 3057000: Train Loss: 0.990242, Train Accuracy: 0.655281\n",
            "Epoch 1955 Test Loss: 1.017786, Test Accuracy: 0.638878, time: 7.4s\n",
            "  Epoch 1956 @ step 3058000: Train Loss: 0.972624, Train Accuracy: 0.661188\n",
            "Epoch 1956 Test Loss: 1.038262, Test Accuracy: 0.637380, time: 7.4s\n",
            "  Epoch 1957 @ step 3059000: Train Loss: 0.976257, Train Accuracy: 0.657750\n",
            "  Epoch 1957 @ step 3060000: Train Loss: 0.978551, Train Accuracy: 0.655375\n",
            "Epoch 1957 Test Loss: 1.083797, Test Accuracy: 0.603734, time: 7.6s\n",
            "  Epoch 1958 @ step 3061000: Train Loss: 0.979319, Train Accuracy: 0.657625\n",
            "Epoch 1958 Test Loss: 0.970687, Test Accuracy: 0.665935, time: 8.4s\n",
            "  Epoch 1959 @ step 3062000: Train Loss: 0.979365, Train Accuracy: 0.656531\n",
            "  Epoch 1959 @ step 3063000: Train Loss: 0.974232, Train Accuracy: 0.657531\n",
            "Epoch 1959 Test Loss: 1.170191, Test Accuracy: 0.591953, time: 7.4s\n",
            "  Epoch 1960 @ step 3064000: Train Loss: 0.985753, Train Accuracy: 0.655656\n",
            "  Epoch 1960 @ step 3065000: Train Loss: 0.978172, Train Accuracy: 0.656938\n",
            "Epoch 1960 Test Loss: 1.004536, Test Accuracy: 0.648462, time: 7.4s\n",
            "  Epoch 1961 @ step 3066000: Train Loss: 0.970569, Train Accuracy: 0.659781\n",
            "Epoch 1961 Test Loss: 1.019730, Test Accuracy: 0.650160, time: 7.3s\n",
            "  Epoch 1962 @ step 3067000: Train Loss: 0.979505, Train Accuracy: 0.658156\n",
            "  Epoch 1962 @ step 3068000: Train Loss: 0.979490, Train Accuracy: 0.654469\n",
            "Epoch 1962 Test Loss: 1.108973, Test Accuracy: 0.612021, time: 7.4s\n",
            "  Epoch 1963 @ step 3069000: Train Loss: 0.972063, Train Accuracy: 0.659531\n",
            "Epoch 1963 Test Loss: 0.982169, Test Accuracy: 0.655252, time: 7.3s\n",
            "  Epoch 1964 @ step 3070000: Train Loss: 0.978446, Train Accuracy: 0.659063\n",
            "  Epoch 1964 @ step 3071000: Train Loss: 0.982660, Train Accuracy: 0.657094\n",
            "Epoch 1964 Test Loss: 1.042213, Test Accuracy: 0.635284, time: 8.4s\n",
            "  Epoch 1965 @ step 3072000: Train Loss: 0.986252, Train Accuracy: 0.655250\n",
            "Epoch 1965 Test Loss: 0.991400, Test Accuracy: 0.654653, time: 7.4s\n",
            "  Epoch 1966 @ step 3073000: Train Loss: 0.977940, Train Accuracy: 0.656406\n",
            "  Epoch 1966 @ step 3074000: Train Loss: 0.979762, Train Accuracy: 0.655281\n",
            "Epoch 1966 Test Loss: 1.024838, Test Accuracy: 0.638678, time: 7.4s\n",
            "  Epoch 1967 @ step 3075000: Train Loss: 0.971994, Train Accuracy: 0.658844\n",
            "Epoch 1967 Test Loss: 1.028984, Test Accuracy: 0.637879, time: 7.5s\n",
            "  Epoch 1968 @ step 3076000: Train Loss: 0.983148, Train Accuracy: 0.654313\n",
            "  Epoch 1968 @ step 3077000: Train Loss: 0.967903, Train Accuracy: 0.660656\n",
            "Epoch 1968 Test Loss: 1.025127, Test Accuracy: 0.637879, time: 8.7s\n",
            "  Epoch 1969 @ step 3078000: Train Loss: 0.987174, Train Accuracy: 0.656563\n",
            "  Epoch 1969 @ step 3079000: Train Loss: 0.980380, Train Accuracy: 0.655313\n",
            "Epoch 1969 Test Loss: 1.035135, Test Accuracy: 0.640875, time: 7.6s\n",
            "  Epoch 1970 @ step 3080000: Train Loss: 0.965696, Train Accuracy: 0.661750\n",
            "Epoch 1970 Test Loss: 1.032588, Test Accuracy: 0.640276, time: 8.5s\n",
            "  Epoch 1971 @ step 3081000: Train Loss: 0.984095, Train Accuracy: 0.655719\n",
            "  Epoch 1971 @ step 3082000: Train Loss: 0.975056, Train Accuracy: 0.658750\n",
            "Epoch 1971 Test Loss: 1.063455, Test Accuracy: 0.633287, time: 7.6s\n",
            "  Epoch 1972 @ step 3083000: Train Loss: 0.977135, Train Accuracy: 0.655375\n",
            "Epoch 1972 Test Loss: 0.992271, Test Accuracy: 0.650260, time: 7.4s\n",
            "  Epoch 1973 @ step 3084000: Train Loss: 0.978689, Train Accuracy: 0.656813\n",
            "  Epoch 1973 @ step 3085000: Train Loss: 0.981873, Train Accuracy: 0.656188\n",
            "Epoch 1973 Test Loss: 1.023102, Test Accuracy: 0.645667, time: 7.5s\n",
            "  Epoch 1974 @ step 3086000: Train Loss: 0.978911, Train Accuracy: 0.658375\n",
            "Epoch 1974 Test Loss: 1.118208, Test Accuracy: 0.607628, time: 8.0s\n",
            "  Epoch 1975 @ step 3087000: Train Loss: 0.977110, Train Accuracy: 0.654781\n",
            "  Epoch 1975 @ step 3088000: Train Loss: 0.979316, Train Accuracy: 0.656906\n",
            "Epoch 1975 Test Loss: 1.054872, Test Accuracy: 0.627196, time: 7.6s\n",
            "  Epoch 1976 @ step 3089000: Train Loss: 0.973805, Train Accuracy: 0.658031\n",
            "  Epoch 1976 @ step 3090000: Train Loss: 0.984735, Train Accuracy: 0.655719\n",
            "Epoch 1976 Test Loss: 1.050268, Test Accuracy: 0.622005, time: 8.3s\n",
            "  Epoch 1977 @ step 3091000: Train Loss: 0.976591, Train Accuracy: 0.659531\n",
            "Epoch 1977 Test Loss: 1.109495, Test Accuracy: 0.595447, time: 7.4s\n",
            "  Epoch 1978 @ step 3092000: Train Loss: 0.974407, Train Accuracy: 0.658344\n",
            "  Epoch 1978 @ step 3093000: Train Loss: 0.982401, Train Accuracy: 0.656781\n",
            "Epoch 1978 Test Loss: 1.014581, Test Accuracy: 0.641573, time: 7.6s\n",
            "  Epoch 1979 @ step 3094000: Train Loss: 0.976464, Train Accuracy: 0.656188\n",
            "Epoch 1979 Test Loss: 1.018531, Test Accuracy: 0.641474, time: 7.5s\n",
            "  Epoch 1980 @ step 3095000: Train Loss: 0.976878, Train Accuracy: 0.660344\n",
            "  Epoch 1980 @ step 3096000: Train Loss: 0.978933, Train Accuracy: 0.658938\n",
            "Epoch 1980 Test Loss: 1.094863, Test Accuracy: 0.613319, time: 7.5s\n",
            "  Epoch 1981 @ step 3097000: Train Loss: 0.977018, Train Accuracy: 0.660219\n",
            "Epoch 1981 Test Loss: 1.083566, Test Accuracy: 0.611322, time: 7.7s\n",
            "  Epoch 1982 @ step 3098000: Train Loss: 0.982155, Train Accuracy: 0.656031\n",
            "  Epoch 1982 @ step 3099000: Train Loss: 0.978795, Train Accuracy: 0.656656\n",
            "Epoch 1982 Test Loss: 1.117733, Test Accuracy: 0.607728, time: 8.2s\n",
            "  Epoch 1983 @ step 3100000: Train Loss: 0.977090, Train Accuracy: 0.659656\n",
            "Epoch 1983 Test Loss: 1.002664, Test Accuracy: 0.640176, time: 7.5s\n",
            "  Epoch 1984 @ step 3101000: Train Loss: 0.987891, Train Accuracy: 0.654250\n",
            "  Epoch 1984 @ step 3102000: Train Loss: 0.974276, Train Accuracy: 0.658188\n",
            "Epoch 1984 Test Loss: 1.015848, Test Accuracy: 0.644169, time: 7.5s\n",
            "  Epoch 1985 @ step 3103000: Train Loss: 0.972741, Train Accuracy: 0.661000\n",
            "  Epoch 1985 @ step 3104000: Train Loss: 0.983478, Train Accuracy: 0.653938\n",
            "Epoch 1985 Test Loss: 1.003101, Test Accuracy: 0.646166, time: 7.5s\n",
            "  Epoch 1986 @ step 3105000: Train Loss: 0.975684, Train Accuracy: 0.658938\n",
            "Epoch 1986 Test Loss: 1.235076, Test Accuracy: 0.564097, time: 7.4s\n",
            "  Epoch 1987 @ step 3106000: Train Loss: 0.976954, Train Accuracy: 0.662031\n",
            "  Epoch 1987 @ step 3107000: Train Loss: 0.978381, Train Accuracy: 0.657719\n",
            "Epoch 1987 Test Loss: 1.099071, Test Accuracy: 0.621006, time: 7.9s\n",
            "  Epoch 1988 @ step 3108000: Train Loss: 0.982590, Train Accuracy: 0.656313\n",
            "Epoch 1988 Test Loss: 1.004526, Test Accuracy: 0.653355, time: 7.9s\n",
            "  Epoch 1989 @ step 3109000: Train Loss: 0.976731, Train Accuracy: 0.658406\n",
            "  Epoch 1989 @ step 3110000: Train Loss: 0.975431, Train Accuracy: 0.658344\n",
            "Epoch 1989 Test Loss: 1.181924, Test Accuracy: 0.576478, time: 7.4s\n",
            "  Epoch 1990 @ step 3111000: Train Loss: 0.978421, Train Accuracy: 0.655563\n",
            "Epoch 1990 Test Loss: 1.191960, Test Accuracy: 0.586861, time: 8.0s\n",
            "  Epoch 1991 @ step 3112000: Train Loss: 0.982056, Train Accuracy: 0.657594\n",
            "  Epoch 1991 @ step 3113000: Train Loss: 0.976631, Train Accuracy: 0.658000\n",
            "Epoch 1991 Test Loss: 0.987593, Test Accuracy: 0.651957, time: 8.0s\n",
            "  Epoch 1992 @ step 3114000: Train Loss: 0.977978, Train Accuracy: 0.658875\n",
            "  Epoch 1992 @ step 3115000: Train Loss: 0.984346, Train Accuracy: 0.656969\n",
            "Epoch 1992 Test Loss: 0.972062, Test Accuracy: 0.661941, time: 7.4s\n",
            "  Epoch 1993 @ step 3116000: Train Loss: 0.970447, Train Accuracy: 0.659781\n",
            "Epoch 1993 Test Loss: 1.056578, Test Accuracy: 0.631090, time: 8.4s\n",
            "  Epoch 1994 @ step 3117000: Train Loss: 0.982275, Train Accuracy: 0.655844\n",
            "  Epoch 1994 @ step 3118000: Train Loss: 0.974587, Train Accuracy: 0.658844\n",
            "Epoch 1994 Test Loss: 0.998238, Test Accuracy: 0.657448, time: 7.5s\n",
            "  Epoch 1995 @ step 3119000: Train Loss: 0.978472, Train Accuracy: 0.657063\n",
            "Epoch 1995 Test Loss: 1.064410, Test Accuracy: 0.625399, time: 7.5s\n",
            "  Epoch 1996 @ step 3120000: Train Loss: 0.978798, Train Accuracy: 0.658844\n",
            "  Epoch 1996 @ step 3121000: Train Loss: 0.982488, Train Accuracy: 0.656875\n",
            "Epoch 1996 Test Loss: 1.028438, Test Accuracy: 0.645268, time: 7.9s\n",
            "  Epoch 1997 @ step 3122000: Train Loss: 0.969215, Train Accuracy: 0.660250\n",
            "Epoch 1997 Test Loss: 1.033215, Test Accuracy: 0.625100, time: 7.4s\n",
            "  Epoch 1998 @ step 3123000: Train Loss: 0.981999, Train Accuracy: 0.654156\n",
            "  Epoch 1998 @ step 3124000: Train Loss: 0.982197, Train Accuracy: 0.656250\n",
            "Epoch 1998 Test Loss: 1.057398, Test Accuracy: 0.626098, time: 7.4s\n",
            "  Epoch 1999 @ step 3125000: Train Loss: 0.977480, Train Accuracy: 0.655969\n",
            "Epoch 1999 Test Loss: 1.053579, Test Accuracy: 0.622704, time: 8.4s\n",
            "  Epoch 2000 @ step 3126000: Train Loss: 0.982236, Train Accuracy: 0.656406\n",
            "  Epoch 2000 @ step 3127000: Train Loss: 0.972323, Train Accuracy: 0.656781\n",
            "Epoch 2000 Test Loss: 1.085101, Test Accuracy: 0.614517, time: 7.5s\n",
            "  Epoch 2001 @ step 3128000: Train Loss: 0.980481, Train Accuracy: 0.656625\n",
            "  Epoch 2001 @ step 3129000: Train Loss: 0.979477, Train Accuracy: 0.654563\n",
            "Epoch 2001 Test Loss: 1.145644, Test Accuracy: 0.596346, time: 7.4s\n",
            "  Epoch 2002 @ step 3130000: Train Loss: 0.973290, Train Accuracy: 0.660250\n",
            "Epoch 2002 Test Loss: 1.013723, Test Accuracy: 0.644169, time: 7.4s\n",
            "  Epoch 2003 @ step 3131000: Train Loss: 0.983343, Train Accuracy: 0.657156\n",
            "  Epoch 2003 @ step 3132000: Train Loss: 0.983463, Train Accuracy: 0.654250\n",
            "Epoch 2003 Test Loss: 0.977292, Test Accuracy: 0.658846, time: 7.4s\n",
            "  Epoch 2004 @ step 3133000: Train Loss: 0.980229, Train Accuracy: 0.658719\n",
            "Epoch 2004 Test Loss: 1.056131, Test Accuracy: 0.626498, time: 7.5s\n",
            "  Epoch 2005 @ step 3134000: Train Loss: 0.972223, Train Accuracy: 0.656250\n",
            "  Epoch 2005 @ step 3135000: Train Loss: 0.973600, Train Accuracy: 0.657188\n",
            "Epoch 2005 Test Loss: 0.989950, Test Accuracy: 0.651757, time: 8.5s\n",
            "  Epoch 2006 @ step 3136000: Train Loss: 0.982658, Train Accuracy: 0.657875\n",
            "Epoch 2006 Test Loss: 1.071772, Test Accuracy: 0.630391, time: 7.4s\n",
            "  Epoch 2007 @ step 3137000: Train Loss: 0.979040, Train Accuracy: 0.652844\n",
            "  Epoch 2007 @ step 3138000: Train Loss: 0.975722, Train Accuracy: 0.658438\n",
            "Epoch 2007 Test Loss: 0.998880, Test Accuracy: 0.641074, time: 7.4s\n",
            "  Epoch 2008 @ step 3139000: Train Loss: 0.980871, Train Accuracy: 0.656781\n",
            "  Epoch 2008 @ step 3140000: Train Loss: 0.979748, Train Accuracy: 0.654000\n",
            "Epoch 2008 Test Loss: 0.992151, Test Accuracy: 0.652256, time: 7.4s\n",
            "  Epoch 2009 @ step 3141000: Train Loss: 0.974050, Train Accuracy: 0.662063\n",
            "Epoch 2009 Test Loss: 1.016717, Test Accuracy: 0.647963, time: 7.4s\n",
            "  Epoch 2010 @ step 3142000: Train Loss: 0.978437, Train Accuracy: 0.656500\n",
            "  Epoch 2010 @ step 3143000: Train Loss: 0.985234, Train Accuracy: 0.653438\n",
            "Epoch 2010 Test Loss: 1.043535, Test Accuracy: 0.624401, time: 7.5s\n",
            "  Epoch 2011 @ step 3144000: Train Loss: 0.974284, Train Accuracy: 0.659281\n",
            "Epoch 2011 Test Loss: 1.056834, Test Accuracy: 0.631490, time: 8.6s\n",
            "  Epoch 2012 @ step 3145000: Train Loss: 0.982073, Train Accuracy: 0.659219\n",
            "  Epoch 2012 @ step 3146000: Train Loss: 0.979379, Train Accuracy: 0.656125\n",
            "Epoch 2012 Test Loss: 0.992881, Test Accuracy: 0.656350, time: 7.5s\n",
            "  Epoch 2013 @ step 3147000: Train Loss: 0.968341, Train Accuracy: 0.662469\n",
            "Epoch 2013 Test Loss: 1.034408, Test Accuracy: 0.633187, time: 8.7s\n",
            "  Epoch 2014 @ step 3148000: Train Loss: 0.987480, Train Accuracy: 0.653906\n",
            "  Epoch 2014 @ step 3149000: Train Loss: 0.976231, Train Accuracy: 0.658188\n",
            "Epoch 2014 Test Loss: 1.047055, Test Accuracy: 0.626797, time: 7.4s\n",
            "  Epoch 2015 @ step 3150000: Train Loss: 0.979979, Train Accuracy: 0.657344\n",
            "  Epoch 2015 @ step 3151000: Train Loss: 0.981946, Train Accuracy: 0.656719\n",
            "Epoch 2015 Test Loss: 1.015802, Test Accuracy: 0.645667, time: 7.4s\n",
            "  Epoch 2016 @ step 3152000: Train Loss: 0.968592, Train Accuracy: 0.661125\n",
            "Epoch 2016 Test Loss: 1.012728, Test Accuracy: 0.641673, time: 7.6s\n",
            "  Epoch 2017 @ step 3153000: Train Loss: 0.988489, Train Accuracy: 0.652063\n",
            "  Epoch 2017 @ step 3154000: Train Loss: 0.980890, Train Accuracy: 0.657125\n",
            "Epoch 2017 Test Loss: 1.029788, Test Accuracy: 0.634884, time: 8.3s\n",
            "  Epoch 2018 @ step 3155000: Train Loss: 0.977845, Train Accuracy: 0.659031\n",
            "Epoch 2018 Test Loss: 1.002803, Test Accuracy: 0.644768, time: 7.5s\n",
            "  Epoch 2019 @ step 3156000: Train Loss: 0.973574, Train Accuracy: 0.660125\n",
            "  Epoch 2019 @ step 3157000: Train Loss: 0.976687, Train Accuracy: 0.658500\n",
            "Epoch 2019 Test Loss: 1.031567, Test Accuracy: 0.647564, time: 7.9s\n",
            "  Epoch 2020 @ step 3158000: Train Loss: 0.983177, Train Accuracy: 0.657688\n",
            "Epoch 2020 Test Loss: 1.061194, Test Accuracy: 0.620507, time: 7.4s\n",
            "  Epoch 2021 @ step 3159000: Train Loss: 0.980524, Train Accuracy: 0.655813\n",
            "  Epoch 2021 @ step 3160000: Train Loss: 0.966679, Train Accuracy: 0.660813\n",
            "Epoch 2021 Test Loss: 1.050084, Test Accuracy: 0.631689, time: 7.4s\n",
            "  Epoch 2022 @ step 3161000: Train Loss: 0.986552, Train Accuracy: 0.653844\n",
            "Epoch 2022 Test Loss: 1.050684, Test Accuracy: 0.630691, time: 7.9s\n",
            "  Epoch 2023 @ step 3162000: Train Loss: 0.977197, Train Accuracy: 0.659750\n",
            "  Epoch 2023 @ step 3163000: Train Loss: 0.972041, Train Accuracy: 0.661438\n",
            "Epoch 2023 Test Loss: 1.093624, Test Accuracy: 0.603534, time: 8.0s\n",
            "  Epoch 2024 @ step 3164000: Train Loss: 0.974577, Train Accuracy: 0.659219\n",
            "  Epoch 2024 @ step 3165000: Train Loss: 0.989971, Train Accuracy: 0.653594\n",
            "Epoch 2024 Test Loss: 0.994273, Test Accuracy: 0.654553, time: 7.5s\n",
            "  Epoch 2025 @ step 3166000: Train Loss: 0.970255, Train Accuracy: 0.660750\n",
            "Epoch 2025 Test Loss: 1.037234, Test Accuracy: 0.635084, time: 7.4s\n",
            "  Epoch 2026 @ step 3167000: Train Loss: 0.973361, Train Accuracy: 0.657938\n",
            "  Epoch 2026 @ step 3168000: Train Loss: 0.983676, Train Accuracy: 0.655719\n",
            "Epoch 2026 Test Loss: 1.182611, Test Accuracy: 0.573283, time: 7.4s\n",
            "  Epoch 2027 @ step 3169000: Train Loss: 0.978884, Train Accuracy: 0.655875\n",
            "Epoch 2027 Test Loss: 1.083272, Test Accuracy: 0.608227, time: 7.4s\n",
            "  Epoch 2028 @ step 3170000: Train Loss: 0.982515, Train Accuracy: 0.657406\n",
            "  Epoch 2028 @ step 3171000: Train Loss: 0.983766, Train Accuracy: 0.654781\n",
            "Epoch 2028 Test Loss: 0.997064, Test Accuracy: 0.647264, time: 8.0s\n",
            "  Epoch 2029 @ step 3172000: Train Loss: 0.972198, Train Accuracy: 0.660469\n",
            "Epoch 2029 Test Loss: 1.001634, Test Accuracy: 0.646066, time: 7.7s\n",
            "  Epoch 2030 @ step 3173000: Train Loss: 0.978845, Train Accuracy: 0.658406\n",
            "  Epoch 2030 @ step 3174000: Train Loss: 0.973988, Train Accuracy: 0.659313\n",
            "Epoch 2030 Test Loss: 0.999004, Test Accuracy: 0.648562, time: 7.4s\n",
            "  Epoch 2031 @ step 3175000: Train Loss: 0.969698, Train Accuracy: 0.659406\n",
            "  Epoch 2031 @ step 3176000: Train Loss: 0.991627, Train Accuracy: 0.651969\n",
            "Epoch 2031 Test Loss: 1.022524, Test Accuracy: 0.638279, time: 7.4s\n",
            "  Epoch 2032 @ step 3177000: Train Loss: 0.974835, Train Accuracy: 0.659156\n",
            "Epoch 2032 Test Loss: 1.101017, Test Accuracy: 0.613319, time: 7.4s\n",
            "  Epoch 2033 @ step 3178000: Train Loss: 0.975361, Train Accuracy: 0.659531\n",
            "  Epoch 2033 @ step 3179000: Train Loss: 0.986384, Train Accuracy: 0.657656\n",
            "Epoch 2033 Test Loss: 0.988455, Test Accuracy: 0.652855, time: 7.4s\n",
            "  Epoch 2034 @ step 3180000: Train Loss: 0.983115, Train Accuracy: 0.654375\n",
            "Epoch 2034 Test Loss: 1.059438, Test Accuracy: 0.620607, time: 8.1s\n",
            "  Epoch 2035 @ step 3181000: Train Loss: 0.967450, Train Accuracy: 0.662531\n",
            "  Epoch 2035 @ step 3182000: Train Loss: 0.986273, Train Accuracy: 0.655875\n",
            "Epoch 2035 Test Loss: 1.121453, Test Accuracy: 0.597045, time: 8.0s\n",
            "  Epoch 2036 @ step 3183000: Train Loss: 0.969597, Train Accuracy: 0.662188\n",
            "Epoch 2036 Test Loss: 0.981716, Test Accuracy: 0.654653, time: 8.1s\n",
            "  Epoch 2037 @ step 3184000: Train Loss: 0.984938, Train Accuracy: 0.655125\n",
            "  Epoch 2037 @ step 3185000: Train Loss: 0.973375, Train Accuracy: 0.659094\n",
            "Epoch 2037 Test Loss: 1.003925, Test Accuracy: 0.648363, time: 7.4s\n",
            "  Epoch 2038 @ step 3186000: Train Loss: 0.980802, Train Accuracy: 0.653969\n",
            "Epoch 2038 Test Loss: 1.021453, Test Accuracy: 0.640974, time: 7.4s\n",
            "  Epoch 2039 @ step 3187000: Train Loss: 0.985450, Train Accuracy: 0.656438\n",
            "  Epoch 2039 @ step 3188000: Train Loss: 0.977252, Train Accuracy: 0.657875\n",
            "Epoch 2039 Test Loss: 1.076480, Test Accuracy: 0.625000, time: 7.5s\n",
            "  Epoch 2040 @ step 3189000: Train Loss: 0.974477, Train Accuracy: 0.658250\n",
            "  Epoch 2040 @ step 3190000: Train Loss: 0.986168, Train Accuracy: 0.654563\n",
            "Epoch 2040 Test Loss: 1.065497, Test Accuracy: 0.628494, time: 8.4s\n",
            "  Epoch 2041 @ step 3191000: Train Loss: 0.970147, Train Accuracy: 0.658750\n",
            "Epoch 2041 Test Loss: 0.991260, Test Accuracy: 0.653055, time: 8.0s\n",
            "  Epoch 2042 @ step 3192000: Train Loss: 0.978662, Train Accuracy: 0.657688\n",
            "  Epoch 2042 @ step 3193000: Train Loss: 0.979458, Train Accuracy: 0.659563\n",
            "Epoch 2042 Test Loss: 1.030657, Test Accuracy: 0.640076, time: 7.4s\n",
            "  Epoch 2043 @ step 3194000: Train Loss: 0.975720, Train Accuracy: 0.657469\n",
            "Epoch 2043 Test Loss: 1.070725, Test Accuracy: 0.631090, time: 7.4s\n",
            "  Epoch 2044 @ step 3195000: Train Loss: 0.981372, Train Accuracy: 0.653875\n",
            "  Epoch 2044 @ step 3196000: Train Loss: 0.980253, Train Accuracy: 0.654281\n",
            "Epoch 2044 Test Loss: 0.964314, Test Accuracy: 0.667033, time: 7.5s\n",
            "  Epoch 2045 @ step 3197000: Train Loss: 0.975429, Train Accuracy: 0.662719\n",
            "Epoch 2045 Test Loss: 1.081287, Test Accuracy: 0.621006, time: 7.4s\n",
            "  Epoch 2046 @ step 3198000: Train Loss: 0.978613, Train Accuracy: 0.658969\n",
            "  Epoch 2046 @ step 3199000: Train Loss: 0.970549, Train Accuracy: 0.661938\n",
            "Epoch 2046 Test Loss: 1.019409, Test Accuracy: 0.634085, time: 8.5s\n",
            "  Epoch 2047 @ step 3200000: Train Loss: 0.984552, Train Accuracy: 0.655656\n",
            "  Epoch 2047 @ step 3201000: Train Loss: 0.981710, Train Accuracy: 0.655219\n",
            "Epoch 2047 Test Loss: 0.957044, Test Accuracy: 0.672324, time: 7.5s\n",
            "  Epoch 2048 @ step 3202000: Train Loss: 0.976400, Train Accuracy: 0.659031\n",
            "Epoch 2048 Test Loss: 1.044992, Test Accuracy: 0.637181, time: 7.4s\n",
            "  Epoch 2049 @ step 3203000: Train Loss: 0.969254, Train Accuracy: 0.659188\n",
            "  Epoch 2049 @ step 3204000: Train Loss: 0.982912, Train Accuracy: 0.653438\n",
            "Epoch 2049 Test Loss: 0.998864, Test Accuracy: 0.654253, time: 7.4s\n",
            "  Epoch 2050 @ step 3205000: Train Loss: 0.981600, Train Accuracy: 0.657281\n",
            "Epoch 2050 Test Loss: 1.062931, Test Accuracy: 0.625799, time: 7.4s\n",
            "  Epoch 2051 @ step 3206000: Train Loss: 0.975122, Train Accuracy: 0.656719\n",
            "  Epoch 2051 @ step 3207000: Train Loss: 0.979809, Train Accuracy: 0.656656\n",
            "Epoch 2051 Test Loss: 0.977717, Test Accuracy: 0.652756, time: 7.5s\n",
            "  Epoch 2052 @ step 3208000: Train Loss: 0.976937, Train Accuracy: 0.656344\n",
            "Epoch 2052 Test Loss: 0.965515, Test Accuracy: 0.667133, time: 8.5s\n",
            "  Epoch 2053 @ step 3209000: Train Loss: 0.974962, Train Accuracy: 0.657313\n",
            "  Epoch 2053 @ step 3210000: Train Loss: 0.975545, Train Accuracy: 0.657719\n",
            "Epoch 2053 Test Loss: 1.186115, Test Accuracy: 0.588658, time: 7.5s\n",
            "  Epoch 2054 @ step 3211000: Train Loss: 0.976164, Train Accuracy: 0.658375\n",
            "Epoch 2054 Test Loss: 1.067349, Test Accuracy: 0.617412, time: 7.5s\n",
            "  Epoch 2055 @ step 3212000: Train Loss: 0.987476, Train Accuracy: 0.653406\n",
            "  Epoch 2055 @ step 3213000: Train Loss: 0.974252, Train Accuracy: 0.661188\n",
            "Epoch 2055 Test Loss: 1.131535, Test Accuracy: 0.598842, time: 7.4s\n",
            "  Epoch 2056 @ step 3214000: Train Loss: 0.980773, Train Accuracy: 0.656656\n",
            "  Epoch 2056 @ step 3215000: Train Loss: 0.979582, Train Accuracy: 0.655000\n",
            "Epoch 2056 Test Loss: 1.093833, Test Accuracy: 0.621506, time: 7.5s\n",
            "  Epoch 2057 @ step 3216000: Train Loss: 0.978320, Train Accuracy: 0.659938\n",
            "Epoch 2057 Test Loss: 1.003149, Test Accuracy: 0.653554, time: 7.6s\n",
            "  Epoch 2058 @ step 3217000: Train Loss: 0.971825, Train Accuracy: 0.656750\n",
            "  Epoch 2058 @ step 3218000: Train Loss: 0.981811, Train Accuracy: 0.655500\n",
            "Epoch 2058 Test Loss: 1.209621, Test Accuracy: 0.588259, time: 9.1s\n",
            "  Epoch 2059 @ step 3219000: Train Loss: 0.976313, Train Accuracy: 0.657281\n",
            "Epoch 2059 Test Loss: 1.045559, Test Accuracy: 0.632188, time: 7.5s\n",
            "  Epoch 2060 @ step 3220000: Train Loss: 0.976928, Train Accuracy: 0.657781\n",
            "  Epoch 2060 @ step 3221000: Train Loss: 0.979052, Train Accuracy: 0.656063\n",
            "Epoch 2060 Test Loss: 0.972527, Test Accuracy: 0.661342, time: 7.5s\n",
            "  Epoch 2061 @ step 3222000: Train Loss: 0.977843, Train Accuracy: 0.658719\n",
            "Epoch 2061 Test Loss: 1.079039, Test Accuracy: 0.605531, time: 7.4s\n",
            "  Epoch 2062 @ step 3223000: Train Loss: 0.976760, Train Accuracy: 0.655750\n",
            "  Epoch 2062 @ step 3224000: Train Loss: 0.979257, Train Accuracy: 0.655781\n",
            "Epoch 2062 Test Loss: 1.109404, Test Accuracy: 0.613518, time: 7.5s\n",
            "  Epoch 2063 @ step 3225000: Train Loss: 0.973321, Train Accuracy: 0.661125\n",
            "  Epoch 2063 @ step 3226000: Train Loss: 0.982342, Train Accuracy: 0.659719\n",
            "Epoch 2063 Test Loss: 1.057303, Test Accuracy: 0.618510, time: 8.6s\n",
            "  Epoch 2064 @ step 3227000: Train Loss: 0.978159, Train Accuracy: 0.656188\n",
            "Epoch 2064 Test Loss: 1.027323, Test Accuracy: 0.634585, time: 7.8s\n",
            "  Epoch 2065 @ step 3228000: Train Loss: 0.981043, Train Accuracy: 0.656125\n",
            "  Epoch 2065 @ step 3229000: Train Loss: 0.977307, Train Accuracy: 0.658781\n",
            "Epoch 2065 Test Loss: 1.054596, Test Accuracy: 0.635583, time: 7.5s\n",
            "  Epoch 2066 @ step 3230000: Train Loss: 0.964434, Train Accuracy: 0.663531\n",
            "Epoch 2066 Test Loss: 1.034832, Test Accuracy: 0.632188, time: 7.5s\n",
            "  Epoch 2067 @ step 3231000: Train Loss: 0.986944, Train Accuracy: 0.653781\n",
            "  Epoch 2067 @ step 3232000: Train Loss: 0.980782, Train Accuracy: 0.655813\n",
            "Epoch 2067 Test Loss: 1.111018, Test Accuracy: 0.609225, time: 7.5s\n",
            "  Epoch 2068 @ step 3233000: Train Loss: 0.977102, Train Accuracy: 0.657031\n",
            "Epoch 2068 Test Loss: 1.101354, Test Accuracy: 0.617812, time: 7.4s\n",
            "  Epoch 2069 @ step 3234000: Train Loss: 0.976665, Train Accuracy: 0.659625\n",
            "  Epoch 2069 @ step 3235000: Train Loss: 0.976123, Train Accuracy: 0.658844\n",
            "Epoch 2069 Test Loss: 1.015341, Test Accuracy: 0.636382, time: 8.2s\n",
            "  Epoch 2070 @ step 3236000: Train Loss: 0.970140, Train Accuracy: 0.658719\n",
            "Epoch 2070 Test Loss: 1.163521, Test Accuracy: 0.585064, time: 7.6s\n",
            "  Epoch 2071 @ step 3237000: Train Loss: 0.986514, Train Accuracy: 0.655406\n",
            "  Epoch 2071 @ step 3238000: Train Loss: 0.973138, Train Accuracy: 0.660594\n",
            "Epoch 2071 Test Loss: 1.080799, Test Accuracy: 0.619708, time: 7.5s\n",
            "  Epoch 2072 @ step 3239000: Train Loss: 0.976329, Train Accuracy: 0.657344\n",
            "  Epoch 2072 @ step 3240000: Train Loss: 0.985053, Train Accuracy: 0.654875\n",
            "Epoch 2072 Test Loss: 0.994941, Test Accuracy: 0.654852, time: 7.6s\n",
            "  Epoch 2073 @ step 3241000: Train Loss: 0.979431, Train Accuracy: 0.656875\n",
            "Epoch 2073 Test Loss: 1.001740, Test Accuracy: 0.652756, time: 7.6s\n",
            "  Epoch 2074 @ step 3242000: Train Loss: 0.975271, Train Accuracy: 0.657594\n",
            "  Epoch 2074 @ step 3243000: Train Loss: 0.980020, Train Accuracy: 0.657844\n",
            "Epoch 2074 Test Loss: 1.034811, Test Accuracy: 0.642372, time: 7.5s\n",
            "  Epoch 2075 @ step 3244000: Train Loss: 0.974698, Train Accuracy: 0.658344\n",
            "Epoch 2075 Test Loss: 0.996147, Test Accuracy: 0.652157, time: 8.4s\n",
            "  Epoch 2076 @ step 3245000: Train Loss: 0.979687, Train Accuracy: 0.657094\n",
            "  Epoch 2076 @ step 3246000: Train Loss: 0.977820, Train Accuracy: 0.654875\n",
            "Epoch 2076 Test Loss: 1.017082, Test Accuracy: 0.636082, time: 7.4s\n",
            "  Epoch 2077 @ step 3247000: Train Loss: 0.984555, Train Accuracy: 0.657250\n",
            "Epoch 2077 Test Loss: 0.989392, Test Accuracy: 0.655351, time: 7.6s\n",
            "  Epoch 2078 @ step 3248000: Train Loss: 0.980606, Train Accuracy: 0.660188\n",
            "  Epoch 2078 @ step 3249000: Train Loss: 0.973283, Train Accuracy: 0.657438\n",
            "Epoch 2078 Test Loss: 1.030341, Test Accuracy: 0.628794, time: 7.7s\n",
            "  Epoch 2079 @ step 3250000: Train Loss: 0.975780, Train Accuracy: 0.659125\n",
            "  Epoch 2079 @ step 3251000: Train Loss: 0.985711, Train Accuracy: 0.655906\n",
            "Epoch 2079 Test Loss: 1.013656, Test Accuracy: 0.648962, time: 7.5s\n",
            "  Epoch 2080 @ step 3252000: Train Loss: 0.972132, Train Accuracy: 0.659188\n",
            "Epoch 2080 Test Loss: 1.036566, Test Accuracy: 0.632288, time: 7.7s\n",
            "  Epoch 2081 @ step 3253000: Train Loss: 0.984713, Train Accuracy: 0.655094\n",
            "  Epoch 2081 @ step 3254000: Train Loss: 0.982118, Train Accuracy: 0.655344\n",
            "Epoch 2081 Test Loss: 0.975258, Test Accuracy: 0.661242, time: 8.7s\n",
            "  Epoch 2082 @ step 3255000: Train Loss: 0.969875, Train Accuracy: 0.657906\n",
            "Epoch 2082 Test Loss: 1.016357, Test Accuracy: 0.643970, time: 7.4s\n",
            "  Epoch 2083 @ step 3256000: Train Loss: 0.980278, Train Accuracy: 0.657875\n",
            "  Epoch 2083 @ step 3257000: Train Loss: 0.985397, Train Accuracy: 0.651219\n",
            "Epoch 2083 Test Loss: 0.999095, Test Accuracy: 0.648163, time: 7.4s\n",
            "  Epoch 2084 @ step 3258000: Train Loss: 0.974095, Train Accuracy: 0.658594\n",
            "Epoch 2084 Test Loss: 1.002570, Test Accuracy: 0.647464, time: 7.4s\n",
            "  Epoch 2085 @ step 3259000: Train Loss: 0.977561, Train Accuracy: 0.656969\n",
            "  Epoch 2085 @ step 3260000: Train Loss: 0.969603, Train Accuracy: 0.658313\n",
            "Epoch 2085 Test Loss: 0.996225, Test Accuracy: 0.650459, time: 8.0s\n",
            "  Epoch 2086 @ step 3261000: Train Loss: 0.981341, Train Accuracy: 0.658563\n",
            "Epoch 2086 Test Loss: 1.054525, Test Accuracy: 0.631290, time: 7.5s\n",
            "  Epoch 2087 @ step 3262000: Train Loss: 0.988207, Train Accuracy: 0.656813\n",
            "  Epoch 2087 @ step 3263000: Train Loss: 0.976663, Train Accuracy: 0.656219\n",
            "Epoch 2087 Test Loss: 0.989839, Test Accuracy: 0.649960, time: 8.4s\n",
            "  Epoch 2088 @ step 3264000: Train Loss: 0.977312, Train Accuracy: 0.659000\n",
            "  Epoch 2088 @ step 3265000: Train Loss: 0.978780, Train Accuracy: 0.657125\n",
            "Epoch 2088 Test Loss: 0.980585, Test Accuracy: 0.663039, time: 7.5s\n",
            "  Epoch 2089 @ step 3266000: Train Loss: 0.979144, Train Accuracy: 0.659969\n",
            "Epoch 2089 Test Loss: 1.011113, Test Accuracy: 0.639976, time: 7.5s\n",
            "  Epoch 2090 @ step 3267000: Train Loss: 0.972801, Train Accuracy: 0.659156\n",
            "  Epoch 2090 @ step 3268000: Train Loss: 0.984629, Train Accuracy: 0.656563\n",
            "Epoch 2090 Test Loss: 1.133593, Test Accuracy: 0.598942, time: 7.5s\n",
            "  Epoch 2091 @ step 3269000: Train Loss: 0.973006, Train Accuracy: 0.661031\n",
            "Epoch 2091 Test Loss: 0.975500, Test Accuracy: 0.659145, time: 7.5s\n",
            "  Epoch 2092 @ step 3270000: Train Loss: 0.980962, Train Accuracy: 0.656438\n",
            "  Epoch 2092 @ step 3271000: Train Loss: 0.982706, Train Accuracy: 0.655563\n",
            "Epoch 2092 Test Loss: 1.040275, Test Accuracy: 0.630990, time: 7.9s\n",
            "  Epoch 2093 @ step 3272000: Train Loss: 0.969602, Train Accuracy: 0.658844\n",
            "Epoch 2093 Test Loss: 1.034773, Test Accuracy: 0.636082, time: 8.3s\n",
            "  Epoch 2094 @ step 3273000: Train Loss: 0.984346, Train Accuracy: 0.656219\n",
            "  Epoch 2094 @ step 3274000: Train Loss: 0.973577, Train Accuracy: 0.657500\n",
            "Epoch 2094 Test Loss: 1.062554, Test Accuracy: 0.624301, time: 7.5s\n",
            "  Epoch 2095 @ step 3275000: Train Loss: 0.973916, Train Accuracy: 0.656156\n",
            "  Epoch 2095 @ step 3276000: Train Loss: 0.990399, Train Accuracy: 0.653000\n",
            "Epoch 2095 Test Loss: 1.111403, Test Accuracy: 0.610823, time: 7.5s\n",
            "  Epoch 2096 @ step 3277000: Train Loss: 0.972826, Train Accuracy: 0.659688\n",
            "Epoch 2096 Test Loss: 0.985198, Test Accuracy: 0.656949, time: 7.6s\n",
            "  Epoch 2097 @ step 3278000: Train Loss: 0.978872, Train Accuracy: 0.656938\n",
            "  Epoch 2097 @ step 3279000: Train Loss: 0.983574, Train Accuracy: 0.654063\n",
            "Epoch 2097 Test Loss: 0.996971, Test Accuracy: 0.654752, time: 7.5s\n",
            "  Epoch 2098 @ step 3280000: Train Loss: 0.971650, Train Accuracy: 0.659719\n",
            "Epoch 2098 Test Loss: 0.989927, Test Accuracy: 0.656849, time: 8.0s\n",
            "  Epoch 2099 @ step 3281000: Train Loss: 0.987091, Train Accuracy: 0.655313\n",
            "  Epoch 2099 @ step 3282000: Train Loss: 0.980194, Train Accuracy: 0.655500\n",
            "Epoch 2099 Test Loss: 0.970440, Test Accuracy: 0.663039, time: 8.0s\n",
            "  Epoch 2100 @ step 3283000: Train Loss: 0.976824, Train Accuracy: 0.657625\n",
            "Epoch 2100 Test Loss: 0.964510, Test Accuracy: 0.663738, time: 7.5s\n",
            "  Epoch 2101 @ step 3284000: Train Loss: 0.979344, Train Accuracy: 0.655719\n",
            "  Epoch 2101 @ step 3285000: Train Loss: 0.977185, Train Accuracy: 0.657875\n",
            "Epoch 2101 Test Loss: 1.084998, Test Accuracy: 0.610723, time: 7.6s\n",
            "  Epoch 2102 @ step 3286000: Train Loss: 0.980916, Train Accuracy: 0.660250\n",
            "Epoch 2102 Test Loss: 1.062908, Test Accuracy: 0.628395, time: 7.6s\n",
            "  Epoch 2103 @ step 3287000: Train Loss: 0.978405, Train Accuracy: 0.658313\n",
            "  Epoch 2103 @ step 3288000: Train Loss: 0.975938, Train Accuracy: 0.657344\n",
            "Epoch 2103 Test Loss: 1.045051, Test Accuracy: 0.630292, time: 8.6s\n",
            "  Epoch 2104 @ step 3289000: Train Loss: 0.980531, Train Accuracy: 0.655500\n",
            "  Epoch 2104 @ step 3290000: Train Loss: 0.983045, Train Accuracy: 0.653719\n",
            "Epoch 2104 Test Loss: 1.034784, Test Accuracy: 0.628594, time: 8.6s\n",
            "  Epoch 2105 @ step 3291000: Train Loss: 0.974456, Train Accuracy: 0.660375\n",
            "Epoch 2105 Test Loss: 1.025193, Test Accuracy: 0.634984, time: 7.5s\n",
            "  Epoch 2106 @ step 3292000: Train Loss: 0.980305, Train Accuracy: 0.656281\n",
            "  Epoch 2106 @ step 3293000: Train Loss: 0.975690, Train Accuracy: 0.655875\n",
            "Epoch 2106 Test Loss: 1.065836, Test Accuracy: 0.621506, time: 7.7s\n",
            "  Epoch 2107 @ step 3294000: Train Loss: 0.970770, Train Accuracy: 0.660438\n",
            "Epoch 2107 Test Loss: 1.020348, Test Accuracy: 0.641074, time: 8.1s\n",
            "  Epoch 2108 @ step 3295000: Train Loss: 0.983626, Train Accuracy: 0.655219\n",
            "  Epoch 2108 @ step 3296000: Train Loss: 0.980736, Train Accuracy: 0.659438\n",
            "Epoch 2108 Test Loss: 1.075359, Test Accuracy: 0.619209, time: 7.7s\n",
            "  Epoch 2109 @ step 3297000: Train Loss: 0.971660, Train Accuracy: 0.663156\n",
            "Epoch 2109 Test Loss: 1.111006, Test Accuracy: 0.618111, time: 7.7s\n",
            "  Epoch 2110 @ step 3298000: Train Loss: 0.988980, Train Accuracy: 0.656125\n",
            "  Epoch 2110 @ step 3299000: Train Loss: 0.974840, Train Accuracy: 0.661000\n",
            "Epoch 2110 Test Loss: 1.056403, Test Accuracy: 0.624002, time: 8.5s\n",
            "  Epoch 2111 @ step 3300000: Train Loss: 0.967233, Train Accuracy: 0.659813\n",
            "  Epoch 2111 @ step 3301000: Train Loss: 0.992315, Train Accuracy: 0.653219\n",
            "Epoch 2111 Test Loss: 1.075917, Test Accuracy: 0.615515, time: 7.5s\n",
            "  Epoch 2112 @ step 3302000: Train Loss: 0.972061, Train Accuracy: 0.659969\n",
            "Epoch 2112 Test Loss: 1.071558, Test Accuracy: 0.623103, time: 7.5s\n",
            "  Epoch 2113 @ step 3303000: Train Loss: 0.977546, Train Accuracy: 0.661156\n",
            "  Epoch 2113 @ step 3304000: Train Loss: 0.985562, Train Accuracy: 0.654000\n",
            "Epoch 2113 Test Loss: 1.011756, Test Accuracy: 0.648063, time: 7.5s\n",
            "  Epoch 2114 @ step 3305000: Train Loss: 0.969956, Train Accuracy: 0.659719\n",
            "Epoch 2114 Test Loss: 0.973698, Test Accuracy: 0.663339, time: 7.5s\n",
            "  Epoch 2115 @ step 3306000: Train Loss: 0.983433, Train Accuracy: 0.655156\n",
            "  Epoch 2115 @ step 3307000: Train Loss: 0.979443, Train Accuracy: 0.656031\n",
            "Epoch 2115 Test Loss: 0.999586, Test Accuracy: 0.650260, time: 7.5s\n",
            "  Epoch 2116 @ step 3308000: Train Loss: 0.978007, Train Accuracy: 0.655781\n",
            "Epoch 2116 Test Loss: 1.066569, Test Accuracy: 0.626298, time: 8.5s\n",
            "  Epoch 2117 @ step 3309000: Train Loss: 0.979547, Train Accuracy: 0.657719\n",
            "  Epoch 2117 @ step 3310000: Train Loss: 0.976348, Train Accuracy: 0.662031\n",
            "Epoch 2117 Test Loss: 1.100571, Test Accuracy: 0.605831, time: 7.5s\n",
            "  Epoch 2118 @ step 3311000: Train Loss: 0.976168, Train Accuracy: 0.656438\n",
            "Epoch 2118 Test Loss: 1.074549, Test Accuracy: 0.612819, time: 7.5s\n",
            "  Epoch 2119 @ step 3312000: Train Loss: 0.983834, Train Accuracy: 0.655031\n",
            "  Epoch 2119 @ step 3313000: Train Loss: 0.971095, Train Accuracy: 0.659500\n",
            "Epoch 2119 Test Loss: 1.130765, Test Accuracy: 0.601937, time: 7.5s\n",
            "  Epoch 2120 @ step 3314000: Train Loss: 0.976870, Train Accuracy: 0.658375\n",
            "  Epoch 2120 @ step 3315000: Train Loss: 0.980791, Train Accuracy: 0.655063\n",
            "Epoch 2120 Test Loss: 0.988143, Test Accuracy: 0.655651, time: 7.5s\n",
            "  Epoch 2121 @ step 3316000: Train Loss: 0.970123, Train Accuracy: 0.658344\n",
            "Epoch 2121 Test Loss: 1.084947, Test Accuracy: 0.624101, time: 7.6s\n",
            "  Epoch 2122 @ step 3317000: Train Loss: 0.983236, Train Accuracy: 0.656250\n",
            "  Epoch 2122 @ step 3318000: Train Loss: 0.983165, Train Accuracy: 0.653469\n",
            "Epoch 2122 Test Loss: 1.042863, Test Accuracy: 0.631689, time: 8.3s\n",
            "  Epoch 2123 @ step 3319000: Train Loss: 0.973807, Train Accuracy: 0.661094\n",
            "Epoch 2123 Test Loss: 0.983841, Test Accuracy: 0.655950, time: 7.6s\n",
            "  Epoch 2124 @ step 3320000: Train Loss: 0.981852, Train Accuracy: 0.655875\n",
            "  Epoch 2124 @ step 3321000: Train Loss: 0.975721, Train Accuracy: 0.658125\n",
            "Epoch 2124 Test Loss: 0.980259, Test Accuracy: 0.660942, time: 7.5s\n",
            "  Epoch 2125 @ step 3322000: Train Loss: 0.983991, Train Accuracy: 0.654438\n",
            "Epoch 2125 Test Loss: 1.020018, Test Accuracy: 0.642871, time: 7.9s\n",
            "  Epoch 2126 @ step 3323000: Train Loss: 0.976972, Train Accuracy: 0.657031\n",
            "  Epoch 2126 @ step 3324000: Train Loss: 0.976016, Train Accuracy: 0.656563\n",
            "Epoch 2126 Test Loss: 0.981468, Test Accuracy: 0.659844, time: 8.1s\n",
            "  Epoch 2127 @ step 3325000: Train Loss: 0.974465, Train Accuracy: 0.657625\n",
            "  Epoch 2127 @ step 3326000: Train Loss: 0.984211, Train Accuracy: 0.657531\n",
            "Epoch 2127 Test Loss: 1.087945, Test Accuracy: 0.614816, time: 8.1s\n",
            "  Epoch 2128 @ step 3327000: Train Loss: 0.980150, Train Accuracy: 0.654344\n",
            "Epoch 2128 Test Loss: 1.154768, Test Accuracy: 0.603534, time: 7.8s\n",
            "  Epoch 2129 @ step 3328000: Train Loss: 0.976261, Train Accuracy: 0.657969\n",
            "  Epoch 2129 @ step 3329000: Train Loss: 0.978631, Train Accuracy: 0.658281\n",
            "Epoch 2129 Test Loss: 1.023939, Test Accuracy: 0.633886, time: 8.0s\n",
            "  Epoch 2130 @ step 3330000: Train Loss: 0.977320, Train Accuracy: 0.657188\n",
            "Epoch 2130 Test Loss: 1.119219, Test Accuracy: 0.611022, time: 7.4s\n",
            "  Epoch 2131 @ step 3331000: Train Loss: 0.981583, Train Accuracy: 0.657594\n",
            "  Epoch 2131 @ step 3332000: Train Loss: 0.981338, Train Accuracy: 0.657375\n",
            "Epoch 2131 Test Loss: 0.998475, Test Accuracy: 0.656350, time: 7.5s\n",
            "  Epoch 2132 @ step 3333000: Train Loss: 0.975061, Train Accuracy: 0.657563\n",
            "Epoch 2132 Test Loss: 0.998489, Test Accuracy: 0.650958, time: 7.4s\n",
            "  Epoch 2133 @ step 3334000: Train Loss: 0.981603, Train Accuracy: 0.659063\n",
            "  Epoch 2133 @ step 3335000: Train Loss: 0.975404, Train Accuracy: 0.658531\n",
            "Epoch 2133 Test Loss: 1.033194, Test Accuracy: 0.646765, time: 8.4s\n",
            "  Epoch 2134 @ step 3336000: Train Loss: 0.978570, Train Accuracy: 0.657125\n",
            "  Epoch 2134 @ step 3337000: Train Loss: 0.982474, Train Accuracy: 0.654375\n",
            "Epoch 2134 Test Loss: 1.045218, Test Accuracy: 0.638079, time: 7.3s\n",
            "  Epoch 2135 @ step 3338000: Train Loss: 0.968149, Train Accuracy: 0.661563\n",
            "Epoch 2135 Test Loss: 1.010797, Test Accuracy: 0.646865, time: 7.4s\n",
            "  Epoch 2136 @ step 3339000: Train Loss: 0.982473, Train Accuracy: 0.655188\n",
            "  Epoch 2136 @ step 3340000: Train Loss: 0.983629, Train Accuracy: 0.654563\n",
            "Epoch 2136 Test Loss: 1.180299, Test Accuracy: 0.570587, time: 7.4s\n",
            "  Epoch 2137 @ step 3341000: Train Loss: 0.968086, Train Accuracy: 0.662313\n",
            "Epoch 2137 Test Loss: 1.001039, Test Accuracy: 0.651957, time: 7.4s\n",
            "  Epoch 2138 @ step 3342000: Train Loss: 0.981229, Train Accuracy: 0.653531\n",
            "  Epoch 2138 @ step 3343000: Train Loss: 0.980458, Train Accuracy: 0.657250\n",
            "Epoch 2138 Test Loss: 0.993987, Test Accuracy: 0.650359, time: 7.5s\n",
            "  Epoch 2139 @ step 3344000: Train Loss: 0.975780, Train Accuracy: 0.658313\n",
            "Epoch 2139 Test Loss: 1.050922, Test Accuracy: 0.621905, time: 8.5s\n",
            "  Epoch 2140 @ step 3345000: Train Loss: 0.981879, Train Accuracy: 0.656188\n",
            "  Epoch 2140 @ step 3346000: Train Loss: 0.980047, Train Accuracy: 0.659125\n",
            "Epoch 2140 Test Loss: 0.973908, Test Accuracy: 0.653155, time: 7.4s\n",
            "  Epoch 2141 @ step 3347000: Train Loss: 0.979445, Train Accuracy: 0.657688\n",
            "Epoch 2141 Test Loss: 1.099507, Test Accuracy: 0.606330, time: 7.6s\n",
            "  Epoch 2142 @ step 3348000: Train Loss: 0.977253, Train Accuracy: 0.656313\n",
            "  Epoch 2142 @ step 3349000: Train Loss: 0.975028, Train Accuracy: 0.659063\n",
            "Epoch 2142 Test Loss: 1.042322, Test Accuracy: 0.628494, time: 7.5s\n",
            "  Epoch 2143 @ step 3350000: Train Loss: 0.978439, Train Accuracy: 0.656594\n",
            "  Epoch 2143 @ step 3351000: Train Loss: 0.982561, Train Accuracy: 0.658063\n",
            "Epoch 2143 Test Loss: 1.062981, Test Accuracy: 0.620008, time: 7.6s\n",
            "  Epoch 2144 @ step 3352000: Train Loss: 0.975669, Train Accuracy: 0.658656\n",
            "Epoch 2144 Test Loss: 1.037140, Test Accuracy: 0.632588, time: 7.5s\n",
            "  Epoch 2145 @ step 3353000: Train Loss: 0.980035, Train Accuracy: 0.653844\n",
            "  Epoch 2145 @ step 3354000: Train Loss: 0.975543, Train Accuracy: 0.658781\n",
            "Epoch 2145 Test Loss: 1.047305, Test Accuracy: 0.628694, time: 8.5s\n",
            "  Epoch 2146 @ step 3355000: Train Loss: 0.977248, Train Accuracy: 0.657656\n",
            "Epoch 2146 Test Loss: 0.990600, Test Accuracy: 0.652855, time: 7.5s\n",
            "  Epoch 2147 @ step 3356000: Train Loss: 0.982577, Train Accuracy: 0.657156\n",
            "  Epoch 2147 @ step 3357000: Train Loss: 0.974520, Train Accuracy: 0.658750\n",
            "Epoch 2147 Test Loss: 1.159571, Test Accuracy: 0.602536, time: 7.5s\n",
            "  Epoch 2148 @ step 3358000: Train Loss: 0.982847, Train Accuracy: 0.659063\n",
            "Epoch 2148 Test Loss: 1.057345, Test Accuracy: 0.628994, time: 8.7s\n",
            "  Epoch 2149 @ step 3359000: Train Loss: 0.979932, Train Accuracy: 0.656688\n",
            "  Epoch 2149 @ step 3360000: Train Loss: 0.978038, Train Accuracy: 0.657188\n",
            "Epoch 2149 Test Loss: 1.067501, Test Accuracy: 0.619409, time: 7.6s\n",
            "  Epoch 2150 @ step 3361000: Train Loss: 0.976353, Train Accuracy: 0.655750\n",
            "  Epoch 2150 @ step 3362000: Train Loss: 0.984247, Train Accuracy: 0.655063\n",
            "Epoch 2150 Test Loss: 1.056874, Test Accuracy: 0.621206, time: 7.8s\n",
            "  Epoch 2151 @ step 3363000: Train Loss: 0.983529, Train Accuracy: 0.654156\n",
            "Epoch 2151 Test Loss: 1.089291, Test Accuracy: 0.610423, time: 8.8s\n",
            "  Epoch 2152 @ step 3364000: Train Loss: 0.975104, Train Accuracy: 0.660938\n",
            "  Epoch 2152 @ step 3365000: Train Loss: 0.977074, Train Accuracy: 0.657781\n",
            "Epoch 2152 Test Loss: 1.203176, Test Accuracy: 0.582568, time: 7.6s\n",
            "  Epoch 2153 @ step 3366000: Train Loss: 0.978216, Train Accuracy: 0.658563\n",
            "Epoch 2153 Test Loss: 0.996356, Test Accuracy: 0.644868, time: 7.6s\n",
            "  Epoch 2154 @ step 3367000: Train Loss: 0.979551, Train Accuracy: 0.656031\n",
            "  Epoch 2154 @ step 3368000: Train Loss: 0.979109, Train Accuracy: 0.657969\n",
            "Epoch 2154 Test Loss: 1.000751, Test Accuracy: 0.653654, time: 7.5s\n",
            "  Epoch 2155 @ step 3369000: Train Loss: 0.975416, Train Accuracy: 0.659563\n",
            "Epoch 2155 Test Loss: 1.115449, Test Accuracy: 0.608626, time: 7.5s\n",
            "  Epoch 2156 @ step 3370000: Train Loss: 0.983471, Train Accuracy: 0.653969\n",
            "  Epoch 2156 @ step 3371000: Train Loss: 0.979916, Train Accuracy: 0.659156\n",
            "Epoch 2156 Test Loss: 0.982663, Test Accuracy: 0.658946, time: 8.1s\n",
            "  Epoch 2157 @ step 3372000: Train Loss: 0.973616, Train Accuracy: 0.657375\n",
            "Epoch 2157 Test Loss: 0.980966, Test Accuracy: 0.659645, time: 7.9s\n",
            "  Epoch 2158 @ step 3373000: Train Loss: 0.987577, Train Accuracy: 0.655188\n",
            "  Epoch 2158 @ step 3374000: Train Loss: 0.974401, Train Accuracy: 0.657625\n",
            "Epoch 2158 Test Loss: 1.097285, Test Accuracy: 0.611621, time: 7.5s\n",
            "  Epoch 2159 @ step 3375000: Train Loss: 0.978045, Train Accuracy: 0.656375\n",
            "  Epoch 2159 @ step 3376000: Train Loss: 0.986911, Train Accuracy: 0.655375\n",
            "Epoch 2159 Test Loss: 1.058584, Test Accuracy: 0.621206, time: 7.5s\n",
            "Final Test Loss: 1.058584, Test Accuracy: 0.621206, Total time: 16791.0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8BGTJw-TPTfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "e53244fd-4fcf-4132-e722-a8fa23378b1a"
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"ConvModel\", metrics)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecFPX5wPHPcwUQBURABVFBJfZ+\nEo3dWFBji8aA+otGDZpYU4wSjVhiokZjb6jYBbuiohRRsaB4FGnS61GPXq/s7vP74zt7N7e3u7d7\nt3O7dzzv1+teuzv12bnZeeb7ne98R1QVY4wxJih52Q7AGGNM82aJxhhjTKAs0RhjjAmUJRpjjDGB\nskRjjDEmUJZojDHGBMoSjckJIjJfRE5upHVtIyIfisg6EXmrMdbZlIhINxFRESlIYdrLROTrxojL\nNF2WaMzW6AJgJ6CDqv4mEwsUkbYi8rCILBSRjSIyx/vcMRPLT7Le+SJSEbseEZngJYtuQa7fmFRY\nojFbo92BmaoaSnfGeGf5ItIC+AzYH+gFtAWOAlYBPRsWakrmAX188RwItG6E9RqTEks0JueISEuv\nNLDE+3tYRFp64zqKyEcislZEVovIVyKS5427WUQWi8gGEZkhIr+Ms+w7gduB33oljytEJE9EbhOR\nBSKyQkReFpF23vTRaqQrRGQhMCpOyL8DdgPOU9VpqhpR1RWqereqDvWWs6+IfOHFPVVEzvbF9KKI\nPCEiH3uxfy8ie3rjnhKRB2K+wwci8hffoFe8GKIuBV6Omaed971Kve95m2+75YvIAyKyUkTmAmfG\nmfd5EVnqbd9/iUh+wn+gMTEs0ZhcdCtwJHAIcDCuVHCbN+6vQAnQCVf99Q9ARWRv4FrgCFVtA5wG\nzI9dsKr2B/4NvKGq26nq88Bl3t+JwB7AdsDjMbMeD+zrLTfWycCnqrox3pcRkULgQ2A4sCNwHfCa\nF3NUb+BOoD0wG7jHGz4IlxTFW1Z74FRgsG/e74C2XjLL95b1akwYjwHtvO93PC4x/d4b9wfgV8Ch\nQBGuatHvRSAE7OVNcypwZbzvakw8lmhMLroYuMsrFZTiDsD/542rBDoDu6tqpap+pa7DvjDQEthP\nRApVdb6qzkljff9T1blesugH9I6pJrtDVTep6pY483cAliZZ/pG45HWvqlao6ijgI3zVXcB7qjrW\nq857DZdkAb4CFDjW+3wBMEZVl8SsI1qqOQX4CVgcHeFLPv1UdYOqzgcepHqbXgg8rKqLVHU18B/f\nvDsBZwA3et9/BfCQtzxjUmKJxuSiLsAC3+cF3jCA/+LO+IeLyFwRuQVAVWcDNwJ3ACtEZLCIdCE1\n8dZXgCsxRS1KMv8qXPJLtvxFqhqJWccuvs/LfO834xITXhIdTHVSugiXiGK94o27jJhqM6AjUEjt\n7xhdfxdqfj//dLt78y71qv3WAs/gSmbGpMQSjclFS3AHuKjdvGF4Z+R/VdU9gLOBv0Svxajq66p6\njDevAvc1YH0hYLlvWLJuzkcCp4nItkmWv2v0mohvHYsTTB9rEHCBiOwO/Bx4J3YCVV2AaxRwBvBu\nzOiVuJJg7HeMrn8psGvMuKhFQDnQUVW39/7aqur+KcZujCUak5MGAbeJSCev2e7teNccRORXIrKX\nd81iHa7KLCIie4vISV6jgTJgCxBJsPx46/uziHQXke2ovoaTaqu0V3AH5HdEZB+vcUEHEfmHiJwB\nfI8rpfxdRApF5ATgLGpeZ0lIVSfgksVzwDBVXZtg0iuAk1R1U8z8YeBN4B4RaeMlrL9QfR3nTeB6\nEenqXQO6xTfvUty1pQe9Jtx5IrKniByfSuzGgCUak5v+BRQDk4DJwHhvGEAPXAliIzAGeFJVP8dd\nn7kXd0Behqva6Zfi+gbiksVoXKmgDHfBPiWqWo5rEDAdGAGsB8biqqy+V9UKXGI53YvvSeB3qjo9\n1XUAr3vreD1JHHNUtTjB6OuATcBc4GtvOQO9cc8Cw4Afcds6tkT0O6AFMA1YA7xN8qpCY2oQe/CZ\nMcaYIFmJxhhjTKDq7MuovkRkIK5t/gpVPSDO+JtwzUqjcewLdFLV1SIyH9iAq38PqWpRUHEaY4wJ\nVmBVZyJyHK4e/eV4iSZm2rOAP6vqSd7n+UCRqq4MJDhjjDGNJrCqM1UdDaxOcfI+uJY/xhhjmpnA\nqs5SJSKtcR0RXusbrLgb8hR4RlUHJJm/L9AXYNtttz18n332CTJcY4xpVsaNG7dSVTsFuY6sJxpc\ns89vvK4voo5R1cUisiMwQkSmeyWkWrwkNACgqKhIi4sTte40xhgTS0QW1D1Vw+RCq7PexFSbqepi\n73UF8B6N09W6McaYAGQ10XhdsR8PfOAbtq2ItIm+x/UUOyU7ERpjjGmoIJs3DwJOADqKSAnQH9c5\nH6r6tDfZecDwmC4zdgLe83pFLwBeV9VPg4rTGGNMsAJLNKraJ4VpXsQ968I/bC7uGSTGGBO4yspK\nSkpKKCsry3YogWrVqhVdu3alsLCw0dedC40BjDEma0pKSmjTpg3dunXDq0lpdlSVVatWUVJSQvfu\n3Rt9/bnQGMAYY7KmrKyMDh06NNskAyAidOjQIWulNks0xpitXnNOMlHZ/I6WaIAxc1YxpzTu496N\nMcY0kCUaoM+z3/HLB7/MdhjGmK3Q2rVrefLJJ9Oe74wzzmDt2kTPwMstlmiMMSaLEiWaUCj5A16H\nDh3K9ttvH1RYGWWtzowxJotuueUW5syZwyGHHEJhYSGtWrWiffv2TJ8+nZkzZ3LuueeyaNEiysrK\nuOGGG+jbty8A3bp1o7i4mI0bN3L66adzzDHH8O2337LLLrvwwQcfsM0222T5m1WzRGOMMZ47P5zK\ntCXrM7rM/bq0pf9Z+yccf++99zJlyhQmTpzIF198wZlnnsmUKVOqmiEPHDiQHXbYgS1btnDEEUdw\n/vnn06FDhxrLmDVrFoMGDeLZZ5/lwgsv5J133uGSSy7J6PdoCEs0xhiTQ3r27FnjXpdHH32U9957\nD4BFixYxa9asWomme/fuHHLIIQAcfvjhzJ8/v9HiTYUlGmOM8SQreTSWbbfdtur9F198wciRIxkz\nZgytW7fmhBNOiHsvTMuWLave5+fns2XLlkaJNVXWGMAYY7KoTZs2bNiwIe64devW0b59e1q3bs30\n6dP57rvvGjm6zLASjTHGZFGHDh04+uijOeCAA9hmm23Yaaedqsb16tWLp59+mn333Ze9996bI488\nMouR1p8lGmOMybLXX3897vCWLVvyySefxB0XvQ7TsWNHpkypfpLK3/72t4zH11BWdWaMMSZQlmiM\nMcYEyhKNMcaYQFmiMcYYEyhLNMYYYwJlicYYY0ygLNEYY0wW1fcxAQAPP/wwmzdvznBEmWeJxhhj\nsmhrSDSB3bApIgOBXwErVPWAOONPAD4A5nmD3lXVu7xxvYBHgHzgOVW9N6g4jTEmm/yPCTjllFPY\ncccdefPNNykvL+e8887jzjvvZNOmTVx44YWUlJQQDof55z//yfLly1myZAknnngiHTt25PPPP8/2\nV0koyJ4BXgQeB15OMs1Xqvor/wARyQeeAE4BSoAfRGSIqk4LKlBjjAHgk1tg2eTMLnPnA+H0xOfK\n/scEDB8+nLfffpuxY8eiqpx99tmMHj2a0tJSunTpwscffwy4PtDatWvH//73Pz7//HM6duyY2Zgz\nLLCqM1UdDayux6w9gdmqOldVK4DBwDkZDc4YY3LQ8OHDGT58OIceeiiHHXYY06dPZ9asWRx44IGM\nGDGCm2++ma+++op27dplO9S0ZLuvs6NE5EdgCfA3VZ0K7AIs8k1TAvw80QJEpC/QF2C33XYLMFRj\ngG8fg5+dDh33ynYkJghJSh6NQVXp168fV111Va1x48ePZ+jQodx222388pe/5Pbbb89ChPWTzcYA\n44HdVfVg4DHg/fosRFUHqGqRqhZ16tQpowEaU0PFZhh+Gww8LduRmGbE/5iA0047jYEDB7Jx40YA\nFi9ezIoVK1iyZAmtW7fmkksu4aabbmL8+PG15s1lWSvRqOp63/uhIvKkiHQEFgO7+ibt6g0zJjdU\nbMp2BKYZ8T8m4PTTT+eiiy7iqKOOAmC77bbj1VdfZfbs2dx0003k5eVRWFjIU089BUDfvn3p1asX\nXbp02WobAyQlIjsDy1VVRaQnrnS1ClgL9BCR7rgE0xu4KFtxGmNM0GIfE3DDDTfU+Lznnnty2mm1\nS9LXXXcd1113XaCxZUKQzZsHAScAHUWkBOgPFAKo6tPABcAfRSQEbAF6q6oCIRG5FhiGa9480Lt2\nY4wxpgkKLNGoap86xj+Oa/4cb9xQYGgQcRljjGlc1jOAMSnTmFfTXLjKlOYtm98x282bc8IRMp0y\nWmQ7DJPrtoKD0daoVatWrFq1ig4dOiAi2Q4nEKrKqlWraNWqVVbWb4kGuKvwBRbqTkDuX1QzOcAS\nTrPStWtXSkpKKC0tzXYogWrVqhVdu3bNyrot0QAh8ikgnO0wTM6zBNMcFRYW0r1792yH0azZNRog\nRAGFhLIdhsl1VpIxpl4s0QCVVqIxKbHGAMbUhyUaIKT5FIqVaEwdrERTbdUcWPRDtqMwTYRdowEq\nKaA15dkOw+Q8SzRVHjvMvd6xLrtxmCbBSjRY1ZlJkZVojKkXSzS4xgCWaEzKLOEYkxZLNLjmzdbq\nzKTOEo0x6bBEg1WdmRRZScaYerFEg2t1ViCWaExdLNE0OWXr4I52MOXdbEeyVbNEA5yX/zW7yCrY\ntCrboZhcZiWapmf1XPf6zcPZjWMrZ4kGKJCIe7NuUXYDMTnOEk2TYycHOcESjV++9eBskogetOzg\nlbpxL8Lkt7MdBdA8e2VuKuyGTT+NZHf9K36CcAV0Pji7cZgELMGk7UPvkcQHXpDdOExWWYkGuL7i\nWvcmnGLvAONegq/+l/lAnjwSnjkuc8tb8ZP7oUeykEBVYfG4xl9vo7CE03TY/yoXWKIBVtPGvQlV\npDbDh9fDZ3cGF1CmDOrjqi7WzGv8dY97AZ49CWYOa/x1ByWXq8zWLoTSmdmOIvdE/2XN9IFmTYVV\nnQGV6jZDOFROfpZjyYjKLbBkYnZjWDHdvUZb/TQLOZxoHj7QvVrfYwlYoskmK9HgbtgEqKhIsUST\n6z7+G7zQKzslmajoGWQulwLSZY0BmqB6/K++vB8Wj898KFuxwBKNiAwUkRUiMiXB+ItFZJKITBaR\nb0XkYN+4+d7wiSJSHFSMUZVewa68bEvQq2ocy+Nu8kYWPYMM8KD8aT8Y0T+45ddiCaZJCFVAxaaa\nw9KpOvv8Hnj2xOrPkUj2awiauCBLNC8CvZKMnwccr6oHAncDA2LGn6iqh6hqUUDxVQk1tRLN4z3h\ng2sTj4/9UWWjfroxSjTfPdm4N+JZSaZpGHga/LtL5pb3zcMw4HhYNDZzy9zKBJZoVHU0sDrJ+G9V\ndY338Tuga1Cx1KUiWqIpL3MDxr0Ea+ZnK5y6rZwBE17JdhR1qEeJRhXCudy5qSWaJmGJr9or3ZOD\ndYtrD1v6ozeupP4xbeVy5RrNFcAnvs8KDBeRcSLSN9mMItJXRIpFpLi0tLReK4+WaHYdda0rdn94\nPQw8vV7Lyg05cOGzPqWoUXfD3R2gsizz8WRC1UGrkRLOsikQrmycdTV7Ke6Pb/+++v2sEZkpxW5e\nnXqL1mYq64lGRE7EJZqbfYOPUdXDgNOBa0Qk4c0lqjpAVYtUtahTp071iiHa6gyAeV+61w1L6rWs\nnFDrIJ/FxJPOD7V4oHuNrV/fGq2eB08fDcNvy3YkTVyaicK/7712AUz/qOEh3N8d3rqs4ctpwrKa\naETkIOA54BxVrerRUlUXe68rgPeAnkHGUelv1PxaDt7BPOWdNEtYOVCiqZLOD70RGhA0SCPGtdn7\nOWTrukDpzOZVVZRqCTv2xGjDMjLyf5/xccOX0YRlLdGIyG7Au8D/qepM3/BtRaRN9D1wKhBoM6rK\nXL+d6O3LYeG3qU+fbrXVljXwwTWZLUnUpzFArt9UF/td1pXAkOsDqt6q57ao2ASvnAer5jRs9U8c\nAQ/t37BlBG3TqgAeAZBkfw16/6zYDKEUeydpYoJs3jwIGAPsLSIlInKFiFwtIld7k9wOdACejGnG\nvBPwtYj8CIwFPlbVT4OKE0DyczzRRKV80E7zB/Hl/TDhVRj/ctoh1R1DPc4Gc7Z1V0xcH94I41+C\nOZ9nJ5x45oxyfyNurx6m2jyrI0t/cq9jn008Tbr7Urz+Dhtrf/x3Z3g8zUa2G5Y1iXt+gmx11kdV\nO6tqoap2VdXnVfVpVX3aG3+lqrb3mjBXNWNW1bmqerD3t7+q3hNUjFEFha2CXoXbWVfOatgyIik+\nnC3d5s1VNyI2sE+0RT/AqHtqrjOtH2mOV53V+i7e54ae6ZYUw7tXZaZPunjb+4fnXHPfNQsavvxk\nlk1OfR/NhOj+KqkcxupZdeYGJl/G9I/hwX0yUxpZuzC96Z/oWfOenxyV9cYAuSC/oDD4lRQ/785W\nFn7nqqq++l/6BxZN9Uec5oEvU/e8PH8yjL4/JgaFqe+lVr2RKI5IBL56EDbWr1VhYDJ1pvvab2DS\nYLdf1F5Jw5f/04fudXU9qtOiLQBXzUneGnDZZHj6GPjyvvTXUV/awEQfroTHj4AZvgqTZCdbidbz\nyc2wYSlsXJ58fWVJugfanPBOkPovM4dYogE0L+AeztbMh4//6t6XznBdxHx2J8xNs8olkuI9Jmn/\n8DJc96xaM2m8dVnNZqN1xhFzcF3wNXx2Fwz9awaDrIdMVqFM+8BdX5g1svqM3H+QS+VfsmAMPLA3\nlK1PPl3V8usR/z07uaa5jx0G71yReLro/SdLJqS/jvpKqUST5DtvXAErZ8JHNyafviH/d/+85Ruq\n38//xv3/Nyxzn184I73lTngVZo+sf1yNzBJNMjM+SX6hd/3S2sPi1Ycv87VliISqdzj/sl+7sO54\nUq6WiHOUKimuXYKa+r47U5UMV1lphIZdo/HiVIVNK2FTjpVkwB0kFjegd6Q3f+dev3vClwji/H+T\nHeRG3Q0bl8Enf0++rnQTTa0Spbefzv4shZlTPGlZOQtmDk9t2kSqEk0K60w6jW9c0m2UYBmx8ywe\n70p4sePEd0L7/VPudeF37jV6vSlVH1wDr56f3jxZZInGU6FxSjWDeldXBWxY7npF9hsSpxuYaH24\nv67Vv5MnKpXMitOdfiQCy6dWf0616iz2R7VgDDz3S/ja9wydSBjeuhSePMq3/Aw9tyYS9pVo0pgv\nOk80oX7zCPx3z+ru7wtbZya+eDavdq1+kor5MpmotlCFaInav/1TaZgR3Zd+HORfYO3p4pWYKsuq\nD4axvnsydgHJ49i0EsrrKFXFerwIXv9NevPUEq06q+9hLF7pJeY3EC9BqcL8rxMnpWdPdNWIscur\nUWKN8z+JGnZr/JPYJswSDW5/2UyCBgHRhPHgz+Dlc2qOi5c0pn3gXkfeAdOHuvf+H0K8eRIdsL56\nEJ76hW/eODvl5tVxusuJ+XGsW+ReV/zkvuzGFfD5v92wcHnmm21GQtSvROPN8+4f3Gv0WTZV93Ok\nGeeSCamXAu/vXvOi6vShMG90zWkSnu364nr0MHjv6gTTxaPV+4c/1nEvxp98/Ctw946uq55k383/\nP61K+r79Z8h17mC4aaU7iYrut+BKurEx1niN8d89q/9njdlEXVNINKmU4vwxp3KNZvJb8OKZMPH1\nupddY5v53yf5fYx5PP5JbFSyVnY5yhKN58qKBPX/m1dXX6Rd9H3yhSyZUN3yZMo7MLiPN8K3I9eo\nivN2sv/uFX95sVUz8Uo0T/SER2Ie/Rz7Y/f/2L5/Gh7oEXMg811PCVe6KsN0+Q966i/R1PFDD1XA\nO3+omSwXjnEXaBtSYlg8DgacAKMfSD7d/G+qS0yl0+Gls2DeV+5/99JZMRMn+i6+4avnxJQwUlCV\naOKchKyc5falqGH/cCcHlZvSKOHGOXte5FXZVGyEV851VXnREl3sgbs+Jd1wZQolRNz1pZJ6VkH6\nr9FUbqnukywub39cOsmtb8OymtdMqhcaZ1DMsNXe4zdqPYYjQeknasYn1THW9ftI1oJt6N8Sj8tR\nTeQGkuDN1AR9es4eAfd1q3sBm1a6A1us0pnw6c01h8XuZOFE/SDF7LixZ7CRSIrXL3w7c/QC4uaV\nteNB4Yt74asH4JJ3YdefQ0FLyE+hVZ6/k89ICEb/t/a645k3Gia/6e6E9yfIQb9NPE8knPxR2qvn\nVnfrnvTgA7wYcxF23uiazdBLiqGrd29DEPdTaIISTVTlJnfD7gFefby/iirl5u5JqmmgutlzNHHF\nnqhEL1in44UzoCSFXg0GXwTzv4Jbl0O6txlEv0/JD/D+n2Dqu3BT7IP2Yv5nzxwbZ0He9920snbz\n4o//CnufWXO62PlSiRGqE8Qd61Kv7qssS3+75CAr0XhCDc25lQnO3gacUPNsXYSqHXTia64NfiKx\nP/jYM96XfpXafP4DZNyDk69qZZpXbbJ5FfxnF3g9hUYKULN5pn8ddd475MU25zMIJWg+u3lVzc/T\nP4LP/+VbX8wB9NFD4eO/1Fx+fX10o9t+C75N0uqvIdVFvkSzem6K1THeOuP9L+Mlw6qDmjeuckv8\n0kaiVlw/Dk4xJi8uSC3JgCt5QhpN932i8W5ZU31R3d9H4ddpPkLi/T+mH0OsWg0c6qhuTZT81bst\n4J6dYPk0N2zzanjl1w2PMQss0XhC9XmI84IxMNfrhDPRwaYyyR3Z0z9yZ3Spiv0xLvim+n240p29\nl62vfeD1t86JV3ryP9Nl1eya4+aMgi1ra7acWzk7eYshf/XglLcTTwc1D4xx7yMBZsZU5cW2BPQn\nnUwrnQHjXoAXTndnzIkMvhiG/zP95c8bXX1gf/037mCXak+/6R6co/vB//bzlWj9DVXCtYe5Gd1L\nvBOBdG8wrBVTiicCP33oWvrd0a66gYz/txM9CYhehAcY2R++fsi9T3btKDouYTPxFGOc81nNBg6b\nV9f+PcWuc9Td8bsL0kj1Nd5lk9zruBfcOpogqzrzdGy7LaTbk3e4HF4+O72icFrqqDrze+/qJAd1\n3w+lKjkJtX5AiX70L53ldvbo8+gfP9y9Fl0OJ/wDtovpNTuczh3S9ShxxB40vnoQjr/FXW/45pH0\nl5dMuKL6QJCo/7A182r38jtxEBzcO/4BLvaAFnutIF7Jaf43sN1O1Z+nf1z/qrMtCW4OHPci7NA9\nTok4yTWahw9MLYaol852NzfGSrTvlYyD5ZPhxzeqhz31C2i3a8z8CbbFrFSaUHvft66GDCKuNaC/\n4YTfhpgbNh/cJ8lvwVvX2oUJTjZ922P+V64ae9EPiWOb+wXscULi8VlmicbTfcc2kEpntVvW1h72\n371Sv9fD31w5kc2rofUO6f3gk5Uc4s0neXF+nAl+7NEzqi//W7P0UDzQ3TP06wE1Y031jHzZZCh+\nIbVp/eIl9Yf2h00rag+PdwB750rYtlPinpHjHQjBNVKIJ97F2fevhoIW1ddW/GJvaiyL2afilTpf\nPAMO7lP9+f2rax9sger/ob/VmS/RrI65hrFxRXWp+7M73etuR9WcpkZV6Gxo3y3OeqPrquNgHX0M\nRyyNuCbD65e6eDYsgxNugedOcuN3P7rm9NGWlP75k5EUaiwSnSz696Eh18WbIP58yU64/Nsp3r16\nkXD1cie86v6SGfusJZqmoEu7bVi6aAc6Sx1dQdy3e+1h6dxQOOkN6J7w8TrO0JvggudrDx95B/R+\nLfV1RUV/KJPf8g2LV79fx3LiVVFNesP9neSrNkq1ROOv5khLnINZvCQDxP1S/u2Qjrq6GIm1ybu2\ntPRHV/VzUoJny8QmlkTXgmIPlvEOUPGeEOm/YXNjzHZ6/uTa08cm1G8frX7/+OFwUJKGGgCjElRl\nxos35N2bNuwftZ8au4evuXnsPWyx6urOKVkC3LLGu/crUaJJ0gPBhmWw3tvm0ZswU1Gj37k4++jm\nVdA+zrEmkUw8NydAlmg8BfnCieUPMr1VKl2lNFDs/Rmxoj0LxNaJT//I9c2UthSrp2okkjQvcPsP\nIola0ZXOcPfGhMtrnp2naqPX+imdezUy0VJszOP1my9U5q5xvfYblzz2OAG6pZBcE22/2K6S4k03\n/Nbq96rufqloM1xVmDYklciTm/RGkpHia3EYo9aNoD7xHk0+wnfysqSOHorL62gKn6xqu2KDK80l\n2q+iPSPkxRwuR9/v69uPxNcY44k2L08kXBFMK8cssURD9WG4jJZZjaOauu4l4vVltHJm7WF1Wbuo\n7mlijbo7ven9VReJqs6e8D2/LtFZbzJzRrnXRHe0xzNrGHw/AFZMhcJtode/019vfX1xb83GIC+e\nCTcvcPcyJZOo26PYRJPoWgu4g+a6kpoHQo24Lm+yJd3HUNR131pa6jho//Rh4oYNpTPca1APRYyt\nzoQktzw0TZZoqgj7d2kL9exENaNmZvjxO3W1/IpnbZpdyvur4oLsUXbuF4nPmBP55Kbq913TfN5H\nQ8Rrcbh5FcwYmny+RIkmlesMNcQcXN/rm+b89ZDoSZJv/i5xC6zGUDrTtVhLRPISV1muj1Md2RC1\nevKII1xJg5vm5xBr3uxz6n4782bo+GyH0TT577VIdrNlQ719eQPnb4Sq0WRSaSk2LkEDiXR6GV8w\nJvjnz6QjUUutxrKxjptO007iDfDJzXVPk2pP7U2EJRqf607aiw0E2HFjc5bps75E8ls0znqCksoB\nJNE1oXhVLIlsWuG6ljGpCfpRIX6p1FiEK2t2PdTEWaLxycsT8shQD8YmGImaHjcVDTlTTemekAyt\na2uzYlq2I6gpkuTxJE2QJZoYlmhMoOzgb1LRzPYTSzQxJkR6ZDsE05w1gee7G5NpgSYaERkoIitE\nZEqC8SIij4rIbBGZJCKH+cZdKiKzvL9Lg4zTr+VhffhdRQoX64wxxqQk6BLNi0CvJONPB3p4f32B\npwBEZAegP/BzoCfQX0TaBxWk/76ou889gB8je8af8Ko6brRsTJ32zXYExjQN5w3IdgRbvUATjaqO\nJvmdKecAL6vzHbC9iHQGTgM0dPxpAAAdoElEQVRGqOpqVV0DjCB5wmqw6E3BLQryuO/i43io8vxa\nJZtuj5Rwy15ZbqYZVZ9u1QF6nJbZOIzJdQel+KiLXLZPgkeCNBHZvkazC+C/bb3EG5ZoeKPodWBn\n9u1zD6MjB9caN3hK9U14CZ/K2QgiqfbcG6sxm3E2BV0OzXYEzh9GZTuCtIXOfLTuiXJBYz5eOiCR\nuE8DbTqynWgaTET6ikixiBSXlqbRuWUdeh3QmZt77ZNw/Hrdps4Wao+F4t/HcFvl71mvte/XOazs\nad4K1exwc6W25d+VtfsFW7hyY9J1Px46h39WXlZr+N/nH171fkzH39Qa31Ddyl7npsq+fBveL+PL\nDsLEEtdl/4xIgiespqlM4z+NdGMd92cd93gdTwKNsUXTu59o+lF1PNK6Hs55N/cPfuV5ren18Gj6\nVvy5athHx2Tm/pT/VPZhr7Lk3erM2v5Y7iqo3ePzR+EjE84T79iQl6jX6yYi24lmMeDv67yrNyzR\n8FpUdYCqFqlqUadOneJNUm9/PCH+tZojyp7kmPJH+TpyIN9H9mFCZC82ais+DvfkrdBxbNBteDt8\nHE+Gzq6a56vwATwSOo9BoRN5NXwKB5U/x78qL+bjsOv/6/bKS1lNW24KXc1q3a5qvlHhQxkQPovv\nIy7pfRp2nWpO1eQ9u4bI59XwyVxWcVON4R+sq/5OE5cleKKlJ94On4q3widwUeVtfBT+ecrznF1e\n3bfanEjnpNNeV3FtveKK+ja8Hz9F3O61WtsA8JTvfwUwLk7rw9sq6+5VYIZW77aDQifyqHeyMTB0\natL5KjW93qB+UV5dmrin8iJuq/w9z4VOjzvtfmUD6fV5F66uuDHh8m6trO5x4dCyp/kh8rM6Y1ig\nO9U5TaquqriRD8K/SGnaPcvidMAZx2Ohczlu8/1MX7aB4ZHqzmivHVl37+IXV/Tj+PIkjwsHngn/\nihAF3Fjxp6phlVpdY/Bo6Fx6LevLipiHmf6s7CWurbyeg8viXzs6qPzZOuPbrK5fxqHe8WNmpNEq\nfOol232dDQGuFZHBuAv/61R1qYgMA/7tawBwKtAvW0EC8IdRzOtyGOIVw1WVp76cw4D5j3PFsd25\nfVwJ7453ufCm0NVVs+1T9gIVFBKJk9OfC58JYbgm5t6saypv4NHCx+kk63g87A5Uv624vWr8KeFi\nRkcO4sVQL/Ilwu6ynPsK3c45IbIXh+bN5vvIvih5fBepWbIox50JL9YORGJ6aP40fAS98t3DlSo0\nnyPKn2RGq8sAV1JpRXlV79YRFW4LXc4tBa/TVuJ34X5t5Q2U8yTn539dY/ifKq7nJ92dz1u6qseT\nyh9grnapGr9YO7IntW/MHBYu4qpK94jmCeU92KDb0EnWMrLl3+OuP5EPI0fRmjL+mfca/6i8gtMj\nYxkS+QX7hhZwVcHHnFd+JxO0BzcXDGJHWcOOrOXY/Cm8Hz6ai/M/Y9+8hSzT9uws1b31Dgz1YmJk\nLz6O/Jw5rf4PgH6hP7AzqzgjbyxvhY/n+oL3q6b/PrIPvStu4+uWN/Bl+KC0e7VaQ1uOKHuCVbSr\n2rfasokrCz6pNW2Z9z//NNKzxvByLaSlVHJ5xd+YpdUHqjW05eKKW9lbFnFE3gxuL6x9YF+nrdlE\nK+6uvIQrCoZyWcXNhMhng7ZmUIt/sVfekhrThzSP8yvuYKHuyIRW7vfxUugULi0YwVUVf2ZY5Aj2\nlhKSPej2kdCveShUs2PLIeGjODt/DMeUP8LZed/w98I3AZgc6caDoZrXZq6vuIbTvP37nsqLuLXw\nda6s+Cv/KHiNPfKWcUvllcyMdKWtbOabiHug20nlD3B2/rfsyFouKoit3nS/n4nqTt4+DR/B1ZV/\nZn4r9yCzh0IXoORRqttXzTEn0pkKXKl3HdtxbvldvN/y9lrL/Ta8H7/Ij38T6WfhQxkZOYz/FD7P\nB+Ff8E74WBbqToxIvOmyLtBEIyKDgBOAjiJSgmtJVgigqk8DQ4EzgNnAZuD33rjVInI3EH2k3F2q\nmp3uLs94ABaPh10Or3FYFhH+dMJeVZ9/sWdH+vTcjTatCtir03YU5NdMLNOWrGe/Lm2pDEcoWbOF\nGcvW0+uAznw5s5SDdmlHWSjMti0LOOiO4YyJ7M/zRw5n2xb55I0vYceKMGcd3IXxC9cwYeFaRkRc\nx5DHnnw2x/2sE5+9eh+Uw+uhE/lH6A+0oryqJ2r/I6pnRLqyfetC/r3bQMaWFrL32pqt6O5rdxsT\n9tuJBV8P5ofI3vz2qB7cNfb/GO+d3ft7t963/AXKacFn4UP5vpUrYUwsOKjW5nsrfALn53/NMeUP\nc1peMQflzWVoxFUbXN7iAeZsyGOB7lxjnkW6IwAXlv+Tw1ss5GZ5CYB7Q64K8bE+h3LdIPfwsHW6\nLZ+FD0VQLq/8O3d1+Y69Vw7n53nTAXeg+H3+p7wTPo6JuidFMoNi3RuAl8OnUUkBL4RdSeC+UB+G\nhY9ggvao+gywM6vYI7yUjbTmvIo7aU05Z/5sW2TOSLahgn6Fg6gknyERd0Z+WNnT5BOhW4fWzF8F\nJ1fUrrYasdOV6KI8ji5/DIBtqC5dLmuzP490f4b/THKPFOhW9nrVweuF0Gm03aYQyqCUmg0xN8fp\nffy2yt9z4K47MLlkLdu3bsHfC/tzf5l7wNm74WPoU/A5YyP7sIlWvB0+jvfD7gFjFRQyWfdgcngP\nKijgX4UvcEXFX3m+xYMAHFz+LCA8Hz6D58NnANBum0LWbamkX+WVvNXyLgC+i+zLdx3P54Ol7Zmn\nrqQ6J9KZPfOW8kr4FPqHqkuJ70aO5ff6KU+HzqJf4aBa36XsyBvo9tMa5q/aTEjzmK27cGPoGv5S\n+UdCFPBk+NyqRPPrCrf+PIGIl8WHRI5mSMR9v2fDZ/J15EB+0t0ZWXE49/fczOCx29BjxzaMX+Gq\npXdu24q567vwsJfcaicaZ752Zo+yV6sS/r2VvZmoe6Hk8auDOnNI1324fdhCPggfzTq2qzHvRN2r\nxufLK9xD9C6tvIWWlRUowtRWV7BBt+E3Ff1Zrttz4mH78u74EmZEduW6Sy9mwapNHNMjs7U5Gaeq\nzebv8MMP1/o4/O4R2u/dSfWaN9PGzlulk0vWJhwfDkf0z4Mn6I+L1vhmek61f1td+8afVFV13ZYK\nDYUjVaM3Lputc5aU1l5YJKI6c7jqPbuo9m8bd32vfjdf+38wpXpA6SzVZVN0U3ml+7y2xM37wN6q\nqjpi6jJdsb4s7rL8McX14Y2qH1yrWr5J9cc3XHyRiOqGFao/fZx01vLKsC5avUlVVV8eM9/F9OJZ\n3teMaHllWMsrw0mXEQpHdMnazTU+L1i5SUs3uO8TDkc0Eqn+DpFIRDcsnamRO7ZXXf5T0mUvX7dF\npy0qVQ3Hj2Hdlgotr6h0/8v1S93AknGq3zzq3vdvq3rv7gmXP33pev1x0RrdWDxYly6cpbphudue\nlXH+F/3bqvZvq6HKCp047SfdUhGqMXr9lgpdsnazbqkI6Z9eG6fvjVukunSy+86r5+nS+dM1XNf/\ncsmP7n8Xz8MHuRhKZ9XYplsqQloR8rbPMye4aT64zr0Ou7XGIkrXbdLKysr43+3hg3X5+i21Ri1c\ntUm/nLFCN5RVVq3vkZEza33/8QtWV/0Gt1SEdHN5qHrZgy9W7d9Wy0Y/piEv9pHTltXaHpFIRCcu\nXKOJrFhfpqFwxH3fZ092y57+iU4uWatTF6+rOfG8rzW8al6tODMFKNaAj82izejhOkVFRVpcXJz+\nfP8ayan778S/z0vz+ee5YtMqeOks+O0r0CHBPUDJVGxyvQq3apv+vJtXw/3d3YPMzqvjOSuNTbVZ\ntDgC3DOFWm4H22TgdrKRd7qOO/+ZucYzaRn3Enx4PfxjCbTYNv405Rvd83a23y29Za9f6pZZn305\nVZGI268ytW+FK91TWBvzERY+IjJOVQNduSUaoOhfIzh1/52bbqLJttIZ7jnyBbny4DhjTKoaI9Fk\nuzFAzmgm573Z0WnvbEdgjMlh2W7ebIwxppmzRGOMMSZQlmiMMcYEyhKNMcaYQFmiMcYYE6iUEo2I\n3CAibb0HlT0vIuNFJHnnTcYYYwypl2guV9X1uD7H2gP/B9wbWFTGGGOajVQTTfQ2kzOAV1R1Ks3o\n1pNmdM+qMcbknFQTzTgRGY5LNMNEpA3U8TCWJqa59FRijDG5JtWeAa4ADgHmqupmEdkBr6dlY4wx\nJplUSzRHATNUda2IXALcBqwLLixjjDHNRaqJ5ilgs4gcDPwVmAMkf4apMcYYQ+qJJuQ9t+Ac4HFV\nfQJoE1xYxhhjmotUr9FsEJF+uGbNx4pIHt6TMo0xxphkUi3R/BYox91PswzoCvw3sKiMMcY0Gykl\nGi+5vAa0E5FfAWWq2myu0dhtNMYYE5xUu6C5EBgL/Aa4EPheRC4IMrDGJs3n/lNjjMkpqV6juRU4\nQlVXAIhIJ2Ak8HZQgRljjGkeUr1GkxdNMp5VqcwrIr1EZIaIzBaRW+KMf0hEJnp/M0VkrW9c2Ddu\nSIpxGmOMyTGplmg+FZFhwCDv82+BoclmEJF84AngFKAE+EFEhqjqtOg0qvpn3/TXAYf6FrFFVQ9J\nMT5jjDE5KqVEo6o3icj5wNHeoAGq+l4ds/UEZqvqXAARGYy7D2dagun7AP1TiccYY0zTkWqJBlV9\nB3gnjWXvAizyfS4Bfh5vQhHZHegOjPINbiUixUAIuFdV308wb1+gL8Buu+2WRnjGGGMaQ9JEIyIb\niN/6VwBV1bYZiqM38Laqhn3DdlfVxSKyBzBKRCar6pzYGVV1ADAAoKioyFoqG2NMjkmaaFS1Id3M\nLAZ29X3u6g2LpzdwTcy6F3uvc0XkC9z1m1qJxhhjTG5LtdVZffwA9BCR7iLSApdMarUeE5F9cE/t\nHOMb1l5EWnrvO+KuDSW6ttNgak8+M8aYwKR8jSZdqhoSkWuBYUA+MFBVp4rIXUCxqkaTTm9gsNY8\n2u8LPCMiEVwyvNffWi0I9uAzY4wJRmCJBkBVhxLTDFpVb4/5fEec+b4FDgwyNmOMMY0jyKozY4wx\nxhKNMcaYYFmiMcYYEyhLNMYYYwJlicYYY0ygLNEYY4wJlCUa7AmbxhgTJEs0Hrtf0xhjgmGJxhhj\nTKAs0RhjjAmUJRpjjDGBskRjjDEmUJZojDHGBMoSjTHGmEBZogHsuWfGGBMcSzQesSefGWNMICzR\nGGOMCZQlGmOMMYGyRGOMMSZQlmiMMcYEyhKNMcaYQAWaaESkl4jMEJHZInJLnPGXiUipiEz0/q70\njbtURGZ5f5cGGacxxpjgFAS1YBHJB54ATgFKgB9EZIiqTouZ9A1VvTZm3h2A/kAR7nEx47x51wQV\nrzHGmGAEWaLpCcxW1bmqWgEMBs5Jcd7TgBGqutpLLiOAXgHFidodm8YYE5ggE80uwCLf5xJvWKzz\nRWSSiLwtIrumOS8i0ldEikWkuLS0NBNxG2OMyaBsNwb4EOimqgfhSi0vpbsAVR2gqkWqWtSpU6eM\nB2iMMaZhgkw0i4FdfZ+7esOqqOoqVS33Pj4HHJ7qvMYYY5qGIBPND0APEekuIi2A3sAQ/wQi0tn3\n8WzgJ+/9MOBUEWkvIu2BU71hxhhjmpjAWp2pakhErsUliHxgoKpOFZG7gGJVHQJcLyJnAyFgNXCZ\nN+9qEbkbl6wA7lLV1UHFaowxJjiBJRoAVR0KDI0ZdrvvfT+gX4J5BwIDg4zPGGNM8LLdGMAYY0wz\nZ4kGd0eoMcaYYFii8dhzz4wxJhiWaIwxxgTKEo0xxphAWaIxxhgTKEs0xhhjAmWJxhhjTKAs0Rhj\njAmUJRpjjDGBskQDdsemMcYEyBKNR7A7No0xJgiWaIwxxgTKEo0xxphAWaIxxhgTKEs0xhhjAmWJ\nxhhjTKAs0RhjjAmUJRpjjDGBskSD3a9pjDFBskTjsSdsGmNMMAJNNCLSS0RmiMhsEbklzvi/iMg0\nEZkkIp+JyO6+cWERmej9DQkyTmOMMcEpCGrBIpIPPAGcApQAP4jIEFWd5ptsAlCkqptF5I/A/cBv\nvXFbVPWQoOIzxhjTOIIs0fQEZqvqXFWtAAYD5/gnUNXPVXWz9/E7oGuA8RhjjMmCIBPNLsAi3+cS\nb1giVwCf+D63EpFiEflORM5NNJOI9PWmKy4tLW1YxMYYYzIusKqzdIjIJUARcLxv8O6qulhE9gBG\nichkVZ0TO6+qDgAGABQVFVkDMmOMyTFBlmgWA7v6Pnf1htUgIicDtwJnq2p5dLiqLvZe5wJfAIcG\nGKsxxpiABJlofgB6iEh3EWkB9AZqtB4TkUOBZ3BJZoVveHsRaem97wgcDfgbEWSUqhWEjDEmKIFV\nnalqSESuBYYB+cBAVZ0qIncBxao6BPgvsB3wlrgbWRaq6tnAvsAzIhLBJcN7Y1qrZZzdRmOMMcEI\n9BqNqg4FhsYMu933/uQE830LHBhkbMYYYxqH9QxgjDEmUJZojDHGBMoSjTHGmEBZojHGGBMoSzTG\nGGMCZYnGGGNMoCzRYA8+M8aYIFmi8diDz4wxJhiWaIwxxgTKEo0xxphAWaIxxhgTKEs0xhhjAmWJ\nxhhjTKAs0RhjjAmUJRpjjDGBskQD2AM2jTEmOJZoPGJ3bBpjTCAs0RhjjAmUJRpjjDGBskRjjDEm\nUIEmGhHpJSIzRGS2iNwSZ3xLEXnDG/+9iHTzjevnDZ8hIqcFGacxxpjgBJZoRCQfeAI4HdgP6CMi\n+8VMdgWwRlX3Ah4C7vPm3Q/oDewP9AKe9JZnjDGmiQmyRNMTmK2qc1W1AhgMnBMzzTnAS977t4Ff\nimv+dQ4wWFXLVXUeMNtbnjHGmCYmyESzC7DI97nEGxZ3GlUNAeuADinOC4CI9BWRYhEpLi0trVeg\np+2/E/vs3KZe8xpjjEmuINsBNJSqDgAGABQVFdXr1suHex+a0ZiMMcZUC7JEsxjY1fe5qzcs7jQi\nUgC0A1alOK8xxpgmIMhE8wPQQ0S6i0gL3MX9ITHTDAEu9d5fAIxSVfWG9/ZapXUHegBjA4zVGGNM\nQAKrOlPVkIhcCwwD8oGBqjpVRO4CilV1CPA88IqIzAZW45IR3nRvAtOAEHCNqoaDitUYY0xwRJtR\nj5JFRUVaXFyc7TCMMabJEJFxqloU5DqsZwBjjDGBskRjjDEmUJZojDHGBMoSjTHGmEA1q8YAIlIK\nLKjn7B2BlRkMpzFYzI3DYm4cFnPjiI15d1XtFOQKm1WiaQgRKQ665UWmWcyNw2JuHBZz48hGzFZ1\nZowxJlCWaIwxxgTKEk21AdkOoB4s5sZhMTcOi7lxNHrMdo3GGGNMoKxEY4wxJlCWaIwxxgRLVbfq\nP6AXMAP3uOhbGnG984HJwERcb9YAOwAjgFnea3tvuACPejFOAg7zLedSb/pZwKW+4Yd7y5/tzSvJ\n1pEgxoHACmCKb1jWYky2jjpivgP3PKOJ3t8ZvnH9vOXNAE6ra78AugPfe8PfAFp4w1t6n2d747vV\ntQ7f+F2Bz3G9lU8Fbsj1bZ0k5pzd1kAr3ONGfvRivjPT68nkd6kj5heBeb7tfEiu7BtxjyWNdWDN\nxT/c4wvmAHsALbx/5n6NtO75QMeYYfdHd07gFuA+7/0ZwCfeP/hI4HvfjjDXe23vvY/uDGO9acWb\n9/Rk60gQ43HAYdQ8aGctxkTrSCHmO4C/xZl2P+9/3hJ3IJjj7RMJ9wvgTaC39/5p4I/e+z8BT3vv\newNvJFtHTBydoz9WoA0w05svZ7d1kphzdlt732U7730h7sB+ZKbWk8nvkkLMLwIXxNnOWd834h5L\nGuOgmqt/wFHAMN/nfkC/Rlr3fGonmhlAZ+99Z2CG9/4ZoE/sdEAf4Bnf8Ge8YZ2B6b7hVdMlWkeS\nOLtR86CdtRgTrSOFmO8g/sGvxv8b9+ykoxLtF94PayVQELv/ROf13hd400middSxzT8ATmkK2zpO\nzE1iWwOtgfHAzzO1nkx+lxRifpH4iSbn9g1V3eqv0ewCLPJ9LvGGNQYFhovIOBHp6w3bSVWXeu+X\nATt57xPFmWx4SZzhydaRqmzG2JD/17UiMklEBopI+3rG3AFYq6qhOOuvmscbv86bPq2YRaQbcCju\nzLVJbOuYmCGHt7WI5IvIRFz16ghcCSRT68nkd0kYs6pGt/M93nZ+SERaxi4vxdga5Xe4tSeabDpG\nVQ8DTgeuEZHj/CPVnSpokAE0dB1NIUbPU8CewCHAUuDBhsYVBBHZDngHuFFV1/vH5eq2jhNzTm9r\nVQ2r6iFAV6AnsE+WQ6pTbMwicgCupLQPcASuOuzmgGNo0P63tSeaxbiLmlFdvWGBU9XF3usK4D3c\nTr9cRDoDeK8r6ogz2fCucYaTZB2pymaM9fp/qepy78caAZ7Fbev6xLwK2F5ECmKG11iWN76dN31K\nMYtIIe6A/ZqqvusNzultHS/mprCtvTjX4hozHJXB9WTyuySLuZeqLlWnHHiB+m/nRvkdbu2J5geg\nh4h0F5EWuItxQ4JeqYhsKyJtou+BU4Ep3rov9Sa7FFfvjTf8d+IcCazzirTDgFNFpL1XRXEqru53\nKbBeRI4UEQF+F7OseOtIVTZjTLSOpKI/Fs95uG0dXV5vEWkpIt2BHrgLo3H3C++s7nPgggSxRWO+\nABjlTZ9oHf74BHge+ElV/+cblbPbOlHMubytRaSTiGzvvd8Gd03ppwyuJ5PfJVnM030JQIBzY7Zz\n7v0Ok13A2Rr+cC0oZuLqam9tpHXugWuREm2yeKs3vAPwGa454UhgB2+4AE94MU4GinzLuhzXzHA2\n8Hvf8CJv55sDPE51k8W460gQ5yBc9Uclrh72imzGmGwddcT8ijf9JO9H0tk3/a3e8mbgtbZJtl94\n/7ux3nd5C2jpDW/lfZ7tjd+jrnX4xh+Dq5aYhK9ZcC5v6yQx5+y2Bg4CJnixTQFuz/R6Mvld6oh5\nlLedpwCvUt0yLev7Rrw/64LGGGNMoLb2qjNjjDEBs0RjjDEmUJZojDHGBMoSjTHGmEBZojHGGBMo\nSzSmWRKRDiIy0ftbJiKLfZ9bpLiMF0Rk7zqmuUZELs5QzOd48f0oItNE5Epv+K9FJOfvYDcmEWve\nbJo9EbkD2KiqD8QMF9xvIJKVwGrG0hLX7XuRqi7xPu+uqjNF5FXgbVV9P7tRGlM/VqIxWxUR2csr\nLbyGu1m2s4gMEJFiEZkqIrf7pv1aRA4RkQIRWSsi93qljTEisqM3zb9E5Ebf9PeKyFgRmSEiv/CG\nbysi73jrfdtb1yExobXD3Qi3GkBVy70kcyzuJsCHvNJONxHpISLDxHXIOlpEfuat51URecobPlNE\nTveGHygiP3jzTxKRPQLdyMbEsERjtkb7AA+p6n7q+py7RVWLgIOBU0RkvzjztAO+VNWDgTG4u6zj\nEVXtCdwERJPWdcAyVd0PuBvX03EN6vq8GwYsEJHXRaSPiOSp6lfAUODPqnqIqs4HBgB/UtXDcZ0r\nPu5b1K64jhbPAgZ4JaM/AQ+o65jxCGBJKhvJmEwpqHsSY5qdOapa7PvcR0SuwP0euuAebDUtZp4t\nqvqJ934ccGyCZb/rm6ab9/4Y4D4AVf1RRKbGm1FVLxORg4CTcQ+a+iVwpX8ar9+rI4F3XM0fUPN3\n/KZXFThDRBbh+uH6FrhNRHYH3lXV2QliNyYQlmjM1mhT9I2I9ABuAHqq6lrvekirOPNU+N6HSfzb\nKU9hmoRUdRIwSURex3X4eGXMJAKs9EoncRdRe5H6ioiMAc4EPhWRy1V1dLqxGVNfVnVmtnZtgQ24\nHmw7A6cFsI5vgAvBXS/BlZhqEJG2UvOZRIcAC7z3G3CPS0ZV1wBLReQ8b748ETnYN99vvF51f4ar\nRpslInuo6mxVfQT4CNdRozGNxko0Zms3HldNNh13YP8mgHU8BrwsItO8dU3DPUnRT4B+IvIssAXY\nSPV1oEHAMyLyV1yX8L2Bp7zWdC1wvff+6E27GCgGtgP6qmqFiFwkIn1wPVovwT1u2ZhGY82bjQmY\nuAdaFahqmVdVNxzoodWP9s3UeqwZtMlJVqIxJnjbAZ95CUeAqzKdZIzJZVaiMcYYEyhrDGCMMSZQ\nlmiMMcYEyhKNMcaYQFmiMcYYEyhLNMYYYwL1//vnwck66of+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8FHX6wPHPk04TEJCjShFUbKiR\nAytWwK6nKNgb/lQUz4q93J1nO3tFDxs27HiiKB5YUYmKSu8eAZRIb+nP74+ZTSbbN9nJJuzzfr3y\nyu7M7Myzu7PzzLfMd0RVMcYYY+oiI9UBGGOMafwsmRhjjKkzSybGGGPqzJKJMcaYOrNkYowxps4s\nmRhjjKkzSybGhCEi7UXkcxHZKCL/SnU8DY2IDBSRwjiXvV1Exvkdk0ktSybGhDcC+APYTlWvTsYK\nRaSDiPxbRFa6SWquiNwhIs2Ssf4o21URWSUiWZ5p2e40u9DMJIUlE9NoiKO+9tkdgdlai6t6vQdt\nz7TtgWlAE2CAqrYAjgRaAT3rGGs81gJDPM+HuNOMSQpLJiYhIjJaRBa5Z9azReSkoPkXicgcz/x9\n3OldRORtESkSkdUi8pg7vUYViIh0c8+ks9znU0XkHyLyFbAF6CEi53m2sVhELg6K4QQRmSEiG9xY\nB4vIqSLyfdByV4nIe2He4/PAOcB1IrJJRI4QkVwReUhEVrh/D4lIrrv8QBEpFJHrReQ34LkwH91V\nwEbgTFVdCqCqy1R1lKr+7K5nfxGZLiLr3f/7e2KaKiJ/E5Gv3Pf9sYi0ded9KCIjg97DTyJysmfS\nS8DZnudnAy8GvaajiEwQkTUislBELvLMayIiz4vIWhGZDewX5rVvud/vEhG5IsxnYLZlqmp/9hf3\nH3Aq0BHnROQ0YDPQwTNvOc6BRoCdcM7wM4GfgAeBZkAecKD7mtuBcZ71dwMUyHKfTwX+B+wGZAHZ\nwDE4Z/MCHIKTZPZxl+8HrMc5688AOgG7ALnAGmBXz7Z+BP4S4X0+D/zd8/xO4BtgB6Ad8DXwN3fe\nQKAcuMfdTpMw6/sGuCPK57o9TknhLPd9DnOft/F8DouA3jilm6nA3e68s4GvPOvqA6wDct3nCuwO\n/I5TEmrtPt7dOQRUve5z4An3++kLFAGHufPuBr5w4+wCzAQK3XkZwPfArUAO0ANYDAwK9x3b37b5\nZyUTkxBVfUNVV6hqpaq+DizAOYADXAjcq6rT1bFQVX9153cErlXVzaparKpfJrDZ51V1lqqWq2qZ\nqn6gqovcbXwGfAwc5C57ATBWVT9xY1yuqnNVtQR4HTgTQER2w0lc/4kzhjOAO1V1laoWAXfgHPgD\nKoHbVLVEVbeGeX0bYGWU9R8DLFDVl9z3+SowFzjOs8xzqjrfXf94nAM+wDtAXxHZ0RPr2+57DigG\n3sc5ATgNmOBOA5ySI3AAcL37/cwAnqW6NDMU+IeqrlHVZcAjnnXvB7RT1TtVtVRVFwPPAKdHeb9m\nG2PJxCRERM52q5DWicg6nLPbtu7sLjhnz8G6AL+qanktN7ssKIYhIvKNWx2zDjg6jhgAXgCGi4jg\nJILxQQfcaDoCv3qe/+pOCyhS1WIiWw10SGD9gW108jz/zfN4C9AcQFU3Ah9QffAeBrwcZhsv4iSH\nkCoud/tr3HWF235Han4P3lh3BDoG9gn3O7kRaB8mBrONsmRi4uae+T4DjMSpfmmFU90h7iLLCN+Y\nvAzoGq5hGqearKnn+Z/CLFPVCO62U7wF3A+0d2OYGEcMqOo3QClOKWY4TjtCvFbgHDQDurrTQmKM\nYDJwUpQOBMHrD2xjeZzxvQoME5EBONVUU8Is8wVOQmsPBJcMVwDbi0iLCNtfiZOovfMClgFLVLWV\n56+Fqh4dZ+xmG2DJxCSiGc5BswhARM7DKZkEPAtcIyL7uj2vdnIT0Hc4B6O7RaSZiOSJyAHua2YA\nB4tIVxFpCdwQI4YcnHaJIqBcRIYAR3nm/xs4T0QOF5EMEekkIrt45r8IPAaUJVjV9ipws4i0cxu+\nbwUSuXbiAWA74IVAdZQb2wMisidOQuwtIsNFJEtETsNp+4i3Gm4iTjK6E3hdVSuDF1BVxak2O959\n7J23DKcd6J/u97MnTpVh4D2OB24QkdYi0hm43PPy74CNbgeEJiKSKSK7i0iNRnqzbbNkYuKmqrOB\nf+F0cf0d2AP4yjP/DeAfwCs4PZfeBbZX1Qqcg9hOOI3phTj19qjqJzhtGT/jNOJGPXi61TBX4Bzc\n1uKUMCZ45n8HnIfT2L8e+IyaZ/wv4STARC+i+ztQ4Mb5C/CDOy0uqroG2B8oA74VkY3Ap26MC1V1\nNXAscDVOldh1wLGq+kec6y8B3gaOwPn8Iy03S1VnRZg9DKcdaQVOO8xtqjrZnXcHTtXWEpw2qqpS\nnfv9HovThrME5/qcZ4GW8cRutg0SdIJizDZNRJoAq3B6fy1IdTzGbCusZGLSzSXAdEskxiRXuAZR\nY7ZJIrIUp6H+xBSHYsw2x6q5jDHG1Jmv1VzuMBbz3KEZRoeZ/6B7zcIMEZnv9k83xhjTyPhWMhGR\nTGA+zrAWhcB0YJjbIyjc8pcDe6vq+dHW27ZtW+3WrVuSozXGmG3b999//4eqtvNr/X62mfTD6fK4\nGEBEXgNOAMImE5xuibfFWmm3bt0oKChIWpDGGJMORCR4hIWk8rOaqxM1h18opObQEFXci7i6A//1\nMR5jjDE+aShdg08H3nQvfgohIiNEpEBECoqKiuo5NGOMMbH4mUyWU3Msn85EHmfodJzhKsJS1TGq\nmq+q+e3a+VblZ4wxppb8TCbTgV4i0l1EcnASxoTghdxxk1rjDNFhjDGmEfItmbjDjY8EJgFzcIb7\nniUid4rI8Z5FTwdeCx54zhhjTOPh6xXwqjoRZzRT77Rbg57f7mcMxhhj/NdQGuCNMcY0YpZMjNlW\nlW6GypDbmph4rFkMi+xKhURYMjHxWfEjTH821VHUtL4Q3h8FFWX+b6tsK5Ruib1cRTlsWuV/PLEU\nb4C7OsKUf6QuhlnvQNH86Mss/yG+z7W+PbI3vHRSctd5e0sYd0py19mAWDJpTOZ9BD+/kZptjxkI\nH1wN5RFumb5iBvyR4KjuE6+D6f+Of/mt65yDdWUl/DYTnjwAvn8elnye2HZr494ecFe0W7i7Pr4Z\n7u/lHMwjKd4Aa5ZUP19f6CTEOf9xDjibYlxLVVEOW9dGX6Z0s/P/x0TuTOzaVOTEMfOtxF/r9ca5\n8HiYmy2qOn9b1sAzh8JLJ8J9veDV4fGtd80S+M9foTLsZWl1U1HmlEpiqayAjb/Hv96yYuf/wk+g\nZFPtYmvgLJmkgKry07LIY1ouW7OFf3+5hP976fuaM149Dd6+EIDKSqW03FOFsWElhc8Mcw4C5SXO\nwWbO+8684g3Ogfjrx5wzbID1y2HhpwBUVCrPfrGY4rIKSssrKS2rYNbkF3inYGlocH/fgY1Lwgxn\nM+YQeCyfz6Z+4sQwJ/SGias2FLNu9Sp47QzYvBq+exo+uMqZ+bd28M7/hbymtLySleu3srW4FO7Z\nEf4zCr56CJ46AIqdz3BjcRkUzYPxZ8OGlU7JYMPKqnVMW7Saoo3VSXDdaxezZcoDVc8rKpX7Js6k\n/JF8mF2z9/oTz7/ICfdNgDLn7LnstzlQNJ/isgq2lJYDsHL9VmavcJPHHOf1Tz9wM2M+X1S1/rWb\nS903tBke2h0e6Qsf3QhfPwoP7sbmt0ayavLDAGwp/JnVCwtYu7mUx6csJNDRUVV5+rNFlLwzEu7p\nBgs+4ZtxtzHn47E1P7TP7oUHnDsVV5YVs3XTBv7YsJmyikpK5k+FyXdULbq1tIKijSWML1jm7CMl\nm9i60hnxaNMXT/HbeucgOPe3DZRVhFaZ/W/Od873vWpuyLwq5SWw2blh5G/riym9bxe4oxWbt7gJ\nb9m3sHkVzPuADyb/l6KNJVRWKr/O+K+z7rVBo4C8dQEUjHVOYOLw3ZI1/Pg/J/mWV1RSXlHJui2l\nXDV+BptKypm5fD3L1rilo49GO6WSMErKK1i3xf0eP70T/tU7Yil0S2k5m0uc/WNzSTlbJ99VNa/i\nk9t49bv/UVHpdmBd9h2sXoR+/Rhf/rKQynmTqpbd5K6jMWh0Q9Dn5+erL2NzLZgMLTvBDrtGX27F\njzBmIGvO+5rtu+wKGRlQvIG3PpzEltx2nHX0QP7YVEKGCC3yssjKEJat2Uq7Frms31rG2i2lDHn4\ni6rVfXDFgYz75ld2bt+Cts2zOLToFf48uQebaArApQN70rl1U97/aQWvrhgEwHk593PYlo/IpJIb\ny53k8mXuFXQW5wf7UPnJDNruf+y6pYD+xY/yTV717bqfLD+OR8tPYnruJTSTEroVv8JusoTzsz7i\nmrKL2V2W8n7uzVXL71U8hvU0Z2le9VnjTWXn83LFEQAM6NGGkvIK3l51dMhHNaTkn6zU7Tkqs4Dx\nFQMBYWTmO1yT/QaPlp/I5VnvAnBg9mt8WXY6APNz+jBkww38ff9MXlyyHXNWbuDYjGnM0m5Myb0a\ngM8r9uDgzF+qtnNO6fW8kHMPAD9V9mCvDOfM8vAW79GqYi2z11TSWwopJocVOT34RYYC0Kd4LCUZ\nTaioVNqwnu/zLqFUM7lr3y948evFVCIszTuDBZWd6JVR83rbw0vu49Pcazmp5A5+1F5V073fwz7F\nT7GeZlyQOZEPK/uxTNvzds6t7JOxMOyuValChijjyg/nzKxPuaz0Clbq9vTNWMTYiiFVy3m/i4Dr\nyy7i9YpDEYFfcs6nuThJoESzyZUyPqzYj0vK/lrjtX2Kx9KUEjbShAMyZjI25342aR7nlF7PW7lO\nwjms5H4KtR1/kjVkUskS7cDQ/M68/9NKupUv5vqs1xiY+RPLs7pQWlrK8JxHaNOyGTOXbwiJs1vx\nKzXi/3vZGdyc/XKNZd4oP5hry52TinuyxnBa1lReaHs13TN/54flW3in4kDeybmV7WUTF+Y9QNH6\nTVyW9R6XlF1Jvx47MG3x6qp1jb94ALe+N5O5v21EqOTdvDvZi/nsV/w4PeQ3puvO7NyhFXNWOicC\nXeV3Ps/9a414hpXexLTK3WpMa5GXxVuVf6V3xnKOLLmXBdoZgMeyH+HwrJ84veXL/PTbVpxb5zju\nynqG4VlTqp6fW3odX1Xuzg/bXUOL0uqE9HNld/bMWMKcyi5MaD6UJ9fsy9VH9mbST79y296b2O/Q\n2t+KR0S+V9X8Wq8gBrs5FlC+/CeyXv4LAJtHzmRL7g60a56DFq+jMrcVGe9dyrqF3/Dd0R+y24xn\n6Axs/9z+AOxS/Bz/zr6fv2Q6t9U+7YtbeD33b7xZcTDXlFWfaWdQSRNK2EwTQGnBVkrIpuKpQ9ha\nPoTbKw9gUEYBx+Y8yI1Zh3JH+Tm0l7U8MdUTaJ7z77nSa6q+uUAyCRzAAK7Mepulm9pDBuRJaY33\nenTGt1yS937V8+Zs4YPcmwCYUtGXC7Nq9ORmdNarDPP8CAAqPT+S7xavYg9ZArmhn+uHuTdUPf5D\nW/Lfyn2qXptB9VnuaVtfq3o/vUtn83nulXT6YTVjS+7lwIy1PJbzaI31ehMJwL4Z86oeBxIJwKKi\nzSzNO5c5OV3YNcMZJm7n4uerPsfZeefTu/gFmlFCHs7nlCMVfDHtKxbnXcsdZWcBhCQSgPMzPwLg\nndzbOKLkXvaSxeyXMbfG9/BDXvX3fyOvckjJAxETCUCGOCd2Z2Y5JcbHcx6pmndS5hccV3pX2NcB\n3JP9DJ9U7Msa3Y5Mz2ebK0570pDM6c7d5z3ez7mJnhkra0xrLsXsm1HdzvGfnJuYq12q4n66/Bgu\nnvkBqyqu4/nce6uW61S+DDLgzOKX2b1kKSO4KiTGQzJ+4rPKvaqeBycSgFOzPueh8r+wnHZV+8rK\nlcs5J/s1Ds5y9u2AZ4uvqtrvOpX/wbTFmTXWNfTpadyf/RRzM7vU2NZt2S9ybOa3jC8/hNmrdqRr\nxvZ0ktWMznolJJ6esoJpOMmksxRxSuZn5JaXkZURWsV2bOY3oPDeupN5N3t/7i8fSqHuAHjTiuO+\n7Kc4o/SmGokEoJc4+9quGcvYdcu/eIln+dcn83ki+yH2++w71vb5hdbtu4ZsuyGwkgkw/o1xDJ11\nGQAnltzJDN2J0zKncE/2M1VnoJFcWzaC+7LHhJ0XOBMDuD3rec7N+pjexS9wZuZkbs2uWZf997Iz\nWKWteSTnMTZqEzbShI6yhu7F49hNlnJc5jQuzvog4jaCzwKXVLane8bvHFryr6oz+mQ6tORfLNE/\ncV3W61yaFTKwQVi/Vu5AFymqOmjGMrViLwZm/lTrGLsVvxL2LD6c+8tO5Zrsmu1R8yo7s3NGYa23\nH2xY6U28mlP7BvFuxa9wU9Y4LgpK+F4Dih9lmqck6jW2fDDnZ32U0DaDS4F19UL5kZyT9UnM5boV\nv8Kj2Y9wXOY3PFc+iPOyJkVd/qCSB1mm7REquSPrBcZVHMFabcH0vEvrHHP34nEcmjGDsTn3h53f\nu/gFSsliad4ZIfMWVXZgjnbl2Mxv49rWFs2lqZTUeD667EIeyXkcgId3G8+oUwfV4l1YycR3n875\nnbd/XMnQHOd5UykGhSMynPaKHrIyyqshg8gHxg9zRpNJBR9W/plzsz4G4JzMSdyUHXoGdHP2y/yn\noj8ALWQrLXDaNoZkfMcTnjPUYBNzbqBPRujI0oGz05frcPCKZkru1XxcsS9tZX3cr9kxI7FeTnVJ\nJAAPZD8R97IHZMwKmZbMRAIgUfaVeOzfag0XFUdOJEDERAIknEgAimiZ8GuiiSeRAFya+R7HZX4D\nEDORAPzfIT0p+/Kxqt/ZoJwZ/KPFzRD/7hlRM4ojJhKAW7Je4lvZI+y8nhkr6Un0Y4hX8B7SVEqq\nEgnA0H3DDrzeIKR1yWT9ho0ccdd79JDfeD33bwAML72RaZV9eCX7LgZkRrr1Sv0p7z+SrG8eS3UY\nxjRsF0yGfx9Rc9rup8DMN+u8am23K1I0J/pCew2DnyKOVRu/7GZQtjny/MumQ7vetVq1lUx88v2v\na9j67LFMz6t5RtpDVnJ41o8NIpEAZG1ckeoQjGn43r8idFoSEgkQO5FAchIJQPi7cHjmN9yLUNM2\nmTz19KM8kxNatfH37OdSEE0Us96OvYwx6W5Vwzj5q7Py4ujzYyWbFErb60yeyXkg9kLGGNOQNOCS\nSVomk1UbY2R/Y4xpiPy46j9J0jKZvPzN/1IdgtnWjapbTzRjwrJqroZl0qzfUh2C2dZlN011BGZb\n1IB736ZlMpn728ZUh2Ciadkl1RHUXWZOqiMwjcG1ixJbvjRKt+EUS8tk0iYZVzIlw4CRqY4g+Xbo\nk+oIUu/g6yB3O//W3/Ow6PN3PT76fOO/P4cOWhpWXqvE1vvDC4nHUk/SLplsLilnXE7kMY7qVWZ2\nqiNIvu4H13x+bRzDeYfTunvir9npSOf/UX+Hzv1qt91kOOwmZwDQWDrsFTqtZRzjLklm9PkZSejx\nf8jouq8jFa6cWbvXtUryeFe7Hg8d96k57egwV9FnxPgugzXg6tO0SyZL/thcNehf6gUP/xZDu12S\nH0IiVUq7nRx7meADXVaYESBjadkZLvk68dftc5ZTvbTr8Q2jmunwW6PP32G30GkSxz4hQT/b7gfX\nnBar5BKPQ2+IPr/r/qHTjn3Queq8ttruXPvXBmzXEXocGnn+oTeFn777X6ofX/gp7Htu9fPa/O4y\nMmHHoM9otzA324rn+/ayZNJw3PP2V6kOoVq7KD+eHQ8MnRbrjLQ2EikddQo602oXZrj+4DPy2hzU\nTxsHObX40bTbFW4pgtY7Jn7GlwyZuTD47urnwQf9eMRzcAl+b8c8CLd5bpa195mJbzdRXcKU/DKy\nCB1dKgGJHljDrwR2HhJ59iHXRX5dQOd8OO7h6ue1KbWohn7/8ewPg2LUmnQ/KPFY6omvyUREBovI\nPBFZKCJhy80iMlREZovILBEJHQExyZasqKdbqoY76wy252lwYZj7TF/ytXOGFbCfM8x8XFUnydB+\nD/jzJaHTgy+YOmBU6DLBP5jaVOU1a5v4a059vuaYRd6qnubtE19fbdyyCvp7Prfy0sjLAuEPvAmW\nTM6dCG13Cppfx4NyG3d90Q6i4ZK1avUJT/754V+34wHO/7a94ax3nWqpk8KPuh3Cm6gjlahFatfj\nKdqBfvDd0Htw+Hk7HxN+ulYkfjJx7EORP7eAXY9LbJ31yLejk4hkAo8DQ4A+wDAR6RO0TC/gBuAA\nVd0NuNKveAKaEscFi2dPgPwLQqd37gfnhN5BMKyLwiSJYCLQed/Q6e13q3ngDhwMt+sc37Z7HRXf\ncl4jPXd1vGASDLkbTnyq5jI7B90Aq3m7MCsSON8zymtdDmxnvxd6QAuOISC4CqHp9s7/gTfAn/aM\nva3cOoyO27Jr+INNrKExwjn95djVPd4DebcDEt9GLIHqmRMej7xMpFLyoLug3wgYcm/4Zfd3RzVu\nvxv0PBRadYGuzmjZtO0NF0e5BXOT1tWPj3uo+nGmpyq1tvtbtNe16QnDX69+fo3n9tRd/xz+NZUV\nkBfUCSNWcsk/D7LyIs9PZTtgHPw81e0HLFTVxapaCrwGnBC0zEXA46q6FkBVfS02FJdVVN2RL6rt\nOkLHvjWn9TwMzv3AKWbeHkdvsGzPTnHEHXBw5HuihOc5u+q4N5zwBJzoGVL9zChjdrXZKfI8qNm4\n7f2h37gSLv0Gcpo5z/sOq553+3poW31HQQB2ChqlFZwfZeDgEHBGLe8l3mNgdTvNTkc673nYq87/\nWFV+gR5NfaLcma5pm+rHB4YpZcXy50uc+vW//lLzYBNQXhI6LSBcB4OWXZyD7FkxxmOrTfWZ18nP\nwJVR7lFSVaqLcoDdOdyZujonGEffV7NE6i0l7nSkk1C8jdGtd4Qz3nT273CdEqp44vF+/+d9GBpH\nOENfjLzqREozzXeAXY4NjanG+ipCe2vG1R4mzslPTvPw62zA/EwmnQBvS3ehO82rN9BbRL4SkW9E\nJGxZUkRGiEiBiBQUFRXVOqDSiko6yJqaE1t3d9onbl1b3U0vI5OQnaT9bpAVpf4/2o/zwCujN5wN\nuc8p4np5SyaZ2bD3GTXPzLaP0tupf5gqKq+WnhLOsFeh/6XO2VdO09DbFv91NoxMZMj/MD+YXp6k\n0z7MfR8G3eWUBqPZcX/Y6XDn8U6HR6n7dvU5Hm75A3aI0njqrTI4KI4biA13b54VaGTec6hTvx5J\npJLJWe/ABR9Hfl3LznDL6sjz40kmpzxXsyTrPZvec2iMKiz34B/t4NdpXxjxWew4IGhfznJ62wVX\nZfY6EnJb1Jy21/Ca7yHwvvueWfMzCG5fi1QF1if4XLZGkFHmhdG6W/T5qqGdTyJ9bzsdAftdVP38\n/76AG5dXJ6wD3VsJN+ChVCD1DfBZQC9gIDAMeEZEQjpeq+oYVc1X1fx27cJVrcSntDzMIGnnTYTz\nPnDaIwLVB8ENie33cIruXl0H1JwWq5Eu+IdyoOde038eEdpoGDhTat0durndbUWqi8HexBLQoW98\nsVSWVz9u2wsG/zPygaNlp9ASiRNMzaeH3VLz+Z+Cksb5k+DkZ+GYf4Wuqv+l0OOQCMFG+JEffC1c\n/IVTvRXp4FF1dhxhHYle59P7KKeEdtLTTokkuENCMG/JxNsu0PMw5+w2WJue1Y8z3QN6h76hy8XT\nEWP3k+Eqz6jY4bYXSXDJJFJX41gHt0DJr7Is+nKRnPSkc2ANEHE+/xMfr7m/BpfEdzkGznkfugSV\nkAM67xc6rbZXlouEXtfT/1LoHrQ/n/VO5HWc+RYcE/nmW1VJqAEP8gj+JpPlgPdX3tmd5lUITFDV\nMlVdAszHSS6+2FRcHjrR+8MUbzLxuOTL0AP0+R85xfnzPoLja96jPKw9To0+P/isJbDjHHFbzYb3\nG1fAdUvCJ5MRU50SFjiN+4P+GX5bR/4tdryxhMTr/hgDP/LzJ8HV1fcSp2t/2PNU54w2ZF21qOfO\nyIQOezoN73+t5bUFbXvFrhIMJzMreokkoMJNJjseALuFqW7zVhMOfwNODbog7eLPnXajZBl0F/xf\nhN6MTT0lhUB7U+B76bRv+LP6WEni5Gec/+13TyzOGnFtH31+t4NCO3mIuN2lI+xXF04OnZZoF/Y9\n3NJp78Fw2ks1jxmD/1l9MhDQ87DEk0EgfksmTAd6iUh3EckBTgeC6zLexSmVICJtcaq9anmVW2zL\n1m4JnejdCbwHyHjPVHYcAPucHXu5Jq2cM9qI6w/e8QMH56CvKCMz8g9MpDrxnDwGBnjuf32u53av\nXcKcmSUqYlWL+z5ymkGLML2ovK/LvyBMfbcPAh0Sho93qqgumQanjHWmjZjqVOWB0z6zz9mQkaSL\nSQPVXP1GhC9N7OG5JqP3Uc4+4tVhL2fa6KDrorKjNNKGk9XE+T/gMviT58C+5+nVj70l5wHBt/4V\naNGREMGl7eB9ulmgFkGcXk+nvZxI1KESPelIpLQR7iSnzwmethFXoANIx73DtyNGk90s/mXBc4Lm\n7jsNPJn4dnMsVS0XkZHAJCATGKuqs0TkTqBAVSe4844SkdlABXCtqkapLK6bNRvCJRPPwS1QzeVX\n3eTuf4EJV7hnrEE7esQz/STl++BeP0Nfin3WF87w8fDK0DBxBZVMIvHO3+mI0Au7Ql+QaISh+o1w\nSoZNt4feg5xp7d2Ohbktqg+Kp7o3Rvuxjge9gCPvdKq6eh1Zt+tegnsFJfKZnP0ebN8zwkzPPug9\nUAXO0r2lzSNud0pjb3l6ObbfzamiGRe44C/SwVthWAK9/gfeAJv/CJ0e7rcQNWEkkEzCdSYJbrC/\ncWWU66bi+E4ys6o777x7KSyKo8cnVO87DXiQR/D5TouqOhGYGDTtVs9jBa5y/3y3w5c3h070njEO\nHA0TLq/ddQ7gNOAXr4s8PzMbDr0RJt8WumMEH4Srftx1PJjucSp0CdN9sU8tx2/q5l5MGSn5xYrX\n+z53idDNN9lEapc4u+7vHFgXT6nddrfvAWe8UbvXJkuPgZHneffBHXaFdb86j6u+I893mp3nlKS8\nyQTCH4TramCEoVy2C+6/E8OkmtUsAAAe4klEQVSAy2DZt/EtG0+pJ+qFtAke6L09M2Np2xv6XQz9\nLoq9bAqlugG+Xu2z/pPQid4zxn3Ods4csptQtXP0PSP+DVz5c+xlQn6owdMDs5NUMvnLs9U74dH3\nO12M6yJQBRRchx6oI94mxhtzP/vjH4Gz301tKAEd9/a0ayTrDNVdzz5nO/tJyOwwpc3eg0OvhQh0\nc/b7zDncVffR9Dkhvm78yRA4+Yv3WrBEiMDR9yZWpZYCaXUPeAm3s8caFC+R6om8BC58CymZRGiA\nT8oQE65knNlk5cDV82peowHONRebVztng7WV0xxKN9UtvmQITuRnvZu8q+h7RxnqI5oRU+H75+H9\nWlwPE0ngfXY7yKnqO/kZ+O4Z7wLuf88+GO56mh6HwPdLQqcnc98NXXn8i+a0gFKfbzuRu51TKxHr\nGqFE+Pr5JV9aJZMMjdGbq15E2EGCk0nHvWHBpJrDqjQULf4UOi2nKQyu42jMo36CrcHVhKmoJ3a3\nGTiR6Bll4MBEXL808UbYcKKVAOK52r96Re5/d5/cc6jzF7ydRnZQC3HpNPhjfuzl6sT9rIK7YF/4\n38ROMr26DoA578e+pqWBSKtkkkWYZJLsAQGPezj0rN0r4g8zqBvgIdc5/eU7JHJwaESCh+cGp60q\nUntVKg5oyer8EBCuO3cy3bgyseHnDxgFiz+LkizjTeQ+fzdtd4Y/5tX+9a26OH/hnPxM7dtIvao+\nqqDPItxwSfHqf6nTeyzaBcoNSFolk7CSfZDyDl298zGRh4eIWM0VuFAsc9tNJJf/kNhFdKlSH6XW\ntr0TP2sWgavmhlYJJjrScoe94Lood/pLuGQStE8HGsy9v4naGDE1yjhndSy57nJs7UaoDtZ3OHz7\npNvemiQijSaRgCWTyJLRmBi2O2SkBng3mdQmuZ38bHVX1/o2siDxRvc2kbqqNjD1MYz9iKlQGqbL\nejSqsF0HP6IJ3pD7P84eesG/mabbJ6cBPKdpcg744Xh/byc+Cc1qeZIz6C447Oba3b9nG5GeyeT4\nR50uwHFJcskl5nUYtaha2TPG1fV+auA9TOok2dVc4eQ0qx5YM6Z6ruqLu2TSCNtUMnOrRygI6Du8\n9uvLyIDcMIMzppH0TCYZWc6QIuGuv/BdhLO4wIGr64D6DcdEVu+dM+JVX50S4iyZpEJj7xSwDUrf\nZHLAFTEW8ukHGxiePfhir6wcZ+DCRlRHWq9ScfVvfd2MrKGqyiUN8MDdcR+n59pRtRxn7qCrYepd\nyRs6x6RpMmnbO/YyAcn+IXXaxxkaPVw7Q6IN7ic/u21XM0H1uFKpqIuuj2quhqzqWqcG+DnkNK05\nonCiBl7v/JmkSatk8l/2Y0DmXJoE3/gqnMAQ0rFG+62NZF0lnsq2kvpywChnyPzArYvrU0Or5gqM\nXN1u1+jLJU2c1Vz7nAUF//ZnaBXTaKRVMpHKctY17Uhcnffa9Ky/oRhMZDlN4fBbYi/nh/rozZWI\nnofCBZPDj3Drp1il88AIuiatpU0yUVUytdzqSE38GlrJBJJz+4B4NfBRak3D0gArQ/1RXqlkUYEm\ncoWwSW8Nsa2gXjXg3lymwUmbX0t5hdJCtlgyMfFraNVc9a0xjc3VpBa3GDBJlTZH1rLSreyRsRTW\nLU11KKaxaAwHUT8Fhrxpt0tq44jHyOmwxbf76pk4pE0yqSiNNLaPMUFi3eQsXXTOh3P+U31tVEMW\nbZBQUy/SJpmUl5XEXsgYgIs/g+XfpzqKhqH7QamOwDQSaZNMKspKAVjY9TR2SnEspoFr3a3R3EPC\nmIYibRrgA8lkXevdUxyJMcZse3xNJiIyWETmichCERkdZv65IlIkIjPcP98ucy4vd5KJZNl1JsYY\nk2y+VXOJSCbwOHAkUAhMF5EJqjo7aNHXVXWkX3EEVLolE8nM8XtTxhiTdvwsmfQDFqrqYlUtBV4D\nTvBxe1FVBEomyRoXyxhjTBU/k0knYJnneaE7LdhfRORnEXlTRMLeqFlERohIgYgUFBUV1SqYSjeZ\nZGRZycQYY5It1Q3w7wPdVHVP4BPghXALqeoYVc1X1fx27drVakNVycRKJsYYk3R+JpPlgLek0dmd\nVkVVV6tq4AKQZwHfhkOtKC8DQKxkYowxSednMpkO9BKR7iKSA5wOTPAuICIdPE+PB+b4FYxaNZcx\nxvjGt95cqlouIiOBSUAmMFZVZ4nInUCBqk4ArhCR44FyYA1wrl/xVFYEugZbMjHGmGTz9Qp4VZ0I\nTAyadqvn8Q3ADX7GEJBR6VRzkZk2F/0bY0y9SXUDfL0pafIn3qo4iPJcG6raGGOSLW2SyYY2e3B1\n2SWUNe+Y6lCMMWabkzbJJCDN71BhjDG+SLtkYowxJvnSJ5lo7EWMMcbUTvokE5ek+61YjTHGB2mX\nTIwxxiRf2iQTtXouY4zxTdokkwCr5DLGmORLu2RijDEm+dImmajVchljjG/SJpkEWGcuY4xJvrRL\nJsYYY5IvbZKJVXMZY4x/0iaZBIj15zLGmKRLu2RijDEm+SyZGGOMqbO0SSbWZGKMMf5Jm2QSYF2D\njTEm+dIumRhjjEk+X5OJiAwWkXkislBERkdZ7i8ioiKS71csan2DjTHGN74lExHJBB4HhgB9gGEi\n0ifMci2AUcC3fsVijDHGX3ElExF5W0SOEZFEkk8/YKGqLlbVUuA14IQwy/0NuAcoTmDdxhhjGpB4\nk8MTwHBggYjcLSI7x/GaTsAyz/NCd1oVEdkH6KKqH8QZR61ZJZcxxvgnrmSiqpNV9QxgH2ApMFlE\nvhaR80QkuzYbdks5DwBXx7HsCBEpEJGCoqKi2mzOs646vdwYY0wYcVdbiUgb4FzgQuBH4GGc5PJJ\nhJcsB7p4nnd2pwW0AHYHporIUqA/MCFcI7yqjlHVfFXNb9euXbwhG2OMqSdZ8SwkIu8AOwMvAcep\n6kp31usiUhDhZdOBXiLSHSeJnI5TVQaAqq4H2nq2MRW4RlUjra9OrDOXMcb4J65kAjyiqlPCzVDV\nsN15VbVcREYCk4BMYKyqzhKRO4ECVZ1Qq4jryAZ6NMaY5Is3mfQRkR9VdR2AiLQGhqnqE9FepKoT\ngYlB026NsOzAOGMxxhjTwMTbZnJRIJEAqOpa4CJ/QvKL1XMZY4xf4k0mmSLV/aDcCxJz/AnJX9ab\nyxhjki/eaq6PcBrbn3afX+xOM8YYY+JOJtfjJJBL3OefAM/6EpFPrDeXMcb4J65koqqVwJPuX6Nm\n1VzGGJN88V5n0gv4J86AjXmB6araw6e4jDHGNCLxNsA/h1MqKQcOBV4ExvkVlB+slssYY/wTbzJp\noqqfAqKqv6rq7cAx/oXlH7to0Rhjki/eBvgSd2DGBe5V7cuB5v6FZYwxpjGJt2QyCmgKXAHsC5wJ\nnONXUH6w3lzGGOOfmCUT9wLF01T1GmATcJ7vUfnIenMZY0zyxSyZqGoFcGA9xGKMMaaRirfN5EcR\nmQC8AWwOTFTVt32Jygdq/bmMMcY38SaTPGA1cJhnmgKNJpkEWC2XMcYkX7xXwDfqdhJjjDH+ivcK\n+OcIc92fqp6f9IiMMcY0OvFWc/3H8zgPOAlYkfxw/GNdg40xxj/xVnO95X0uIq8CX/oSkc+sa7Ax\nxiRfvBctBusF7JDMQIwxxjRe8baZbKRmm8lvOPc4aTSslssYY/wTbzVXC78DqT9Wz2WMMckWVzWX\niJwkIi09z1uJyIlxvG6wiMwTkYUiMjrM/P8TkV9EZIaIfCkifRIL3xhjTEMQb5vJbaq6PvBEVdcB\nt0V7gTum1+PAEJybag0LkyxeUdU9VLUvcC/wQNyRJ0itO5cxxvgm3mQSbrlYVWT9gIWqulhVS4HX\ngBO8C6jqBs/TZtRD04b15jLGmOSL9zqTAhF5AKekAXAZ8H2M13QClnmeFwJ/Dl5IRC4DrgJyqDlc\ni3eZEcAIgK5du8YZsjHGmPoSb8nkcqAUeB2nhFGMk1DqTFUfV9WeOL3Dbo6wzBhVzVfV/Hbt2iVj\ns8YYY5Io3t5cm4GQBvQYlgNdPM87u9MieQ3nPvO+slouY4xJvnh7c30iIq08z1uLyKQYL5sO9BKR\n7iKSA5wOTAhaby/P02OABfGFbYwxpiGJt82krduDCwBVXSsiUa+AV9Vy937xk4BMYKyqzhKRO4EC\nVZ0AjBSRI4AyYC0+3grYOnMZY4x/4k0mlSLSVVX/ByAi3Yij55WqTgQmBk271fN4VNyRJolYdy5j\njEm6eJPJTcCXIvIZTrPDQbi9q4wxxph4G+A/EpF8nATyI/AusNXPwJLNbttrjDH+iXegxwuBUTg9\nsmYA/YFpRLgupCGzSi5jjEm+eK8zGQXsB/yqqocCewPror/EGGNMuog3mRSrajGAiOSq6lxgZ//C\nSj7rzWWMMf6JtwG+0L3O5F3gExFZC/zqX1j+sc5cxhiTfPE2wJ/kPrxdRKYALYGPfIvKGGNMoxJv\nyaSKqn7mRyB+s2ouY4zxT23vAd9oifXnMsaYpEu7ZGKMMSb5LJkYY4yps7RJJtZkYowx/kmbZBJg\nXYONMSb50i6ZGGOMSb60SSZqfYONMcY3aZNMjDHG+MeSiTHGmDpLm2RilVzGGOOftEkmAdabyxhj\nki/tkokxxpjk8zWZiMhgEZknIgtFZHSY+VeJyGwR+VlEPhWRHX0Lxuq5jDHGN74lExHJBB4HhgB9\ngGEi0idosR+BfFXdE3gTuNeveDxx+b0JY4xJO36WTPoBC1V1saqWAq8BJ3gXUNUpqrrFffoNzj3m\njTHGNDJ+JpNOwDLP80J3WiQXAB+GmyEiI0SkQEQKioqKahWMWj2XMcb4pkE0wIvImUA+cF+4+ao6\nRlXzVTW/Xbt2ddtWnV5tjDEmnITvtJiA5UAXz/PO7rQaROQI4CbgEFUt8TEeY4wxPvGzZDId6CUi\n3UUkBzgdmOBdQET2Bp4GjlfVVT7GYrftNcYYH/mWTFS1HBgJTALmAONVdZaI3Ckix7uL3Qc0B94Q\nkRkiMiHC6pLGOnMZY0zy+VnNhapOBCYGTbvV8/gIP7dvjDGmfjSIBvj6YLVcxhjjn7RJJgFi/bmM\nMSbp0i6ZGGOMSb60SSbWm8sYY/yTNskkwHpzGWNM8qVdMjHGGJN8aZNMbGwuY4zxT9okkwCr5TLG\nmORLu2RijDEm+SyZGGOMqbO0SSbWNdgYY/yTNsmkijWaGGNM0qVfMjHGGJN0aZNMrJbLGGP8kzbJ\nJMAGejTGmORLu2RijDEm+dInmVh3LmOM8U36JBOXDfRojDHJl3bJxBhjTPKlTTKxSi5jjPGPr8lE\nRAaLyDwRWSgio8PMP1hEfhCRchE5xc9YqrZZHxsxxpg041syEZFM4HFgCNAHGCYifYIW+x9wLvCK\nX3EYY4zxX5aP6+4HLFTVxQAi8hpwAjA7sICqLnXnVfoYh7stv7dgjDHpy89qrk7AMs/zQndaSol1\n5zLGmKRrFA3wIjJCRApEpKCoqCjV4RhjjAniZzXXcqCL53lnd1rCVHUMMAYgPz+/VhVWavVcxqS1\nsrIyCgsLKS4uTnUovsrLy6Nz585kZ2fX63b9TCbTgV4i0h0niZwODPdxe3GxSi5j0lNhYSEtWrSg\nW7du22x1t6qyevVqCgsL6d69e71u27dqLlUtB0YCk4A5wHhVnSUid4rI8QAisp+IFAKnAk+LyCy/\n4jHGpLfi4mLatGmzzSYScNqE27Rpk5LSl58lE1R1IjAxaNqtnsfTcaq/fGeVXMaYbTmRBKTqPTaK\nBvhkSoN9yRhj6l3aJRNjjEmFdevW8cQTTyT8uqOPPpp169b5EFFypU0ysc5cxphUipRMysvLo75u\n4sSJtGrVyq+wksbXNpOGyO60aIy54/1ZzF6xIanr7NNxO247breI80ePHs2iRYvo27cv2dnZ5OXl\n0bp1a+bOncv8+fM58cQTWbZsGcXFxYwaNYoRI0YA0K1bNwoKCti0aRNDhgzhwAMP5Ouvv6ZTp068\n9957NGnSJKnvo7bSpmRijDGpdPfdd9OzZ09mzJjBfffdxw8//MDDDz/M/PnzARg7dizff/89BQUF\nPPLII6xevTpkHQsWLOCyyy5j1qxZtGrVirfeequ+30ZEaVMysVouY0xAtBJEfenXr1+Na0EeeeQR\n3nnnHQCWLVvGggULaNOmTY3XdO/enb59+wKw7777snTp0nqLN5a0SSZVrJbLGNMANGvWrOrx1KlT\nmTx5MtOmTaNp06YMHDgw7LUiubm5VY8zMzPZunVrvcQaD6vmMsaYetCiRQs2btwYdt769etp3bo1\nTZs2Ze7cuXzzzTf1HF3dpV/JxBhjUqBNmzYccMAB7L777jRp0oT27dtXzRs8eDBPPfUUu+66Kzvv\nvDP9+/dPYaS1kzbJxAZ6NMak2iuvhL8PYG5uLh9++GHYeYF2kbZt2zJz5syq6ddcc03S46uLtKvm\nsivgjTEm+dIumRhjjEk+SybGGGPqLO2SidVyGWNM8qVdMjHGGJN8aZNMrDOXMcb4J22SSUA63BzH\nGNPw1HYIeoCHHnqILVu2JDmi5Eq7ZGKMMamwrSeT9Llo0YZ6NMYEfDgafvsluev80x4w5O6Is71D\n0B955JHssMMOjB8/npKSEk466STuuOMONm/ezNChQyksLKSiooJbbrmF33//nRUrVnDooYfStm1b\npkyZkty4kyRtkkmAVXIZY1Lh7rvvZubMmcyYMYOPP/6YN998k++++w5V5fjjj+fzzz+nqKiIjh07\n8sEHHwDOmF0tW7bkgQceYMqUKbRt2zbF7yKytEsmxhgTrQRRHz7++GM+/vhj9t57bwA2bdrEggUL\nOOigg7j66qu5/vrrOfbYYznooINSGmcifE0mIjIYeBjIBJ5V1buD5ucCLwL7AquB01R1qR+xWG8u\nY0xDoarccMMNXHzxxSHzfvjhByZOnMjNN9/M4Ycfzq233pqCCBPnWwO8iGQCjwNDgD7AMBHpE7TY\nBcBaVd0JeBC4x694quPyewvGGBPKOwT9oEGDGDt2LJs2bQJg+fLlrFq1ihUrVtC0aVPOPPNMrr32\nWn744YeQ1zZUfpZM+gELVXUxgIi8BpwAzPYscwJwu/v4TeAxERG1IX6NMdsY7xD0Q4YMYfjw4QwY\nMACA5s2bM27cOBYuXMi1115LRkYG2dnZPPnkkwCMGDGCwYMH07Fjx7RsgO8ELPM8LwT+HGkZVS0X\nkfVAG+AP70IiMgIYAdC1a9daBdOjXXOO2aMDGVY0McakSPAQ9KNGjarxvGfPngwaNCjkdZdffjmX\nX365r7HVVaNogFfVMcAYgPz8/FqVWo7s054j+7SPvaAxxpiE+XnR4nKgi+d5Z3da2GVEJAtoidMQ\nb4wxphHxM5lMB3qJSHcRyQFOByYELTMBOMd9fArwX2svMcb4JR0OL6l6j74lE1UtB0YCk4A5wHhV\nnSUid4rI8e5i/wbaiMhC4CpgtF/xGGPSW15eHqtXr96mE4qqsnr1avLy8up929LYPtj8/HwtKChI\ndRjGmEamrKyMwsJCiouLUx2Kr/Ly8ujcuTPZ2dk1povI96qa79d2G0UDvDHG1FV2djbdu3dPdRjb\nLBs12BhjTJ1ZMjHGGFNnlkyMMcbUWaNrgBeRIuDXWr68LUFX1zcCFnP9aYxxW8z1Y1uIeUdVbefX\nxhpdMqkLESnwszeDHyzm+tMY47aY64fFHJtVcxljjKkzSybGGGPqLN2SyZhUB1ALFnP9aYxxW8z1\nw2KOIa3aTIwxxvgj3UomxhhjfGDJxBhjTN2palr8AYOBecBCYHQ9bXMp8AswAyhwp20PfAIscP+3\ndqcL8Igb38/APp71nOMuvwA4xzN9X3f9C93XSrRtRIlzLLAKmOmZlrI4o20jRsy349wjZ4b7d7Rn\n3g3u+uYBg2LtF0B34Ft3+utAjjs9132+0J3fLdY23HldgCk4t62eBYxq6J9zlJgb7Ofszs8DvgN+\ncuO+I9nbSub7iRHz88ASz2fdt6HsHyGfe30cVFP9B2QCi4AeQI77hfWph+0uBdoGTbs3sPPhDLl/\nj/v4aOBD9wvsD3zr+aIXu/9bu48DX/Z37rLivnZItG1EifNgYB9qHphTFmekbcQR8+3ANWGW7eN+\n57k4P/ZF7j4Rcb8AxgOnu4+fAi5xH18KPOU+Ph14Pdo2PDF0CPwYgRbAfPc1DfZzjhJzg/2c3WUE\naO4+zsY5ePdP1raS+X7iiPl54JQwn3XK94+QmPw+oDaEP2AAMMnz/AbghnrY7lJCk8k8oIP7uAMw\nz338NDAseDlgGPC0Z/rT7rQOwFzP9KrlIm0jRqzdqHlgTlmckbYRR8y3E/4gV+P7xrnHzoBI+4X7\n4/kDyArefwKvdR9nuctJpG1E+bzfA45sDJ9zmJgb0+fcFPgB+HOytpXM9xNHzM8TPpk0uP0jXdpM\nOgHLPM8L3Wl+U+BjEfleREa409qr6kr38W9A4Mb0kWKMNr0wzPRo20hEKuOsy/c1UkR+FpGxItK6\nljG3Adapc4O34O1Xvcadv95dPu6YRaQbsDfO2Wej+JyDYoYG/jmLSKaIzMCpCv0EpySRrG0l8/1E\njFlVA5/1P9zP+kERyQ1eX5yx+f47TJdkkioHquo+wBDgMhE52DtTnXSvfgaQjG00ljiBJ4GeQF9g\nJfCvusaVbCLSHHgLuFJVN3jnNdTPOUzMDf5zVtUKVe0LdAb6AbukOKSYgmMWkd1xSjy7APvhVF1d\n73MMtd4H0yWZLMdpTAzo7E7zlaoud/+vAt7B2al/F5EOAO7/VTFijDa9c5jpRNlGIlIZZ62+L1X9\n3f1BVgLP4HzetYl5NdBKRLKCptdYlzu/pbt8zJhFJBvnoPyyqr5dy8+gXj/ncDE39M/ZS1XX4XQi\nGJDEbSXz/USLebCqrlRHCfActf+sff8dpksymQ70EpHuIpKD0wA2wc8NikgzEWkReAwcBcx0t3uO\nu9g5OPXQuNPPFkd/YL1b9JwEHCUird3qhKNw6mFXAhtEpL+ICHB20LrCbSMRqYwz0jaiCvwgXCfh\nfN6B9Z0uIrki0h3ohdMYGXa/cM/OpgCnRIgtEPMpwH/d5SNtIxCbAP8G5qjqA544G+znHCnmhvw5\nu/G1E5FW7uMmOO08c5K4rWS+n2gxz/Uc5AU4Meizbli/w0iNKdvaH07PhPk4dac31cP2euD08gh0\n9bvJnd4G+BSnG95kYHt3ugCPu/H9AuR71nU+Tve8hcB5nun57s61CHiM6q5+YbcRJdZXcaorynDq\nRS9IZZzRthEj5pfc5X92fwgdPMvf5K5vHm4vlmj7hfv9fee+lzeAXHd6nvt8oTu/R6xtuPMOxKk+\n+BlPl9qG/DlHibnBfs7u/D2BH934ZgK3JntbyXw/MWL+r/tZzwTGUd3jK+X7R/CfDadijDGmztKl\nmssYY4yPLJkYY4ypM0smxhhj6sySiTHGmDqzZGKMMabOLJmYRktE2ojIDPfvNxFZ7nmeE+c6nhOR\nnWMsc5mInJGkmE9w4/tJRGaLyIXu9JNFpMFfpW1MJNY12GwTROR2YJOq3h80XXD288qUBFYzllyc\n4cTzVXWF+3xHVZ0vIuOAN1X13dRGaUztWMnEbHNEZCf3rP9lnAtGO4jIGBEpEJFZInKrZ9kvRaSv\niGSJyDoRudstNUwTkR3cZf4uIld6lr9bRL4TkXkisr87vZmIvOVu9013W32DQmuJcyHYGgBVLXET\nyUE4F8E96JZauolILxGZJM4goZ+LSG93O+NE5El3+nwRGeJO30NEpruv/1lEevj6IRsTxJKJ2Vbt\nAjyoqn3UGSNttKrmA3sBR4pInzCvaQl8pqp7AdNwriQOR1S1H3AtEEhMlwO/qWof4G84I+zWoM4Y\nbZOAX0XkFREZJiIZqvoFMBH4q6r2VdWlwBjgUlXdF2ewv8c8q+qCM/DfccAYt4RzKXC/OgMF7ges\niOdDMiZZsmIvYkyjtEhVCzzPh4nIBTj7fEecGx/NDnrNVlX90H38PXBQhHW/7Vmmm/v4QOAeAFX9\nSURmhXuhqp4rInsCR+DciOhw4ELvMu4YTf2Bt5xaOqDmb3W8W203T0SW4YwZ9TVws4jsCLytqgsj\nxG6MLyyZmG3V5sADEekFjAL6qeo6t30iL8xrSj2PK4j8+yiJY5mIVPVn4GcReQVnAMILgxYR4A+3\nlBF2FaGr1JdEZBpwDPCRiJyvqp8nGpsxtWXVXCYdbAdsxBk1tQMwyIdtfAUMBaf9AqfkU4OIbCc1\n72nTF/jVfbwR59a4qOpaYKWInOS+LkNE9vK87lR3JNfeOFVeC0Skh6ouVNWHgf/gDBxoTL2xkolJ\nBz/gVGnNxTl4f+XDNh4FXhSR2e62ZuPcTc9LgBtE5BlgK7CJ6naZV4GnReRqnKHGTweedHup5eCM\nGPuTu+xyoABoDoxQ1VIRGS4iw3BGUV6Bc2tdY+qNdQ02JgnEueFRlqoWu9VqHwO9tPoWrsnajnUh\nNg2SlUyMSY7mwKduUhHg4mQnEmMaMiuZGGOMqTNrgDfGGFNnlkyMMcbUmSUTY4wxdWbJxBhjTJ1Z\nMjHGGFNn/w/EaeEXTwO0PQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oFAc6IORmzCJ"
      },
      "cell_type": "markdown",
      "source": [
        "Do you notice the improvement over the accuracy compared to that in Part 1?"
      ]
    },
    {
      "metadata": {
        "id": "Q4HYDaV0XhoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes! +10% on accuracy for only a few more seconds of runtime.  It plateaued at 65%."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hXpJgsjhftDP"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 Open Design Competition (35 Points + 10 bonus points)\n",
        "\n",
        "Try to beat the previous models by adding additional layers, changing parameters, etc.  You should add at least one layer.\n",
        "\n",
        "Possible changes include:\n",
        "*   Dropout\n",
        "*   Batch Normalization\n",
        "*   More layers\n",
        "*   Residual Connections (harder)\n",
        "*   Change layer size\n",
        "*   Pooling layers, stride\n",
        "*   Different optimizer\n",
        "*   Train for longer\n",
        "\n",
        "Once you have a model you think is great, evaluate it against our hidden test data (see hidden_loader above) and upload the results to the leader board on gradescope.  **The top 3 scorers will get a bonus 10 points.**\n",
        "\n",
        "You can steal model structures found on the internet if you want.  The only constraint is that **you must train the model from scratch**.\n"
      ]
    },
    {
      "metadata": {
        "id": "grxlSZ6STgl0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, n=7, res_option='A', use_dropout=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.res_option = res_option\n",
        "        self.use_dropout = use_dropout\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(16)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.layers1 = self._make_layer(n, 16, 16, 1)\n",
        "        self.layers2 = self._make_layer(n, 32, 16, 2)\n",
        "        self.layers3 = self._make_layer(n, 64, 32, 2)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "    \n",
        "    def _make_layer(self, layer_count, channels, channels_in, stride):\n",
        "        return nn.Sequential(\n",
        "            ResBlock(channels, channels_in, stride, res_option=self.res_option, use_dropout=self.use_dropout),\n",
        "            *[ResBlock(channels) for _ in range(layer_count-1)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.norm1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.layers1(out)\n",
        "        out = self.layers2(out)\n",
        "        out = self.layers3(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_filters, channels_in=None, stride=1, res_option='A', use_dropout=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "        \n",
        "        # uses 1x1 convolutions for downsampling\n",
        "        if not channels_in or channels_in == num_filters:\n",
        "            channels_in = num_filters\n",
        "            self.projection = None\n",
        "        else:\n",
        "            if res_option == 'A':\n",
        "                self.projection = IdentityPadding(num_filters, channels_in, stride)\n",
        "            elif res_option == 'B':\n",
        "                self.projection = ConvProjection(num_filters, channels_in, stride)\n",
        "            elif res_option == 'C':\n",
        "                self.projection = AvgPoolPadding(num_filters, channels_in, stride)\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels_in, num_filters, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(inplace=True)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.use_dropout:\n",
        "            out = self.dropout(out)\n",
        "        if self.projection:\n",
        "            residual = self.projection(x)\n",
        "        out += residual\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# various projection options to change number of filters in residual connection\n",
        "# option A from paper\n",
        "class IdentityPadding(nn.Module):\n",
        "    def __init__(self, num_filters, channels_in, stride):\n",
        "        super(IdentityPadding, self).__init__()\n",
        "        # with kernel_size=1, max pooling is equivalent to identity mapping with stride\n",
        "        self.identity = nn.MaxPool2d(1, stride=stride)\n",
        "        self.num_zeros = num_filters - channels_in\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.pad(x, (0, 0, 0, 0, 0, self.num_zeros))\n",
        "        out = self.identity(out)\n",
        "        return out\n",
        "\n",
        "# option B from paper\n",
        "class ConvProjection(nn.Module):\n",
        "\n",
        "    def __init__(self, num_filters, channels_in, stride):\n",
        "        super(ResA, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels_in, num_filters, kernel_size=1, stride=stride)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out\n",
        "\n",
        "# experimental option C\n",
        "class AvgPoolPadding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_filters, channels_in, stride):\n",
        "        super(AvgPoolPadding, self).__init__()\n",
        "        self.identity = nn.AvgPool2d(stride, stride=stride)\n",
        "        self.num_zeros = num_filters - channels_in\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.pad(x, (0, 0, 0, 0, 0, self.num_zeros))\n",
        "        out = self.identity(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QZFkuraiZOaT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "065b3afe-7014-46ed-b99c-ee55e916ac29"
      },
      "cell_type": "code",
      "source": [
        "#import torchvision.models as models\n",
        "#model = models.resnet101().to(device)\n",
        "# You Awesome Super Best model code here\n",
        "# model = rgb_resnet152().to(device)\n",
        "#model= ResNet110().to(device)\n",
        "##model = ResNet(n=9).to(device)\n",
        "#loss = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=1e-3, weight_decay=0.0001)\n",
        "\n",
        "#metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)\n",
        "awesome_metrics = train(model, train_loader, test_loader, loss, optimizer, 10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Epoch 0 @ step 0: Train Loss: 0.002016, Train Accuracy: 0.000992\n",
            "Epoch 0 Test Loss: 0.267836, Test Accuracy: 0.927017, time: 45.0s\n",
            "Epoch 1 Test Loss: 0.266400, Test Accuracy: 0.926622, time: 44.4s\n",
            "  Epoch 2 @ step 1000: Train Loss: 0.036556, Train Accuracy: 0.988389\n",
            "Epoch 2 Test Loss: 0.271913, Test Accuracy: 0.925633, time: 44.6s\n",
            "Epoch 3 Test Loss: 0.267420, Test Accuracy: 0.926820, time: 44.6s\n",
            "Epoch 4 Test Loss: 0.273473, Test Accuracy: 0.926523, time: 44.5s\n",
            "  Epoch 5 @ step 2000: Train Loss: 0.034557, Train Accuracy: 0.989389\n",
            "Epoch 5 Test Loss: 0.274091, Test Accuracy: 0.925732, time: 44.2s\n",
            "Epoch 6 Test Loss: 0.271662, Test Accuracy: 0.927116, time: 44.7s\n",
            "  Epoch 7 @ step 3000: Train Loss: 0.034006, Train Accuracy: 0.989173\n",
            "Epoch 7 Test Loss: 0.274177, Test Accuracy: 0.926622, time: 44.3s\n",
            "Epoch 8 Test Loss: 0.269862, Test Accuracy: 0.927017, time: 44.4s\n",
            "Epoch 9 Test Loss: 0.272088, Test Accuracy: 0.925435, time: 44.4s\n",
            "Final Test Loss: 0.272088, Test Accuracy: 0.925435, Total time: 445.0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xbx5REUdc-Xs"
      },
      "cell_type": "markdown",
      "source": [
        "**What changes did you make to improve your model?**"
      ]
    },
    {
      "metadata": {
        "id": "zpegYi6jz5a7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/bryanyzhu/two-stream-pytorch/blob/master/models/rgb_resnet.pyOn Resnet-50:\n",
        "* Learning rate 6e-4 got me up to 70%\n",
        "* 1e-4 got me to 72%\n",
        "\n",
        "On Resnet-152:\n",
        "*  1e-4 got me 50%\n",
        "*   List item\n",
        "\n",
        "https://towardsdatascience.com/resnets-for-cifar-10-e63e900524e0\n",
        "Resnet-100:\n",
        "* 1e-4 got me 73%\n",
        "\n",
        "Resnet-56:\n",
        "* 1e-4 got me 73%\n",
        "\n",
        "rgb_resnet152\n",
        "* 1e-4 got me 75%.\n",
        "\n",
        "Got me 67%:\n",
        "model= ResNet110().to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=1e-4)\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "After lots of changes from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10, 150 epochs, 77% test accuracy.  That was with lowering LR, ending with lr=6e-5.\n",
        "\n",
        "Same setup, but lr=1e-1.  50 epochs -> 88% accuracy.  10 more epochs with lr=1e-2 -> 92.5.  10 more epochs with lr=1e-3 -> 92.5.  Reached 92.7."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uItu0w4fZTuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "a0c9a34f-c067-477c-831b-4c62559335e1"
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"AwesomeModel\", awesome_metrics)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rWlP2WsodgK0"
      },
      "cell_type": "markdown",
      "source": [
        "After you get a nice model, download the test_file.zip and unzip it to get test_file.pt. In colab, you can explore your files from the left side bar. You can also download the files to your machine from there."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z6UZInUCdfQ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "4e216e07-e2ad-463b-8dbd-de33124d56b9"
      },
      "cell_type": "code",
      "source": [
        "!wget http://courses.engr.illinois.edu/cs498aml/sp2019/homeworks/test_file.zip\n",
        "!unzip test_file.zip"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-27 17:34:22--  http://courses.engr.illinois.edu/cs498aml/sp2019/homeworks/test_file.zip\n",
            "Resolving courses.engr.illinois.edu (courses.engr.illinois.edu)... 130.126.151.9\n",
            "Connecting to courses.engr.illinois.edu (courses.engr.illinois.edu)|130.126.151.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://courses.engr.illinois.edu/cs498aml/sp2019/homeworks/test_file.zip [following]\n",
            "--2019-04-27 17:34:22--  https://courses.engr.illinois.edu/cs498aml/sp2019/homeworks/test_file.zip\n",
            "Connecting to courses.engr.illinois.edu (courses.engr.illinois.edu)|130.126.151.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3841776 (3.7M) [application/x-zip-compressed]\n",
            "Saving to: â€˜test_file.zipâ€™\n",
            "\n",
            "test_file.zip       100%[===================>]   3.66M  15.3MB/s    in 0.2s    \n",
            "\n",
            "2019-04-27 17:34:22 (15.3 MB/s) - â€˜test_file.zipâ€™ saved [3841776/3841776]\n",
            "\n",
            "Archive:  test_file.zip\n",
            "  inflating: test_file.pt            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7g5caFPreMLT"
      },
      "cell_type": "markdown",
      "source": [
        "Then use your model to predict the label of the test images. Fill the remaining code below, where x has two dimensions (batch_size x one image size). Remember to reshpe x accordingly before feeding it into your model. The submission.txt should contain one predicted label (0~9) each line. Submit your submission.txt to the competition in gradscope."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yNdYAH9XeLlb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import torch.utils.data as Data\n",
        "\n",
        "test_file = 'test_file.pt'\n",
        "pred_file = 'submission.txt'\n",
        "\n",
        "f_pred = open(pred_file,'w') \n",
        "tensor = torch.load(test_file)\n",
        "torch_dataset = Data.TensorDataset(tensor)  \n",
        "test_loader = torch.utils.data.DataLoader(torch_dataset, batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "for ele in test_loader:\n",
        "    x = ele[0]\n",
        "    x = x.view(-1, 3, 32, 32).to(device)\n",
        "    with torch.no_grad():\n",
        "      preds = model(x)\n",
        "      preds = torch.argmax(model(x), dim=1)\n",
        "      for pred in preds:\n",
        "        f_pred.write(str(pred.item()))\n",
        "        f_pred.write('\\n')\n",
        "    \n",
        "f_pred.close()\n",
        "files.download(pred_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8-L6F0CMqc-A"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Report\n",
        "\n",
        "## Part 0: Imports and Basic Setup (5 Points)\n",
        "Nothing to report for this part. You will be just scored for finishing the setup.\n",
        "\n",
        "## Part 1: Fully connected neural networks (25 Points)\n",
        "\n",
        "Test (on validation set) accuracy (5 Points):\n",
        "\n",
        "Test loss (5 Points):\n",
        "\n",
        "Training time (5 Points):\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "\n",
        "## Part 2: Convolution Network (Basic) (35 Points)\n",
        "\n",
        "Tensor dimensions: A good way to debug your network for size mismatches is to print the dimension of output after every layers:\n",
        "\n",
        "(10 Points)\n",
        "\n",
        "Output dimension after 1st conv layer: \n",
        "\n",
        "Output dimension after 1st max pooling: \n",
        "\n",
        "Output dimension after 2nd conv layer: \n",
        "\n",
        "Output dimension after flatten layer:\n",
        "\n",
        "Output dimension after 1st fully connected layer:\n",
        "\n",
        "Output dimension after 2nd fully connected layer:\n",
        "\n",
        "\n",
        "Test (on validation set) Accuracy (5 Points):\n",
        "\n",
        "Test loss (5 Points):\n",
        "\n",
        "Training time (5 Points):\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "\n",
        "\n",
        "## Part 3: Convolution Network (Add one or more  suggested changes) (35 Points)\n",
        "\n",
        "Describe the additional changes implemented, your intuition for as to why it works, you may also describe other approaches you experimented with (10 Points):\n",
        "\n",
        "\n",
        "Test (on validation set) Accuracy (5 Points):\n",
        "\n",
        "Test loss (5 Points):\n",
        "\n",
        "Training time (5 Points):\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "10 bonus points will be awarded to top 3 scorers on leaderboard (in case of tie for 3rd position everyone tied for 3rd position will get the bonus)"
      ]
    },
    {
      "metadata": {
        "id": "SyeA1bwNo0at",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}